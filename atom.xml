<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Error_666</title>
  
  
  <link href="http://error666.top/atom.xml" rel="self"/>
  
  <link href="http://error666.top/"/>
  <updated>2025-02-11T06:49:19.660Z</updated>
  <id>http://error666.top/</id>
  
  <author>
    <name>Error_666</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Embodied AI/Memory-Augmented Neural Networks 论文阅读</title>
    <link href="http://error666.top/2025/02/11/Embodied-AI-Memory-Augmented-Neural-Networks-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    <id>http://error666.top/2025/02/11/Embodied-AI-Memory-Augmented-Neural-Networks-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</id>
    <published>2025-02-11T06:43:07.000Z</published>
    <updated>2025-02-11T06:49:19.660Z</updated>
    
    <content type="html"><![CDATA[<p>一些自己感兴趣的研究领域的论文阅读。</p><span id="more"></span><p>本篇 blog只是呈现了我阅读的论文的一个简单列表和一句话总结。更多关于每篇 paper的详细笔记以及个人思考于本人本地电脑中。写这篇 blog的目的只是为了在本科生涯中直观展示自己学术阅读量的量化体现，这样方便与各位同学、老师们交流讨论。</p><p>不定期更新列表... ...</p><h2 id="embodied-ai">1 Embodied AI</h2><ul><li>《<span class="math inline">\(\pi_0\)</span>: AVision-Language-Action Flow Model for General Robot Control》：通过 VLM和 flow matching 的方式，用 GPT式的方法，先大规模预训练再对齐的方法实现了通用的机器人控制模型。</li></ul><h2 id="memory-augmented-neural-networks">2 Memory-Augmented NeuralNetworks</h2><ul><li><p>《Longformer: The Long-DocumentTransformer》：提出了一种计算注意力权重矩阵的 pattern ，稀疏化注意力矩阵+ 部分全局注意力矩阵，使显存与 query 长度呈线性。</p></li><li><p>《Memorizing Transformers》：通过将曾经的 K, V 存储到外部 memory中，需要用到时拿出来用。</p></li><li><p>《Scaling Transformer to 1M tokens and beyond withRMT》：先分段，然后通过用 transformer对每一段信息进行有损压缩进记忆向量中，从而实现记忆。</p></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;一些自己感兴趣的研究领域的论文阅读。&lt;/p&gt;</summary>
    
    
    
    <category term="1. 科研" scheme="http://error666.top/categories/1-%E7%A7%91%E7%A0%94/"/>
    
    
  </entry>
  
  <entry>
    <title>暂停更新</title>
    <link href="http://error666.top/2024/12/19/%E6%9A%82%E5%81%9C%E6%9B%B4%E6%96%B0/"/>
    <id>http://error666.top/2024/12/19/%E6%9A%82%E5%81%9C%E6%9B%B4%E6%96%B0/</id>
    <published>2024-12-18T17:17:21.000Z</published>
    <updated>2025-02-11T06:50:19.538Z</updated>
    
    <content type="html"><![CDATA[<p>暂停更新的意思是，大幅降低更新频率了，可能偶尔兴致来了会再写点东西。</p><span id="more"></span><p>这个博客搭建于 2023-10-1 ，现在是2024/12/19 ，陪伴了我一年多了。</p><p>当初搭建这个博客的目的，就是为了记录我学习的过程，也方便大家一起交流。从现在来看，我当初的目的已经达到了。这一年里，我学到了很多东西，也或许，找到了我以后想要研究的方向。</p><p>从最开始 Vs Code的配置，到各种工具（anaconda、latex、chatgpt4、matlab、origin），再到大学各种专业课的自学笔记，以及穿插着我科研的一些笔记和系统数学知识的补充，最后是近期对于RL的一些学习。当把自己的成长路径记录下来的时候，就会发现，“噢，原来我已经收获了这么多东西”。这是件令人愉快的事情。</p><p>校内课程的专业课差不多结束了，所以接下来我将会去探索一下 Embodied AI领域（另外，出于个人兴趣，还会对 Memory-Augmented Neural Networks方向进行调研研究）。肯定也会写一些总结思考笔记什么的，不过那就是在我本地的笔记库了。大家方向不同，兴趣不同，也就没必要将具体细分方向的笔记放上来献丑了。这个博客，更像一个初入计算机领域的学生写的一个笔记本吧。</p><p>一年之后该域名 error666.top我将不再续费，如需访问，请访问：potatoQi.github.io</p><p>最后，还是那句老话，也是最重要的，祝愿自己和各位，身体健康，学业顺利！</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;暂停更新的意思是，大幅降低更新频率了，可能偶尔兴致来了会再写点东西。&lt;/p&gt;</summary>
    
    
    
    <category term="杂项" scheme="http://error666.top/categories/%E6%9D%82%E9%A1%B9/"/>
    
    
  </entry>
  
  <entry>
    <title>机器学习自学笔记</title>
    <link href="http://error666.top/2024/12/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/"/>
    <id>http://error666.top/2024/12/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/</id>
    <published>2024-12-18T17:07:27.000Z</published>
    <updated>2024-12-31T17:16:40.283Z</updated>
    
    <content type="html"><![CDATA[<p>看学校ppt + 西瓜书的一些总结</p><span id="more"></span><h3 id="绪论">1 绪论</h3><ol type="1"><li><p>什么是机器学习</p><ul><li>通过算法使得机器能从大量数据中学习规律从而对新的样本做决策。（相当于构建一个映射函数）</li></ul></li><li><p>机器学习研究的主要内容</p><ul><li>在计算机上从数据中产生“模型”的算法，即“学习算法”。我们把经验数据提供给它，它就能基于这些数据产生模型。在面对新的情况时，模型会给我们提供相应的判断</li></ul></li><li><p>常见的机器学习基本任务</p><ul><li>分类：是定性输出，输出类型是离散数据，是离散变量预测</li><li>回归：是定量输出，输出类型是连续数据，是连续变量预测</li></ul></li><li><p>常见的机器学习方法/类型(按学习形式分类)</p><ul><li>监督学习：数据都有明确的标签，根据机器学习产生的模型可以将新数据分到一个明确的类或得到一个预测值。</li><li>无监督学习：数据没有标签，机器学习出的模型是从数据中提取出来的模式（提取决定性特征或者聚类等）</li><li>半监督学习：利用少量的标记样本和大量的未标记样本来改善模型的学习能力。位于无监督学习和监督学习之间。</li></ul></li><li><p>几个常见的术语：数据集、样本(示例)、特征(属性)、样本空间(属性空间/输入空间)、特征向量、维数</p><ul><li>一行记录就是一个样本，然后字段就是特征(属性)，样本空间就是轴名字为所有字段的一个空间，特征向量就是(字段1,字段2, ...)，维数就是字段的个数。</li></ul></li><li><p>独立同分布：是指随机过程中，任何时刻的取值都为随机变量，如果这些随机变量服从同一分布，并且互相独立，那么这些随机变量是独立同分布。</p></li><li><p>假设空间、版本空间、归纳偏好</p><ul><li><p>假设空间：属性所有可能取值的组合。</p><p><img src="1.png" style="zoom:67%;" /></p></li><li><p>版本空间：与训练集一致的“假设集合”。</p><ul><li><p>求法：把跟正例不一致的假设删除，跟反例一至的假设删除。</p><p><img src="image-20241205233706344.png" alt="" style="zoom:67%;" /></p></li></ul></li><li><p>归纳偏好：在学习过程中对某种类型假设的偏好，称为归纳偏好</p><ul><li>例如，上面西瓜的例子求出的版本空间中有三个假设，它们都能满足训练集，但是面对新样本时，会产生不同的输出。所以选择特殊点的假设呢？还是选一般点的假设呢？</li><li>奥卡姆剃刀原则：选最简单的假设。</li></ul></li></ul></li><li><p>没有免费的午餐定理：在所有“问题”出现机会相同的前提下，所有算法的期望性能相同。</p></li></ol><h3 id="模型评估与选择">2 模型评估与选择</h3><ol type="1"><li><p>几个误差</p><ul><li>训练误差：训练集上的训练所得模型与目标的差异</li><li>泛化误差：在未见过的数据集上训练所得模型与目标的差异</li><li>测试误差：测试集上的训练所得模型与目标的差异</li></ul></li><li><p>过拟合/欠拟合</p><ul><li>过拟合：为了得到一致假设而使假设变得过度严格（解决方法：模型剪枝/减少参数量/增加训练数据量/使用正则化约束）</li><li>欠拟合：模型没有很好地捕捉到数据特征，不能够很好地拟合数据（解决方法：模型复杂化/增加特征数/降低正则化约束）</li></ul></li><li><p>评估方法</p><ul><li>K折交叉验证法：<ul><li>你拿到一个数据集，先均分为k个互斥子集(k折)，依次将每一个子集作为测试集，剩余k-1个子集作为训练集，完成k次训练与测试，记录下每次的测试误差</li><li>进行完k次训练与测试后，对k个测试误差求均值。然后把均值测试误差最小的那个model选出来。它就是最优秀的model。</li><li>上述过程就是1次k折交叉验证。一般可以做p次，即p次k折交叉验证。<ul><li>细节：采样记得分层采样，即如果男女比例3:7，那么train,test子集里的男女比率都要为3:7。</li><li>留一法：若数据集有m个样本，那么就拆为m份，每次只有一个样本作为测试集，进行m次。</li></ul></li></ul></li><li>留出法：<ul><li>无脑。就是选一部分作为训练集，另一部分作为测试集。结束。</li></ul></li><li>自助法：<ul><li>若数据集<spanclass="math inline">\(D\)</span>有m个样本，那么进行m次有放回采样，每次采样到的样本copy到<spanclass="math inline">\(D_1\)</span>中。</li><li>采样m次后，将<spanclass="math inline">\(D_1\)</span>作为训练集，<spanclass="math inline">\(D/D_1\)</span>作为测试集，结束。</li><li>因为从概率上来讲，有<spanclass="math inline">\(\frac{1}{e}=36.8\%\)</span>样本一次都不会被采样过，所以<spanclass="math inline">\(D/D_1\)</span>作为测试集从概率上来讲就是<spanclass="math inline">\(36.8\%\)</span>的数据集。比较不错。<spanclass="math inline">\(\lim_{m\mapsto\infty}\left(1-\frac{1}{m}\right)^m\mapsto\frac{1}{e}\approx0.368\)</span></li></ul></li><li>优缺点：<ul><li>留出法简单易懂计算量小，但是结果可信度不算很高；K折交叉验证法可信度高，但计算量大；留出法在小数据集时表现较好，但是引入了估计偏差，但大数据集时非常明显。</li></ul></li></ul></li><li><p>性能度量1️⃣</p><ul><li><p>回归问题一般用MSE(均方误差)：<spanclass="math inline">\(E(f;D)={\frac{1}{m}}\sum_{i=1}^{m}\left(f\left(x_{i}\right)-y_{i}\right)^{2}\)</span></p></li><li><p>分类问题一般用错误率和精度。除此之外，还有P/R/F1/<spanclass="math inline">\(F_\beta\)</span></p><p><img src="image-20241206192619955.png" alt="" style="zoom: 50%;" /></p></li><li><p>正例(Positive sample)，反例(Negative sample)</p></li><li><p>查准率(P)：你查的正例里对了多少？<spanclass="math inline">\(\frac{TP}{TP + FP}\)</span></p></li><li><p>查全率(R)：正例都被查出来了吗？<spanclass="math inline">\(\frac{TP}{TP + FN}\)</span></p></li><li><p>F1：是P和R的调和平均。<span class="math inline">\(\frac{1}{F1} =\frac{1}{2}(\frac{1}{P} + \frac{1}{R})\)</span>。范围0 ~1，越靠近1越好</p></li><li><p><span class="math inline">\(F_\beta\)</span>：<spanclass="math inline">\(\frac{1}{F_\beta} = \frac{1}{1 +\beta^2}(\frac{1}{P} + \frac{\beta^2}{R})\)</span>。<spanclass="math inline">\(\beta&gt;1\)</span>时，认为较注重R指标</p></li><li><p>宏P、宏R、宏F1：就是做p次实验，然后宏P=<spanclass="math inline">\(\frac{1}{p}\sum P_i\)</span>，宏R=<spanclass="math inline">\(\frac{1}{p}\sumR_i\)</span>，宏F1是宏P和宏R的调和平均。</p></li><li><p>微P、微R、微F1：就是做p次实验，先得到<spanclass="math inline">\(\overline{TP}, \overline{FN}, \overline{FP},\overline{TN}\)</span>，然后算P、R、F1，就是微P微R微F1。</p></li><li><p>混淆矩阵：就是上面那个表</p></li></ul></li><li><p>性能度量2️⃣</p><ul><li><p>PR曲线：</p><p><img src="image-20241206195849226.png" alt="" style="zoom:67%;" /></p><ul><li>绘制过程：阈值从1降到0，不同阈值会得到不同的混淆矩阵，对于每个混淆矩阵计算P、R。然后标点上去。</li><li>评估好坏：如果把别人包住就比别人好，如果面积比别人大就比别人好，如果BEP点(P=R=BEP)比别人大就比别人好</li></ul></li><li><p>ROC曲线：</p><p><img src="image-20241206200121089.png" alt="" style="zoom:67%;" /></p><ul><li>绘制过程：阈值从1降到0，不同阈值会得到不同的混淆矩阵，对于每个混淆矩阵计算TPR、FRR。然后标点上去。</li><li>意义：TPR是正例中预测正确的比率，FPR是负类中预测错误的比率</li><li>评估好坏：如果把别人抱住就比别人好，如果面积比别人大就比别人好。</li></ul></li><li><p>AUC：area under curve，曲线下的面积</p></li><li><p>代价敏感错误率：就是原本计算错误率的时候，每个样本的权重是1。但是代价敏感错误率就是每个样本的权重自己去定。</p></li></ul></li><li><p>偏差和方差</p><ul><li><p>对于特征<span class="math inline">\(x_1, x_2, \cdots,x_m\)</span>，上帝视角存在标准映射：<spanclass="math inline">\(f(X)\)</span>。</p></li><li><p>那么在得到数据集的过程中，由于数据集是采样得到的，假设采样到的特征为<spanclass="math inline">\(x_1, x_2, \cdots,x_m\)</span>，采样得到的标签为<span class="math inline">\(y_1, y_2,\cdots, y_m\)</span>。因为采样存在噪声，所以有：<spanclass="math inline">\(y_i = f(x_i) + \varepsilon_i\)</span>。</p></li><li><p>噪声<spanclass="math inline">\(\varepsilon\)</span>是个随机变量，假设其<spanclass="math inline">\(E(\varepsilon) = 0, D(\varepsilon) =\sigma^2\)</span></p></li><li><p>假设我们训练得到的模型为<spanclass="math inline">\(\tilde{f}(X)\)</span>，模型的期望为<spanclass="math inline">\(E(\tilde{f}(X))\)</span>。</p></li><li><p>那么模型的泛化误差： <span class="math display">\[\begin{align*}E(Y - \tilde{f}(X)) &amp;= E(f(X) + \varepsilon - \tilde{f}(X)) \\&amp;= E(f(X) - E(\tilde{f}(X)) + E(\tilde{f}(X)) - \tilde{f}(X) +\varepsilon) \\&amp;= [f(X) - E(\tilde{f}(X))]^2 + E[\tilde{f}(X) - E(\tilde{f}(X))]^2+ E(\varepsilon^2) \\&amp;= \text{偏差} + \text{方差} + \sigma^2\text{(误差)}\end{align*}\]</span></p></li><li><p>可以看出，偏差就是标准标签与模型期望之间的差距；方差就是模型预测与其期望的偏离程度；误差就是一个定值。</p></li><li><p>与欠拟合/过拟合之间的关系，当欠拟合时，偏差很大（因为根本拟合不了标准标签），方差小（因为都没有拟合什么数据）；当过拟合时，偏差很小，但是方差变大（因为拟合了过多的噪声）。</p></li></ul></li><li><p>补充</p><ul><li>二项分布的共轭分布是 Beta 分布</li><li>多项式分布的共轭分布是 Dirichlet 分布</li></ul></li></ol><h3 id="线性模型">3 线性模型</h3><ol type="1"><li><p>线性回归</p><ul><li><p>线性回归的形式：<spanclass="math inline">\(f(\textbf{x})=\textbf{w}^\mathrm{T}\textbf{x} +b\)</span></p></li><li><p>基于MSE均方误差求解出线性回归模型的方法叫做最小二乘法。</p></li><li><p>求解目标，最小化<span class="math inline">\(F\)</span>： <spanclass="math display">\[F = \sum_{i=1}^{m}\left(f(x_i) - y_i\right)^2 = \sum_{i=1}^{m}\left(wx_i+ b - y_i\right)^2\]</span></p></li><li><p>求解方法（线性代数的方法）：</p><p><img src="91157626bd33416d4cff2cf0b5fa643.jpg" alt="" style="zoom: 15%;" /></p></li><li><p>最小二乘法的等效回归方法是线性均值和正态误差的最大似然回归。</p></li></ul></li><li><p>逻辑回归（对数几率回归）</p><ul><li><p>本质 loss 是用的最大似然估计，也可以说交叉熵。</p></li><li><p>就是把线性回归的结果，套一层sigmoid函数，使得输出的是一个0 ~1的值，可用来做二分类问题。</p></li><li><p>逻辑回归的形式：<spanclass="math inline">\(y={\frac{1}{1+e^{-(w^{\mathrm{T}}x+b)}}}\)</span></p></li><li><p>求解目标，最大化F：</p><p><img src="ac38e05ba9b7237f1617037b6e8b920.jpg" alt="" style="zoom: 33%;" /></p></li><li><p>求解方法：最大似然法</p></li></ul></li><li><p>线性判别分析（LDA）</p><ul><li>思想就是弄出一条线<spanclass="math inline">\(w^\mathrm{T}x\)</span>，使得同类投影到这条线的距离尽可能近，不同类投影到这条线距离尽可能远。</li><li>从数学上，就是最大化这个东西：<spanclass="math inline">\(J={\frac{w^{\textsf{T}}\mathrm{S}_{b}w}{w^{\mathrm{T}}\mathrm{S}_{w}w}}\)</span></li><li><span class="math inline">\(S_b\)</span>：类间散度矩阵</li><li><span class="math inline">\(S_w\)</span>：类内散度矩阵</li></ul></li><li><p>多分类方法</p><ul><li><p>OvO：两两配对产生<spanclass="math inline">\(\frac{N(N-1)}{2}\)</span>个结果，投票。（需要<spanclass="math inline">\(\frac{N(N-1)}{2}\)</span>个分类器）</p></li><li><p>OvR：依次将每个类作为正类，其余作为负类。若只有一个分类器输出正类，其余都为负类，则预测结果就为输出的那个正类。若有多个正类输出，选择置信度最大的类别标记。（需要<spanclass="math inline">\(N\)</span>个分类器）</p></li><li><p>MvM：</p><ul><li><p>MvM就是在每个分类器中，将若干个类作为正类，若干个类作为负类。MvM中，最常见的一种分类技术叫“纠错输出码"(ECOC)</p></li><li><p>具体看下面这幅图，一目了然。</p><p><img src="image-20241207002341592.png" /></p></li><li><p>上面这幅图中，一共有5个分类器，4个类别。第一个分类器<spanclass="math inline">\(f_1\)</span>规定<spanclass="math inline">\(C_2\)</span>为正类，<spanclass="math inline">\(C_1, C_3,C_4\)</span>为负类。然后对于测试示例，经过5个分类器，跑出了(-1, -1, +1,-1, +1)这个预测向量。记此向量为x。那么x与<spanclass="math inline">\(C_1\)</span>的向量(-1, +1, -1, +1,+1)的欧氏距离是<spanclass="math inline">\(2\sqrt{3}\)</span>，海明距离(即不同的个数)是3。</p><p>通过观察，可以发现预测向量与<spanclass="math inline">\(C_3\)</span>的欧氏距离和海明距离均最小，那么就判定该样例属于<spanclass="math inline">\(C_3\)</span>。</p><p>可以发现，EOOC编码越长(分类器越多)，那么纠错能力越强(鲁棒性越好)。</p><p>而且可以发现，两个类别<span class="math inline">\(C_i,C_j\)</span>的编码距离越远越好，这样子区分度就越高。所以我们称任意俩类别之间编码距离最远的编码方式为理论最优编码。</p></li></ul></li></ul></li><li><p>类别不平衡</p><ul><li>欠采样：直接丢弃过多类别的样本，使得不同类别的样本数均衡<ul><li>代表算法：EasyEnsemble</li></ul></li><li>过采样：增加过少类别的样本，使得不同类别的样本数均衡<ul><li>代表算法：SMOTE（通过对训练集过少类别的样本进行插值来生成新样本）</li><li>直接对初始过少类别样本进行重复采样会造成严重的过拟合现象</li></ul></li><li>再缩放：根据 <span class="math inline">\(\frac{m+}{m-}\)</span>对预测值进行调整</li></ul></li></ol><h3 id="决策树">4 决策树</h3><ul><li><p>不会受到数据归一化的影响。</p></li><li><p>决策树学习的关键在于如何选择最优划分属性，一般而言，随着划分过程不断进行，我们希望决策树的分支节点所包含的样本尽可能属于同一类别。</p></li><li><p>经典的属性划分方法有：信息增益ID3、增益率C4.5、基尼指数CART。</p></li><li><p>信息量：<span class="math inline">\(I(x) = -\logP(x)\)</span>。一个事件<spanclass="math inline">\(x\)</span>发送的概率越大，那么其蕴含的信息越少，即信息量越小</p></li><li><p>信息熵：随机变量的信息量的期望，<span class="math inline">\(H(X)= E(I(X)) = E(-\log P(X)) = -\sum p(x)\logp(x)\)</span>。如果随机变量蕴含的值越多，那么就越混乱，其信息熵就越大。</p></li><li><p>三种建树方法</p><ol type="1"><li><p>信息增益ID3</p><ul><li><p>先计算数据集<span class="math inline">\(D\)</span>得到信息熵<spanclass="math inline">\(H(D) = -\sum p_k \cdot \log p_k\)</span></p><ul><li><spanclass="math inline">\(p_k\)</span>是第k个label占全部label的比例。这个定义很好理解，如果全部样本都是同一个label，那么信息熵就是0。如果有很多个label，那么信息熵就比较大，说明混乱程序比较高，不确定性程度高。</li></ul></li><li><p>我们需要找到一个属性<spanclass="math inline">\(A\)</span>，使得<span class="math inline">\(G(D,A) = H(D) - H(D|A)\)</span>最大，即划分后的信息熵尽可能小。</p><ul><li><span class="math inline">\(G(D, A)\)</span>叫做信息增益</li><li><span class="math inline">\(H(D|A)\)</span>是用<spanclass="math inline">\(A\)</span>去划分数据集<spanclass="math inline">\(D\)</span>之后得到的信息熵：<spanclass="math inline">\(H(D | A) = \sum_{i=1}^{C} \frac{cnt_{D_i}}{cnt_D}\cdot H(D_i)\)</span></li></ul></li><li><p>划分后，对划分后新得到的数据集对相同的事情。递归结束条件为当前数据集label全一样，或者属性集空了，或者数据集在当前属性的label'都一样。</p></li><li><p>伪代码</p><p><img src="image-20241207182842913.png" alt="" style="zoom:67%;" /></p></li></ul></li><li><p>增益率C4.5</p><ul><li><p>上面的信息增益算法其实我们没考虑到一个东西，就是不同属性天生包含的类别数不同。即属性自身固有的“混乱”程度不一样。</p></li><li><p>直接给出数学定义，<span class="math inline">\(G(D,A)\)</span>是信息增益(Gain)，<span class="math inline">\(Gr(D,A)\)</span>是增益率(Gain ratio) <span class="math display">\[\begin{align*}&amp;G(D, A) = H(D) - H(D | A) \\&amp;Gr(D, A) = \frac{G(D, A)}{H(A)} \\&amp;H(A) = -\sum p_k \log p_k\end{align*}\]</span></p></li><li><p>所以C4.5算法就说：先选出信息增益高于平均信息增益的属性，然后再在这些属性中选出增益率最高的属性作为划分属性！基尼指数CART</p></li></ul></li><li><p>基尼指数CART</p><ul><li><p>除了用熵去衡量数据集“的纯度”，还可以用基尼指数去衡量数据集的纯度。</p></li><li><p>数据集<spanclass="math inline">\(D\)</span>的纯度可用基尼值(Gini)来衡量：</p></li></ul><p><span class="math display">\[Gn(D) = \sum_{i=1}^{C}\sum_{j=1}^{C}p_ip_j = 1 - \sum_{k=1}^{C}p_k^2,\quad i \ne j\]</span></p><ul><li><p>可以发现，基于指数反映了从<spanclass="math inline">\(D\)</span>中随机抽取两个样本，其类别lable不一致的概率。基尼系数越小，数据集纯度越高，即类别数越少。那么用属性A划分后的基尼指数是多少？<span class="math display">\[Gn(D, A) = \sum_{k=1}^{C} \frac{cnt_{D_i}}{cnt_D}Gn(D_i)\]</span></p></li><li><p>CART算法就是利用基尼指数来建决策树的算法，它每次看看用哪个属性划分后的基尼指数最小，就用它划分。</p></li><li><p>CART算法直接算的 <span class="math inline">\(Gn(D | A)\)</span>然后取的最小的那个。总之跟笔记写的不太一样，建议去看真题复习一下。</p></li></ul></li></ol></li><li><p>预剪枝、后剪枝</p><ul><li>因为决策树是个强方差的算法，所以很容易出现过拟合。为了解决过拟合，就有预剪枝和后剪枝两种方法。</li><li>预剪枝：<ul><li>就是在划分节点的时候，拿验证集跑一跑，如果划分后效果反而不如不划分，那么就不继续划分该节点了，直接连个叶子节点上去，类别就是该节点内人数最多的类别。（从上到下）</li><li>降低过拟合风险，训练速度快；但是有欠拟合风险</li><li>（此时引入了一个东西叫做“验证集”，他与测试集的区别主要是，测试集在训练的全程对于学习器来说是不可见的。而验证集的作用是辅助训练。）</li></ul></li><li>后剪枝：<ul><li>就是先用算法生成一棵决策树，然后从下到上依次考察是否将节点替换为叶子节点会更优，如果更优，就替换</li><li>比预剪枝保留了更多分支，欠拟合风险较小；但是训练时间大</li></ul></li></ul></li><li><p>连续与缺失值处理</p><ul><li>连续<ul><li>就拿身高这个属性举例，假设样本中，有n个不同的身高，那么划分点就有n个（就是先排序，然后俩俩身高的中点就是划分点），每确定一个划分点t，其实就可以算出按照t去划分身高这个属性之后得到的信息增益<spanclass="math inline">\(G(D, A,t)\)</span>。以用信息增益算法建树举例，那么划分身高这个属性的时候，就是找一个划分点t，使得<spanclass="math inline">\(H(D) - G(D, A, t)\)</span>最大即可。</li><li>需要注意的是，连续值与离散值有一个地方不同就是，离散值的属性如果用过，那么后面就不会再用来划分了。但是连续值的属性可以再次使用，比如第一次划分是身高是否低于180，进入子节点后可以继续用身高这个属性划分，身高是否低于160。</li></ul></li><li>缺失值<ul><li>最简单的方法当然就是丢弃有缺失值的样本，或者用众数/平均数去填充缺失值。但是这么做有些暴力。</li><li>比较复杂的方法就是划分建树过程中，给每个样本一个全局变量<spanclass="math inline">\(w_i\)</span>，为自己的权重，初始为1。对于那些属性<spanclass="math inline">\(A\)</span>无缺失的样本呢，直接划分到对应子集中，对应进入的权重为<spanclass="math inline">\(w_i\)</span>；对于那些属性<spanclass="math inline">\(A\)</span>缺失的样本，就等无缺失的样本都划分完后，然后划分到每一个子集中，对应进入的权重变为<spanclass="math inline">\(w_i \cdot\frac{\sum\text{子集中样本的}w_i}{\sum\text{当前属性无缺失的样本的}w_i}\)</span>。同时将<spanclass="math inline">\(\frac{\sum\text{子集中样本的}w_i}{\sum\text{当前属性无缺失的样本的}w_i}\)</span>记录下来作为该子节点的权重。</li><li>在验证的时候，就给每一个要验证的样本带一个权重<spanclass="math inline">\(w\)</span>，初始值为1。假设走到某个属性A，若该样本在属性A上无缺失，则进入到对应子节点，权重<spanclass="math inline">\(w\)</span>不变；若有缺失，则每个子集都进入，但是进入的权重要乘对应子节点的权重。那么最终若进入到多个叶子节点，选择最终权重最大的那个叶子节点的类别判定为该样本的label。</li></ul></li></ul></li><li><p>多变量决策树</p><ul><li><p>传统的单变量决策树，其决策边界都是与轴平行的：</p><p><img src="image-20241207192319312.png" alt="" style="zoom:50%;" /></p></li><li><p>但如果我决策树的节点换为多变量的，那么我的决策边界就可以变为线性，更为灵活：</p><p><img src="image-20241207192436420.png" alt="" style="zoom:50%;" /></p></li></ul></li></ul><h3 id="神经网络">5 神经网络</h3><ol type="1"><li><p>几个网络拓扑的概念</p><ul><li><p>单层感知机</p><p><img src="image-20241207010044315.png" alt="" style="zoom: 80%;" /></p><ul><li>只有输入层和输出层，输出层是M-P神经元，也称为阈值逻辑单元，负责加一个偏置项和经过激活函数，得到输出。</li><li>激活函数有：sgn阶跃函数，sigmoid函数</li><li>（单层感知机是线性模式，但神经网络以及多层感知机不是）</li></ul></li><li><p>多层感知机(MLP)</p><p><img src="image-20241207010250215.png" alt="" style="zoom:80%;" /></p><ul><li>多层感知机（MLP）是一种前向结构的人工神经网络，包含输入层、输出层及多个隐藏层。除了输入层，隐藏层和输出层的每个神经元都有加一个偏置项和经过激活函数的功能。</li></ul></li><li><p>多层前馈神经网络</p><p><img src="image-20241207011345869.png" alt="" style="zoom: 80%;" /></p><ul><li>定义：每层神经元与下一层神经元全互联，神经元之间不存在同层连接也不存在跨层连接。</li><li>前馈：网络拓扑结构不存在环或回路。</li><li>前向传播：从输入层开始，将上一层的输出作为下一层的输入，并计算下一层的输出，一直到运算到输出层为止</li></ul></li></ul></li><li><p>反向传播BP</p><ul><li><p>西瓜书上的例子和手推过程：</p><p><imgsrc="3a5ef9c69c1f5d4ecc439ec709f3273-1733563383350-7.jpg" /></p></li></ul></li><li><p>几个其它概念</p><ul><li><p>解决BP神经网络过拟合手段</p><ul><li><p>早停：将数据集分为训练集和验证集，当训练集误差降低但验证集误差升高时，停止训练。</p></li><li><p>正则化：就是在目标误差函数中加一项用于描述网络复杂度的部分，例如：<span class="math display">\[E=\lambda{\frac{1}{m}}\sum_{k=1}^{m}E_{k}+(1-\lambda)\sum_{i}w_{i}^{2}\]</span></p></li><li><p><spanclass="math inline">\(E_k\)</span>为第k个训练样例上的误差。这么搞的话网络将会偏好较小的权重<spanclass="math inline">\(w_i\)</span>，从而使网络输出更为“光滑”，缓解过拟合现象。</p></li><li><p>L2正则化：对绝对值较大的给予较重乘法，且处处可导，方便计算</p></li><li><p>L1正则化：对所有权重基于相同力度乘法，因此较小权重乘法后就变为0了，从而达到稀疏化的目的。</p></li></ul></li><li><p>全局最小与局部极小</p><ul><li>全局最小：在函数的整个定义域中，如果一个点的函数值是所有可行点中最小的，那么这个点就是一个全局最小点、</li><li>局部极小：在函数的定义域内的某个区域中，如果一个点的函数值不大于其邻近点的函数值，那么这个点就是一个局部极小点。</li></ul></li><li><p>梯度爆炸：梯度由于误差累计变得非常大，导致网络权重大幅更新甚至权重值溢出</p></li><li><p>梯度消失（sigmoid /tanh，因为它们的导数图像都形如正态分布）：梯度非常小甚至趋于0，导致网络训练不佳甚至无法训练。</p></li></ul></li></ol><h3 id="支持向量机">6 支持向量机</h3><ol type="1"><li><p>几个定义</p><ul><li><p>超平面：n维线性空间中维度为n-1的子空间，它可以把n维线性空间分割为不相交的两部分</p></li><li><p>支持向量：距离超平面最近的且满足一定条件的几个训练样本点</p></li><li><p>间隔：两个异类支持向量到超平面的距离之和</p></li><li><p>SVM原理：m个样本分为两类，每个样本的数据维度为n维，然后我们需要找出一个n-1维的超平面，来区别这两类样本，使得间隔最大。</p></li><li><p>SVM求解目标推导：</p><p><img src="5a3e5bc7c09017ce20fa869b07f577c.jpg" alt="" style="zoom: 50%;" /></p></li></ul></li><li><p>核函数</p><ul><li>我们知道，升维可以使得原本不可分的数据变得可分。但是通过维度转换函数去升维很困难，因为维度转换函数很难找。</li><li>而通过前面的分析，原优化问题的对偶问题的最优解仅由支持向量的点积结果决定。</li><li>而核函数的功能就是得到转换后空间中向量点积。所以，我们只需找到一个恰当的核函数即可。</li><li>核函数定义：将原始空间中的向量作为输入向量，并返回转换后的数据空间中向量的点积的函数称为核函数。</li><li>常见的核函数：线性核、多项式核、高斯核、拉普拉斯核、sigmoid核</li></ul></li><li><p>软间隔</p><p><img src="image-20241208000557083.png" alt="" style="zoom: 67%;" /></p><ul><li>软间隔 SVM的阈值趋于无穷，则只要最佳分类超平面存在，它就能将所有数据正确分类。</li></ul></li><li><p>支持向量回归（SVR）</p><p><img src="image-20241208001845615.png" alt="" style="zoom:50%;" /></p></li></ol><h3 id="贝叶斯分类器">7 贝叶斯分类器</h3><ol type="1"><li><p>贝叶斯决策论</p><ul><li><p>是个理论框架，不是一个实际的模型。</p><p><img src="image-20241208165450045.png" alt="" style="zoom: 50%;" /></p></li><li><p>其实，就是拿到一个样本<spanclass="math inline">\(x\)</span>，然后对于所有类别<spanclass="math inline">\(c \in \mathcal{Y}\)</span>，计算出最小的那个<spanclass="math inline">\(R(c | x)\)</span>，对应的<spanclass="math inline">\(c\)</span>就是<spanclass="math inline">\(x\)</span>分到的类别。</p></li><li><p>如何计算条件风险<span class="math inline">\(R(c |x)\)</span>呢？如图公式即可，但是容易发现，<spanclass="math inline">\(P(c_j |x)\)</span>这个概率我们是不知道的。</p></li><li><p>我们的机器学习，其实本质上就是在求<spanclass="math inline">\(P(c_j | x)\)</span>。</p></li><li><p>所以这是个理论框架，它反应了学习性能的理论上限。</p></li></ul></li><li><p>先验/后验</p><ul><li>先验概率就是通过历史经验来确定事件。</li><li>后验概率就是通过结果来推测原因。</li><li>贝叶斯公式：<span class="math inline">\(P(c | x) = \frac{P(c) \cdotP(x | c)}{P(x)}\)</span></li><li>P(c)是先验，P(x|c)是条件概率(或者叫似然)，P(x)叫证据。</li></ul></li><li><p>生成式/判别式模型</p><ul><li><p>判别式模型：直接对 P(c|x)建模（SVM、神经网络、决策树）</p></li><li><p>生成式模型：对 P(x, c) 建模（贝叶斯分类器）</p></li><li><p>从前面的知识可以知道，难点就是在于求条件概率 <spanclass="math inline">\(P(x|c)\)</span>。所以历史上就出现了两派：频率主义学派和贝叶斯学派。频率主义学派就认为<span class="math inline">\(P(x | c)\)</span>潜在的是服从某种分布的。所以我们只需要根据现有的数据不断去估计条件概率，从而求出后验。</p></li><li><p>频率主义学派/贝叶斯学派 和前面的 判别式生成式模型没有必然联系。两个概念。机器学习这门课我看到的算法，按照学派分类，应该都是频率主义学派。但是按照什么式来分类，大部分是判别式，小部分是生成式。</p></li><li><p>举个例子：</p><p><img src="image-20241217001634379.png" alt="" style="zoom: 67%;" /></p><p><img src="image-20241217001700091.png" alt="" style="zoom:67%;" /></p></li></ul></li><li><p>极大似然估计</p><ul><li>也是一种理论框架，不是实际的模型。没有考虑先验分布。</li><li>对于我们不是要求<spanclass="math inline">\(P(x|c)\)</span>嘛，即要求<spanclass="math inline">\(P(x|c)\)</span>满足什么分布。不妨假设<spanclass="math inline">\(P(x|c)\)</span>具有确定的概率分布形式，由<spanclass="math inline">\(\theta_c\)</span>唯一确定。所以我们的任务就是利用数据集<spanclass="math inline">\(D\)</span>求出<spanclass="math inline">\(\theta_C\)</span>。</li><li>那么<spanclass="math inline">\(D\)</span>中所有类别为c的样本出现的概率：<spanclass="math inline">\(\prod_{x \in D_c} P(x | \theta_c)\)</span></li><li>我们就是找到一个<spanclass="math inline">\(\theta_c\)</span>，使得上面这个概率最大。这就是最大似然。</li><li>为了方便，取个log：<span class="math inline">\(f = \sum_{x \in D_c}\log P(x | \theta_c)\)</span>。</li><li>注意，上面的参数<spanclass="math inline">\(\theta_c\)</span>是指标针对<spanclass="math inline">\(D_c\)</span>的，每类数据集合的参数不一样。</li></ul></li><li><p>朴素贝叶斯分类器</p><ul><li><p>基于先验推后验，可解决有监督学习问题。</p></li><li><p>由贝叶斯公式，得：<span class="math inline">\(P(c|x) = \frac{P(c)\cdot P(x | c)}{P(x)}\)</span>。</p></li><li><p>不妨假设<spanclass="math inline">\(x\)</span>的各个属性相互独立，且若为属性连续则假设<spanclass="math inline">\(P(x_i | c)\)</span>满足正态分布。</p></li><li><p>那么可得：<span class="math inline">\(P(c|x) =\frac{P(c)}{P(x)}\prod_{i=1}^d P(x_i | c)\)</span></p></li><li><p>对于不同类别<span class="math inline">\(c\)</span>，<spanclass="math inline">\(P(x)\)</span>一样，所以只需要计算<spanclass="math inline">\(P(c) \cdot \prod_{i=1}^dP(x_i|c)\)</span>谁大就行，最大的对应的c就是其类别。</p></li><li><p>这里一定要用连乘噢！回去看看真题上的例子复习一下，易错。</p></li><li><p>下面是一个计算的例子：</p><p><img src="image-20241208173142846.png" /></p></li></ul></li><li><p>朴素贝叶斯分类器的改进</p><ul><li>因为朴素贝叶斯分类器的假设过于强，所以半朴素贝叶斯就是说对这个假设进行一定程序的放松。经典算法有：SPODE、TAN、AODE。</li><li>贝叶斯网：借助DAG来描述属性之间的依赖关系。</li><li>EM算法：我们知道，知道了数据概率，可以去估计背后的数据分布；知道了数据分布，可以推测数据概率。所以EM算法中的E就是去估计样本所属类别的概率，M就是用估计的分类来更新分布的参数。循环往复，蛋生鸡鸡生蛋，直到分布收敛。<ul><li>经典的无监督分类模型。</li><li>对初始化敏感。</li></ul></li></ul></li></ol><h3 id="集成学习">8 集成学习</h3><ol type="1"><li><p>好而不同</p><ul><li>要获得好的集成，个体学习器应该“好而不同”。个体学习器性能不能太坏（至少50%正确率），且学习器之间要有差异。</li><li>但是“准确性”与“多样性”之间存在冲突。准确性增高后就要牺牲掉一些多样性。</li></ul></li><li><p>两类集成学习方法</p><ul><li><p>个体学习器存在强依赖关系（序列化方法）</p><ul><li><p>Boosting框架（AdaBoost）</p><p><img src="image-20241208224231596.png" alt="" style="zoom: 67%;" /></p></li><li><p>上图就是Boosting框架的思路：对于数据集1里的每个样本有一个权重，初始都一样。然后通过基学习算法得到一个模型1。模型1做错的样本加大其权重；做对的样本减小其权重。然后从数据集1中采样得到数据集2。再做相同的事情，以此类推。直到得到T个模型。最后将这T个弱分类器加权线性组合为强分类器。</p></li></ul></li><li><p>个体学习器不存在强依赖关系（并行化方法）‘’</p><ul><li><p>Bagging框架（Random forest）</p><p><img src="image-20241208225104052.png" alt="" style="zoom: 50%;" /></p></li><li><p>对数据集进行n次自助采样，得到n个新的数据集。对于每个数据集训练用基学习算法训练出一个模型。对于分类任务，通常采用多数投票或平均概率决定最终类别；对于回归任务，采用平均值作为集成模型的预测结果。</p></li><li><p>算法伪代码：</p><p><img src="image-20241208230513550.png" alt="" style="zoom:67%;" /></p><ul><li><spanclass="math inline">\(\mathcal{D_{bs}}\)</span>是自助采样出来的数据集。</li></ul></li></ul></li><li><p>随机森林</p><ul><li>如果以决策树算法作为基学习算法的集成学习就叫做随机森林。</li><li>随机森林的基学习器之间的差异由两方面带来，第一来自样本扰动（自助采样带来的训练集不一样），第二是属性扰动（决策树中的每个节点划分时只考虑一个属性子集）。</li></ul></li></ul></li><li><p>结合策略</p><ul><li>平均法、投票法</li><li>学习法<ul><li>先从初始数据集中训练出初级学习器，然后“生成”一个新数据集用于训练次级学习器。在新数据集中，经过初级学习器的输出被当做输入特征，初始样本的标记仍被当做样本标记。这样去训练好一个次级学习器。然后对于预测一个样本，先得到各个初级学习器的输出，然后把这个输出丢进次级学习器中，得到预测标签输出。</li></ul></li></ul></li><li><p>多样性增强</p><ul><li>我们知道集成学习的性能由准确率和多样性决定。所以提升多样性的方法有如下几种：<ul><li>数据样本扰动（例如自助采样）</li><li>输入属性扰动（例如随机森林）</li><li>输出表示扰动（例如利用ECOC将多分类输出转换为一系列二分类输出）</li><li>算法参数扰动（就基学习算法的参数进行设置，产生差异较大的基学习器）</li></ul></li></ul></li></ol><h3 id="聚类">9 聚类</h3><ol type="1"><li><p>概念：聚类是一种无监督算法。它将数据集中的样本划分为若干个不相交的子集，称为簇。</p></li><li><p>性能度量</p><ul><li><p>目的：评估聚类的好坏、确定优化的目标</p></li><li><p>结论：簇内相似度越高越好，簇间相似度越低越好</p></li><li><p>外部指标（与标准聚类模型效果对比）：</p><p><img src="image-20241209004435722.png" alt="" style="zoom:67%;" /></p><ul><li>这仨指标都是[0, 1]，越高越好</li></ul></li><li><p>内部指标：</p><p><img src="image-20241209004608662.png" alt="" style="zoom:67%;" /></p><ul><li><p>avg(C)是类内的平均距离。diam(C)是类内的最大距离。dmin(C1,C2)是俩类间的最小距离。dcen(C1, C2)是俩类间的中心之间的距离。</p><p><img src="image-20241209004738727.png" alt="" style="zoom:67%;" /></p><p><img src="image-20241209004828626.png" alt="" style="zoom: 50%;" /></p></li></ul></li></ul></li><li><p>距离计算</p><ul><li><p>前面我们用了一些性能指标去度量聚类的好坏，其中用到了“距离“。那么距离也需要一个度量方法，才能够进行计算。</p></li><li><p>距离度量满足直递性<img src="image-20241209005125739.png" alt="" style="zoom: 50%;" />，非距离度量不满足（例如相似度）</p></li><li><p>当属性为有序的时候，可以用闵可夫斯基距离（下图），无序属性可采用VDM距离。</p><p><img src="image-20241209005254974.png" alt="" style="zoom:50%;" /></p></li></ul></li><li><p>K-means</p><ul><li>算法描述：<ol type="1"><li>随机选取样本作为初始均值向量（初始值：k 的值【即几个簇】）</li><li>分别计算每个样本点到均值向量的距离，距离哪个近就属于哪簇</li><li>通过2计算出来的划分，重新计算均值向量（直接对簇内点取平均）</li><li>重复直到达到停止指标</li></ol></li><li>仔细看图片，很清晰了</li></ul><p><img src="image-20241209010527965.png" alt="" style="zoom:80%;" /></p><ul><li>优点就是简单快速。缺点事先得确定k值且对初始值敏感，对孤立点敏感。</li></ul></li><li><p>其它一些方法：</p><ul><li>学习向量量化：是监督学习，知道了每个样本的标签。这个算法返回的是每个簇最终的原型向量。每个类别的原型向量不是简单的均值向量，而是考虑了附近同/异类点的影响。</li><li>高斯混合聚类：采用概率模型来表达聚类原型。</li><li>密度聚类（DBSCAN）：基于密度的聚类，假设聚类结构能通过样本分布的紧密程序来决定。</li><li>层次聚类（AGNES）：试图在不同层次对数据集进行划分，从而形成树形的聚类结构。（自底向上真题上有一道例题，要去复习，易错！）</li></ul></li></ol><h3 id="降维与度量学习">10 降维与度量学习</h3><ol type="1"><li><p>KNN</p><ul><li>这里的k是指要参考k个与自己最近的点，k-means里的k是簇的个数。</li><li>（最小距离分类器算法：通过求出未知类别向量 X到事先已知的各类别（如A，B，C 等）中心向量的距离 D，然后将待分类的向量 X归结为这些距离中最小的那一类的分类方法）</li><li>（k近邻属于分类算法，样本多且典型性不好容易造成分类错误，样本分布对其影响不大。但是样本分布对聚类算法的影响较大）</li><li>它是一个监督学习。需要很多有标签的数据。这样新数据来的时候才能做预测。</li><li>算法流程描述：<ol type="1"><li>计算测试数据与各个训练数据之间的距离</li><li>对距离从小到大进行排序</li><li>选取距离最小的k个点</li><li>确定前k个点类别出现概率</li><li>出现概率最高的类别作为预测分类</li></ol></li></ul></li><li><p>维数灾难：数据属性维数过高，出现数据样本系数、距离因为维数过高从而计算困难的问题，称为维数灾难。解决方法——降维。</p></li><li><p>降维方法：</p><ul><li><p>PCA主成分分析（线性）</p><ul><li><p>思想：简单来说就是第一阶段找了一个新的坐标系来表示数据，这个新的坐标系能最大限度的看出每个轴上的数据变化大小，第二阶段在新坐标系下取前k个变化最大的轴上的数据，从而实现降维。</p></li><li><p>算法伪代码：</p><p><img src="image-20241209022227832.png" alt="" style="zoom:80%;" /></p><ul><li>将数据投影到W坐标系下即可：<span class="math inline">\(X&#39; =WX\)</span></li></ul></li></ul></li><li><p>核化线性降维：对于线性不可分数据，我们需要先利用核技巧先升维。然后再利用PCA进行降维。</p></li><li><p>流形学习：借鉴了拓扑流形概念的降维方法。</p></li></ul></li><li><p>度量学习</p><p><img src="image-20241216215310297.png" /></p></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;看学校ppt + 西瓜书的一些总结&lt;/p&gt;</summary>
    
    
    
    <category term="4. 大学" scheme="http://error666.top/categories/4-%E5%A4%A7%E5%AD%A6/"/>
    
    <category term="计算机专业课" scheme="http://error666.top/categories/4-%E5%A4%A7%E5%AD%A6/%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%93%E4%B8%9A%E8%AF%BE/"/>
    
    
  </entry>
  
  <entry>
    <title>RL代码学习框架</title>
    <link href="http://error666.top/2024/10/09/RL%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/"/>
    <id>http://error666.top/2024/10/09/RL%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/</id>
    <published>2024-10-08T18:07:09.000Z</published>
    <updated>2024-10-09T13:04:34.441Z</updated>
    
    <content type="html"><![CDATA[<p>为巩固RL算法的掌握程度，所以我打算写一个学习框架。测试自己写的算法是否正确以及观察算法的各种指标。既巩固了所学理论，又弥补了实战编程经验的不足，也会以后算法创新的仿真测试做了铺垫。所以我认为写这么一个学习框架性价比很高。</p><span id="more"></span><p>框架正在开发中，点击<ahref="https://github.com/potatoQi/Grid_World">链接</a>前往。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;为巩固RL算法的掌握程度，所以我打算写一个学习框架。测试自己写的算法是否正确以及观察算法的各种指标。既巩固了所学理论，又弥补了实战编程经验的不足，也会以后算法创新的仿真测试做了铺垫。所以我认为写这么一个学习框架性价比很高。&lt;/p&gt;</summary>
    
    
    
    <category term="1. 科研" scheme="http://error666.top/categories/1-%E7%A7%91%E7%A0%94/"/>
    
    <category term="强化学习" scheme="http://error666.top/categories/1-%E7%A7%91%E7%A0%94/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>强化学习3</title>
    <link href="http://error666.top/2024/10/07/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A03/"/>
    <id>http://error666.top/2024/10/07/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A03/</id>
    <published>2024-10-07T15:30:16.000Z</published>
    <updated>2024-10-10T16:40:37.837Z</updated>
    
    <content type="html"><![CDATA[<p>值函数近似(DQN)、策略梯度方法(REINFORCE)、Actor-Critic方法</p><span id="more"></span><h3 id="值函数近似">值函数近似</h3><h4 id="引入">引入</h4><p>上一章介绍了时序差分方法，也就是model-free下求state/actionvalue/optimal action value的迭代法。</p><p>其思路是在MC ExploringStarts上演变过来的，即完全的记忆化。既然使用了记忆化，那么就有记忆化数组，即tabular。</p><p>这一章的值函数近似不是基于tabular，而是基于函数。这是有实际意义的，比如actionvalue是连续的时候，离散化后用tabular存就很有可能存不下，所以我们需要一个连续的算法。</p><p>从另一个方面考虑，假如state-actionpair太多太多，那么我很难把全部的state-actionpair都估计到，那么假如我们有一个函数，那么无论你状态有多少个，因为我是表达式，所以随便给一个状态我都能代入表达式估计出来。</p><p>想到啥了吗？用函数拟合任意一个散点图，是的，神经网络最喜欢干这件事了。</p><p>Interesting, right？</p><h4 id="目标函数">目标函数</h4><p>我们的目标就是通过拟合的方法估计<spanclass="math inline">\(v_\pi(s)\)</span>嘛，所以<spanclass="math inline">\(v_\pi(s)\)</span>是真实的值，我们拟合的函数是<spanclass="math inline">\(\hat{v}(s, w)\)</span>。</p><p>注意这是个函数哦，<spanclass="math inline">\(s\)</span>是自变量，<spanclass="math inline">\(w\)</span>是参数。</p><p>显然我们的优化函数为： <span class="math display">\[J(w) = \mathbb{E}[(v_\pi(S) - \hat{v}(S, w))^2]\]</span> 显然，我们希望<spanclass="math inline">\(J(w)\)</span>尽可能小。</p><p>这个优化函数是标准形式，但实际计算的时候我们需要将里面的随机变量和期望替换为样本。</p><h4 id="优化算法和函数设计">优化算法和函数设计</h4><p>前面我们有了目标函数，那么现在我们就来minimize <spanclass="math inline">\(J(w)\)</span>.</p><p>来计算下<span class="math inline">\(J(w)\)</span>的导数： <spanclass="math display">\[\begin{align*}\nabla J(w) &amp;= \nabla \mathbb{E}[(v_\pi(S) - \hat v(S, w))^2] \\            &amp;= \mathbb{E}[\nabla (v_\pi(S) - \hat v(S, w))^2] \\            &amp;= 2\mathbb{E}[(v_\pi(S) - \hat v(S, w))(-\nabla_w \hatv(S, w))] \\            &amp;= -2\mathbb{E}[(v_\pi(S) - \hat v(S, w))\nabla_w \hatv(S, w)]\end{align*}\]</span> 当然，用SGD即可，那么迭代式为： <span class="math display">\[w_{t+1} = w_t + \alpha_t(v_\pi(s_t) - \hat v(s_t, w_t))\nabla_w \hatv(s_t, w_t)\]</span> 可以看出，上面这个迭代式是没法用的。因为<spanclass="math inline">\(v_\pi(s_t)\)</span>我们不知道啊。</p><p>所以可以结合MC或者TD algorithm，这里我就结合TDalgorithm其中的迭代式： <span class="math display">\[v_{t+1}(s_t) = v_t(s_t) - \alpha_t(s_t)[v_t(s_t) - [r_{t+1} + \gammav_t(s_{t+1})]]\]</span> 那么上面的迭代式可以改写为： <span class="math display">\[w_{t+1} = w_t + \alpha_t(r_{t+1} + \gamma \hat v(s_{t+1}, w_t) - \hatv(s_t, w_t)) \cdot \nabla_w \hat v(s_t, w_t)\]</span> 那么我们可以得到TD + 值函数近似算法： <spanclass="math display">\[\begin{align*}&amp;\textbf{Initialization: } \text{A function $\hat v(s, w)$ that is adifferentiable in $w$. Initial parameter $w_0$.} \\&amp;\text{For each episode generated following the polticy $\pi$, do}\\&amp;\quad\quad \text{For each step $(s_t, r_{t+1}, s_{t+1})$, do} \\&amp;\quad\quad\quad\quad w_{t+1} = w_t + \alpha_t[r_{t+1} + \gamma \hatv(s_{t+1}, w_t) - \hat v(s_t, w_t)] \cdot \nabla_w \hat v(s_t, w_t)\end{align*}\]</span> 简洁而优美。</p><p>好了，就剩一件事了。<span class="math inline">\(\hat v(s,w)\)</span>这个函数如何设计？</p><p>一般现在有两种设计方案，第一种是设计为线性的，第二种是用神经网络去拟合。</p><p>先来看第一种：</p><p><span class="math inline">\(\hat v(s, w) =\phi(s)^\mathrm{T}w\)</span>，其中<spanclass="math inline">\(\phi\)</span>是特征<spanclass="math inline">\(s\)</span>的特征向量。</p><p>如何理解呢？就是你对于一个状态的value，看你想用什么特征来描述它，假设你用特征<spanclass="math inline">\(a_1, a_2, a_3, 1\)</span>来描述一个状态<spanclass="math inline">\(s\)</span>，那么其特征向量就为<spanclass="math inline">\([a_1, a_2, a_3,1]^{\mathrm{T}}\)</span>，那么函数就为：<span class="math inline">\(\hatv(s,w) = a_1w_1 + a_2w_2 + a_3w_3 + w_4\)</span>。</p><p>所以这种方法的关键就是选好特征很关键。举个例子，比如想描述某人某时刻的state，那么特征就可以选择：身高、体重、性别。</p><p>再来看第二种：略，神经网络没啥数学推导，这里没必要再展开。</p><p>相同的，我们还可以得到Sarsa + 值函数近似算法： <spanclass="math display">\[w_{t+1} = w_t + \alpha_t[r_{t+1} + \gamma \hat q(s_{t+1}, a_{t+1}, w_t)- \hat q(s_t, a_t, w_t)] \cdot \nabla_w \hat q(s_t, a_t, w_t)\]</span> 相同的，我们还可以得到Q-learning + 值函数近似算法： <spanclass="math display">\[w_{t+1} = w_t + \alpha_t[r_{t+1} + \gamma \max_{a \in\mathcal{A(s_{t+1})}} \hat q(s_{t+1}, a, w_t) - \hat q(s_t, a_t, w_t)]\cdot \nabla_w \hat q(s_t, a_t, w_t)\]</span></p><h4 id="deep-q-learning">Deep Q-learning</h4><p>Deep Q-learning，也可以叫deep Q-network，DQN。</p><p>此方法将DL那一套搬了过来，而且效果还很好。<del>（这世界的本质难道真的是无限拟合？）</del></p><p>Deep Q-learning就是在Q-learning + 值函数近似的基础上，<spanclass="math inline">\(\hat q(s_t, a_t,w_t)\)</span>用神经网络去算的一个算法。</p><p>回顾一下一下optimal bellman equation：</p><p><span class="math display">\[q(s,a) = \mathbb{E}\left[ R_{t+1} + \gamma \max_{a \in\mathcal{A}(S_{t+1})} q(S_{t+1}, a) | S_t = s, A_t = a \right], \foralls, a\]</span> 其实Q-learning本质就是在用迭代法使得<spanclass="math inline">\(q_t(s_t, a_t) \to r_{t+1} + \gamma \max_{a \in\mathcal{A}} q_t(s_t,a)\)</span></p><p>当用神经网络来拟合actionvalues时，它与Q-learning一样，本质是在minimize这个函数： <spanclass="math display">\[J(w) = \mathbb{E}\left[ (R + \gamma \max_{a \in \mathcal{A}(S&#39;)}\hat q(S&#39;, a, w) - \hat q(S, A, w))^2 \right]\]</span> 但是<span class="math inline">\(J(w)\)</span>这个函数对<spanclass="math inline">\(w\)</span>的梯度很难求，因为第二项和第三项都包含了<spanclass="math inline">\(w\)</span>。所以这里原文作者用了一个trick。就是设置了两个<spanclass="math inline">\(w\)</span>，一个叫<spanclass="math inline">\(w\)</span>，一个叫<spanclass="math inline">\(w_T\)</span>。<spanclass="math inline">\(w\)</span>是持续更新的，<spanclass="math inline">\(w_T\)</span>是各种一段时间更新一次的。那么，loss函数可以写为下面这种形式：<span class="math display">\[J(w) = \mathbb{E}\left[ (R + \gamma \max_{a \in \mathcal{A}(S&#39;)}\hat q(S&#39;, a, w_\mathrm{T}) - \hat q(S, A, w))^2 \right]\]</span> 这样的话，<spanclass="math inline">\(w_\mathrm{T}\)</span>就是个常数，那么<spanclass="math inline">\(\nabla_w J(w)\)</span>就可以写出来了： <spanclass="math display">\[\nabla_w J(w) = -2 \mathbb{E}\left[ (R + \gamma \max_{a \in\mathcal{A}(S&#39;)} \hat q(S&#39;, a, w_\mathrm{T}) - \hat q(S, A, w))\cdot \nabla_w \hat q(S, A, w) \right]\]</span>然后既然都用神经网络了，那么全部的思路都转换为深度学习。现在有了目标函数，梯度，就差数据了。</p><p>这里的数据就是很多<span class="math inline">\((s, a, r,s&#39;)\)</span> pairs。</p><p>那么可以写出下列算法： <span class="math display">\[\begin{align*}&amp;\text{Store the experience samples generated by $\pi_b$ in a replaybuffer $\mathcal{B} = \{(s,a,r,s&#39;)\}$} \\&amp;\quad\quad \text{For each iteration, do} \\&amp;\quad\quad\quad\quad \text{Uniformly draw a mini-batch of samplesfrom $\mathcal{B}$} \\&amp;\quad\quad\quad\quad \text{For each sample $(s,a,r,s&#39;)$,calculate the target values as $y_{\mathrm{T}} = r + \gamma \max_{a \in\mathcal{A}(s&#39;)}\hat q(s&#39;,a,w_{\mathrm{T}})$, where} \\&amp;\quad\quad\quad\quad \text{$w_\mathrm{T}$ is the parameter of thetarget network} \\&amp;\quad\quad\quad\quad \text{Update the main network to minimize$(y_\mathrm{T} - \hat q(s,a,w))^2$ using the mini-batch$\{(s,a,y_\mathrm{T})\}$} \\&amp;\quad\quad \text{Set $w_\mathrm{T} = w$ every $C$ iterations}\end{align*}\]</span> 思考累了？换种角度重新看看DQN，会发现很简单。</p><p>首先，它有很多样本，每个样本会得到一个输出<spanclass="math inline">\(y_\mathrm{T}\)</span>（通过optimal bellmanequation得到的输出），我们的目的，就是让我们的网络<spanclass="math inline">\(\hat q(s,a,w)\)</span>（其中<spanclass="math inline">\(s,a\)</span>是输入，<spanclass="math inline">\(w\)</span>是模型参数）尽可能拟合所以样本的<spanclass="math inline">\(y_\mathrm{T}\)</span>。所以就是个简单的深度学习问题。</p><h3 id="策略梯度方法">策略梯度方法</h3><h4 id="引入-1">引入</h4><p>其实就是用连续函数去直接拟合policy，而非像以前那样关注中间量statevalues、action values。</p><p>具体来说，即通常用神经网络去拟合一个函数<spanclass="math inline">\(\pi(a | s, \theta)\)</span>，其中<spanclass="math inline">\(\theta\)</span>是网络参数。</p><p>那如何评价我们拟合的这个<span class="math inline">\(\pi(a | s,\theta)\)</span>是否好坏呢？</p><p>所以我们需要一个指标<spanclass="math inline">\(J(\theta)\)</span>，我们的任务，就是通过大量经验(样本)，去训练拟合这些样本，从而改变<spanclass="math inline">\(\theta\)</span>，去maximize这个指标。</p><p>Interesting，越来越像深度学习的感觉了。</p><p>回顾一下，RL从MDP开始，MDP就是建立在标准的数学动态规划、矩阵论、概率论上的数学框架。解决RL问题就是在解决这个数学框架。求解方法有迭代法或者直接解方程。</p><p>后面因为概率很难提前获得，也就是我们通常不能开“上帝视角”，所以解数学框架的时候会缺失一些信息。因此我们通过大量采样来近似模拟这些信息，进行解题。这就诞生MC系列算法、时序差分系列算法。</p><p>再后来，我们的视角逐渐不再放在最底层的MDP框架公式中，而是在借助MDP框架的关键定义和公式上，试图直接利用数学，拟合出最佳的statevalues / action values / optimal policy。这就是DeepQ-learning、REINFORCE、Actor-Critic。</p><p>未来，至少在短期内可以预见的是，RL将于DL深度结合，且大量的DL技巧将会被运用到RL中来。</p><p>未来RL会走出一条什么样的路，我们不知道。但是，万一哪天RL突破了“RL”的定义和边界，我想，会是件令人激动的事。</p><h4 id="目标函数-1">目标函数</h4><p>也就是引入中说到的<spanclass="math inline">\(J(\theta)\)</span>是啥？</p><p>有两大类metrics，第一类是average state value，就是statevalues的加权平均。记作<spanclass="math inline">\(\bar{v}_\pi\)</span></p><p>第二类是average reward，就是<spanclass="math inline">\(r_\pi(s)\)</span>的加权平均。记作<spanclass="math inline">\(\bar{r}_\pi\)</span></p><p>似乎这两个metrics，前一个更加远视，后一个更加近视（因为考虑的是immediateexpected reward），但其实，对这两个指标做优化是等价的，因为在<spanclass="math inline">\(\gamma &lt; 1\)</span>的时候，满足：<spanclass="math inline">\(\bar{r}_\pi = (1 - \gamma)\bar{v}_\pi\)</span></p><p>average state value, <spanclass="math inline">\(\bar{v}_\pi\)</span>，可写为： <spanclass="math display">\[\bar{v}_\pi = \sum_{s}d_\pi(s)v_\pi(s) = \mathbb{E}\left[\sum_{t=0}^{\infty}\gamma^t R_{t+1} \right]\]</span> 第一个定义就是本身的定义，<spanclass="math inline">\(d_\pi(s)\)</span>是不同state的权重。</p><p>第二个定义不太直观，我来推导一下： <span class="math display">\[\mathbb{E}\left[\sum_{t=0}^{\infty}\gamma^{t}R_{t+1}\right]=\sum_{s\in\mathcal{S}}d(s)\mathbb{E}\left[\sum_{t=0}^{\infty}\gamma^{t}R_{t+1}|S_{0}=s\right]=\sum_{s\in\mathcal{S}}d(s)v_{\pi}(s)\]</span> average reward, <spanclass="math inline">\(\bar{r}_\pi\)</span>，可写为： <spanclass="math display">\[\bar{r}_\pi = \sum_{s}d_\pi(s)r_\pi(s) = \lim_{n \to\infty}\frac{1}{n}\mathbb{E}(\sum_{t=1}^{n}R_t)\]</span> 第一个定义就是本身的定义，第二个定义很好理解，<spanclass="math inline">\(\bar{r}_\pi\)</span>表达的就是所有immediateexpected reward的期望，那么你把所有所有的<spanclass="math inline">\(R_t\)</span>求平均就是<spanclass="math inline">\(\bar{r}_\pi\)</span>了。</p><h4 id="梯度计算">梯度计算</h4><p>直接给出结论，详细证明去书里看： <span class="math display">\[\nabla_\theta\bar{r}_\pi\simeq\sum_sd_\pi(s)\sum_a\nabla_\theta\pi(a|s,\theta)q_\pi(s,a),\\\nabla_\theta\bar{v}_\pi=\frac1{1-\gamma}\nabla_\theta\bar{r}_\pi\]</span> 当<span class="math inline">\(\gamma &lt; 1\)</span>时<spanclass="math inline">\(\nabla_\theta\bar{r}_\pi\)</span>是约等于右边那一坨，当<spanclass="math inline">\(\gamma = 1\)</span>时是严格等于。<spanclass="math inline">\(\gamma &lt; 1\)</span>时第二个式子成立。</p><p>但是上面那个式子还可以进行化简：</p><p>不妨对<span class="math inline">\(\ln \pi(a|s,\theta)\)</span>求导，<span class="math inline">\(\nabla_\theta \ln\pi(a|s, \theta) = \frac{1}{\pi(a|s, \theta)} \cdot \nabla_\theta\pi(a|s, \theta)\)</span></p><p><span class="math inline">\(\therefore \nabla_\theta \pi(a|s, \theta)= \pi(a|s, \theta)\nabla_\theta\ln\pi(a|s, \theta)\)</span></p><p>带回上面的式子，得：<span class="math inline">\(\nabla_\theta\bar{r}_\pi = \sum_{s}d_\pi(s)\sum_{a}\pi(a|s,\theta)\nabla_\theta\ln\pi(a|s, \theta)q_\pi(s, a)\)</span></p><p>那么就可以把<spanclass="math inline">\(\sum\)</span>写为期望的方式：<spanclass="math inline">\(\nabla_\pi \bar{r}_\pi = \mathbb{E}_{\mathcal{S}\sim d, \mathcal{A} \sim \pi}\left[ \nabla_\theta\ln\pi(A|S, \theta)\cdot q_\pi(S, A) \right]\)</span></p><p>这样有什么好处呢？相当于我们只需要有state actionpairs的样本，就可以去拟合<spanclass="math inline">\(\nabla_\pi\bar{r}_\pi\)</span>了，相比于前面求和的形式，训练简直不要简单太多。很牛的idea。</p><p>但是既然你取了<spanclass="math inline">\(\ln\)</span>，那么就要保证<spanclass="math inline">\(\pi(a|s, \theta) &gt;0\)</span>，所以对于神经网络的话，在最后一层就要做一个softmax：</p><p><img src="1.png" style="zoom:67%;" /></p><p>就行了，但是这样搞的话，policy就具有探索性了需要注意。</p><p>所以我们的为了更新matrics的梯度上升就可以这么写，用SGD： <spanclass="math display">\[\theta_{t+1} = \theta_t + \alpha\nabla_\theta\ln\pi(a_t|s_t,\theta_t)q_\pi(s_t,a_t)\]</span> 但上面这个式子目前还用不了，因为<spanclass="math inline">\(q_\pi(s_t,a_t)\)</span>我们不知道。所以用MC系列或者TD系列呗，如果你用MC去拟合<spanclass="math inline">\(q_\pi(s_t,a_t)\)</span>，那么你就得到了REINFORCE算法，表示如下： <spanclass="math display">\[\begin{align*}&amp;\textbf{Initialization: }\text{A parameterized function $\pi(a|s,\theta)$} \\&amp;\text{At time $k$, do} \\&amp;\quad\quad \text{Select $s_0$ and generate an episode following$\pi(\theta_k)$. Suppose the episode is $\{s_0, a_0, r_1, \cdots,s_{T-1}, a_{T_1}, r_T\}$.} \\&amp;\quad\quad \text{For $t = 0,1,\cdots, T-1$, do} \\&amp;\quad\quad\quad\quad q_t(s_t, a_t) = \sum_{k=t+1}^{T}\gamma^{k - t- 1}r_k \\&amp;\quad\quad\quad\quad \theta_{t+1} = \theta_t +\alpha\nabla_\theta\ln\pi(a_t|s_t, \theta_t)q_t(s_t, a_t)\end{align*}\]</span></p><h3 id="actor-critic">Actor-Critic</h3><h4 id="qac">QAC</h4><p>我们仍然是直接估计最优策略<span class="math inline">\(\pi(a | s,\theta)\)</span>，然后前面已经推导出了<spanclass="math inline">\(\theta\)</span>的更新式： <spanclass="math display">\[\theta_{t+1} = \theta_t + \alpha\nabla_\theta\ln\pi(a_t|s_t,\theta_t)q_\pi(s_t,a_t)\]</span> 关键这个<span class="math inline">\(q_\pi(s_t,a_t)\)</span>我们不知道，所以用MC方法去估计得到的方法就叫REINFORCE。</p><p>但是其实可以通过神经网络的方法去估计它：<spanclass="math inline">\(q(s_t,a_t,w_t)\)</span>，更新方式如下（为什么更新方式是这样，去看“值函数近似-优化算法和函数设计”部分）：<span class="math display">\[w_{t+1} = w_t + \alpha_w \left[ r_{t+1} + \gamma q(s_{t+1}, a_{t+1},w_t) - q(s_t, a_t, w_t) \right] \nabla_w q(s_t, a_t, w_t)\]</span> 所以我直接给出算法流程：</p><p><img src="2.png" style="zoom:67%;" /></p><p>但是直到现在为止，我还是没解释Actor-Critic这名字啥意思。其实我觉得这名字没啥意思。只需要记住直接估计最优policy的系列方法都是AC系列算法（除了REINFORCE开除AC学籍）</p><p>上面的算法叫QAC，首先是它属于AC系列算法，然后它的<spanclass="math inline">\(q(s,a)\)</span>是通过神经网络算的，所以叫QAC。</p><h4 id="a2c">A2C</h4><p>Advantage actor-critic, A2C，因为全称有俩A，所以叫A2。</p><p>它是QAC的一个改进版本，在更新<spanclass="math inline">\(\theta\)</span>的那一步进行了优化，具体来说，通过添加偏置项，减小了梯度的方差，但是期望不变。</p><p>这么做的好处，就是在通过SGD优化目标函数时，因为梯度期望不变，所以优化结果不会改变。但是梯度方差减小，所以采样带来的误差会减小。</p><h4 id="重要性采样">重要性采样</h4><p>略</p><h4 id="dpg">DPG</h4><p>前面的三个AC系列算法，在<span class="math inline">\(\pi(a|s,\theta)\)</span>这个神经网络中，最后一层都是加了softmax的，所以无论如何都是具有探索性的。</p><p>所以如何让其变为一个greedy的算法呢，Deterministic Policy Gradient,DPG，就是greedy的直接估计最优policy的算法。</p><p>略</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;值函数近似(DQN)、策略梯度方法(REINFORCE)、Actor-Critic方法&lt;/p&gt;</summary>
    
    
    
    <category term="1. 科研" scheme="http://error666.top/categories/1-%E7%A7%91%E7%A0%94/"/>
    
    <category term="强化学习" scheme="http://error666.top/categories/1-%E7%A7%91%E7%A0%94/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>强化学习2</title>
    <link href="http://error666.top/2024/10/06/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/"/>
    <id>http://error666.top/2024/10/06/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/</id>
    <published>2024-10-05T18:11:53.000Z</published>
    <updated>2024-10-10T16:10:36.626Z</updated>
    
    <content type="html"><![CDATA[<p>值/策略迭代算法、蒙特卡洛、随机近似理论、时序差分方法</p><span id="more"></span><h3 id="一.-值策略迭代算法">一. 值/策略迭代算法</h3><h4 id="值迭代算法">值迭代算法</h4><p>其实在BOE那一章的结尾我已经给出了值迭代算法的流程了：</p><ol type="1"><li>设定好<span class="math inline">\(\gamma\)</span>，<spanclass="math inline">\(r\)</span>，<spanclass="math inline">\(p(r|s,a)\)</span>，<spanclass="math inline">\(p(s&#39;|s,a)\)</span></li><li>随意取一个<span class="math inline">\(v_0\)</span>，然后通过<spanclass="math inline">\(q_\pi(s, a) = \sum_{r}p(r|s, a)r + \gamma\sum_{s&#39;}p(s&#39; | s, a)v_\pi(s&#39;)\)</span>算出对应的<spanclass="math inline">\(q_0\)</span></li><li>For each state <span class="math inline">\(s_i\)</span>, at time<span class="math inline">\(k\)</span>：<ul><li>算出<span class="math inline">\(q_{k}(s_i, a)\)</span></li><li>Find the <span class="math inline">\(a_k^*(s_i)\)</span>, s.t, <spanclass="math inline">\(q_k(s_i, a_k^*(s_i))\)</span>最大</li><li><span class="math inline">\(\pi_{k+1}(a|s_i)=\begin{cases} 1 \quada=a_k^*(s_i) \\ 0 \quad a \ne a_k^*(s_i) \end{cases}\)</span></li><li><span class="math inline">\(v_{k+1}(s_i) =\sum_{a}\pi_{k+1}(a|s)q_k(s,a)\)</span></li></ul></li></ol><p>现在，用正式的语言描述这个algorithm： <span class="math display">\[\begin{align*}&amp;\textbf{Initialization: }\text{The probability model $p(r|s,a)$ and$p(s&#39;|s,a)$ for all $(s,a)$ are known. Initial guess $v_0$.} \\&amp;\text{At time $k$, do} \\&amp;\quad\quad \text{For every state $s \in \mathcal{S}$, do} \\&amp;\quad\quad\quad\quad \text{q-value: $q_k(s,a)=\sum_{r}p(r|s,a)r +\gamma \sum_{s&#39;}p(s&#39;|s,a)v_k(s&#39;)$} \\&amp;\quad\quad \text{Maximum action value: $a_k^*(s) =\text{argmax}_{a}q_k(a,s)$} \\&amp;\quad\quad \text{Policy update: $\pi_{k+1}(a|s)=1$ if $a=a_k^*(s)$,and $\pi_{k+1}(a|s)=0$ otherwise} \\&amp;\quad\quad \text{Value update: $v_{k+1}=q_k(a_k^*(s), s)$}\end{align*}\]</span></p><h4 id="策略迭代算法">策略迭代算法</h4><p>思想就是首先先初始化一个策略，然后先得到该策略下的statevalue（即Policy evaluation, PE），然后得到statevalue后就可以算出对应的action value，然后选择actionvalue最大的action，即优化当前policy（Policyimprovement），得到新的policy。依次类推下去，最终即可得到<spanclass="math inline">\(\pi^*, v^*\)</span>。</p><p>用正式的语言描述这个algorithm： <span class="math display">\[\begin{align*}&amp;\textbf{Initialization: }\text{The probability model $p(r|s,a)$ and$p(s&#39;|s,a)$ for all $(s,a)$ are known. Initial guess $\pi_0$.} \\&amp;\text{At time $k$, do} \\&amp;\quad\quad \text{Policy evaluation:} \\&amp;\quad\quad \text{Initialization: an arbitrary initial guess$v_{\pi_k}^{(0)}$} \\&amp;\quad\quad \text{While $v_{\pi_k}^{(j)}$ has not converged, for the$j$th iteration, do} \\&amp;\quad\quad\quad\quad \text{For every state $s \in \mathcal{S}$, do}\\&amp;\quad\quad\quad\quad\quad\quad v_{\pi_k}^{(j+1)}(s) =\sum_{a}\pi_k(a|s)\left[ \sum_{r}p(r|s,a)r +\gamma\sum_{s&#39;}p(s&#39;|s,a)v_{\pi_k}^{(j)}(s&#39;) \right] \\&amp;\quad\quad \text{Policy improvement:} \\&amp;\quad\quad \text{For every state $s \in \mathcal{S}$, do} \\&amp;\quad\quad\quad\quad q_{\pi_k}(s,a) = \sum_{r}p(r|s,a)r +\gamma\sum_{s&#39;}p(s&#39;|s,a)v_{\pi_k}(s&#39;) \\&amp;\quad\quad a_k^*(s) = \text{argmax}_{a}q_{\pi_k}(s,a) \\&amp;\quad\quad \text{$\pi_{k+1}(a|s) = 1$ if $a = a_k^*(s)$, and$\pi_{k+1}(a|s)=0$ otherwise}\end{align*}\]</span></p><h4 id="两者比较">两者比较</h4><p>值迭代算法是从一个初始state value开始，有了statevalue，就可以算出action value，进而得出当前最优策略，然后去更新statevalue，依次类推。</p><p>策略迭代算法是从一个初始policy开始，然后通过迭代算法求出当前policy下的最优statevalue，然后再通过state value得到actionvalue，进而更新当前最优策略。依次类推。</p><p>可以发现，不同点就在于，同样是得到一个policy，值迭代是立马用其代入bellman-equation算出迭代一次后的statevalue。而策略迭代是代入bellman-equation迭代很多次算出的statevalue。所以直观上来说，策略迭代的收敛次数会更少，但是单次计算量会更大。</p><p><img src="1.png" style="zoom:50%;" /></p><h3 id="二.-蒙特卡洛">二. 蒙特卡洛</h3><h4 id="引入">引入</h4><p>前面的值/策略迭代算法都是model-basedRL，蒙特卡洛是我们接触到的第一个model-free的方法。</p><p>model不知道的时候怎么办呢？蒙特卡洛其实就是大量采样，用样本的分布来估计model的分布。</p><p>蒙特卡洛，Monte Carlo，MC。</p><h4 id="mc-basic">MC Basic</h4><p>MCBasic算法其实就跟policy迭代算法一样，只不过把policy迭代算法里的model-based部分，即计算<spanclass="math inline">\(v_{\pi_k},q_{\pi_k}\)</span>的部分，换成了依靠采样直接算出基于一个策略<spanclass="math inline">\(\pi_{k}\)</span>的<spanclass="math inline">\(q_{\pi_k}\)</span>。第二步policyimprovement就一样了。</p><p>原本policy迭代算法里求<spanclass="math inline">\(q_{\pi_k}\)</span>是依赖于这个公式：<spanclass="math inline">\(q_{\pi_k}(s,a)=\sum_{r}p(r|s,a)r +\gamma\sum_{s&#39;}p(s&#39;|s,a)v_{\pi_k}(s&#39;)\)</span></p><p>但是MC Basic算<spanclass="math inline">\(q_{\pi_k}\)</span>是依赖于它的原始定义：<spanclass="math inline">\(q_{\pi_k}(s,a)=\mathbb{E}(G_t | S_t = s, A_t =a)\)</span></p><p>即对于state-action pairs, 通过大量采样估计出所有的<spanclass="math inline">\(q_{\pi_k}(s,a)\)</span>，然后再进行policyimprovement。</p><p>用数学语言来描述如下： <span class="math display">\[\begin{align*}&amp;\textbf{Initialization: }\text{Initial guess $\pi_0$.} \\&amp;\text{At time $k$, do} \\&amp;\quad\quad \text{For every state $s \in \mathcal{S}$, do} \\&amp;\quad\quad\quad\quad \text{For every action $a \in \mathcal{A}(s)$,do} \\&amp;\quad\quad\quad\quad\quad\quad \text{Collect sufficiently manyepisodes starting from $(s,a)$ following $\pi_k$} \\&amp;\quad\quad\quad\quad\quad\quad \text{$q_{\pi_k}(s,a)=$ averagereturn of all the episodes starting from $(s,a)$} \\&amp;\quad\quad\quad\quad \text{Policy improvement step:} \\&amp;\quad\quad\quad\quad a_k^*(s) = \text{argmax}_{a}q_{\pi_k}(s,a) \\&amp;\quad\quad\quad\quad \text{$\pi_{k+1}(a|s)=1$ if $a=a_k^*()s$, and$\pi_{k+1}(a|s)=0$ otherwise}\end{align*}\]</span> 很简单，right？</p><p>为啥这里要用episode这个词而非trajectory这个词呢？因为trajectory可能是无限的，而采样是离散的，所以通常我们设置一个采样长度上限，那么每采样一条trajectory其实就是有限的，也叫一条episode。</p><h4 id="mc-exploring-starts">MC Exploring Starts</h4><p>MC Exploring Starts其实就是对MC Basic算法的一个时间复杂度优化。</p><p>MC Basic是对每一个<span class="math inline">\((s,a)\)</span>pair都采样很多episode来估计其<span class="math inline">\(q_{\pi_k}(s,a)\)</span>，采样的途中可能会路过很多其余的<spanclass="math inline">\((s&#39;,a&#39;)\)</span>pair，其实采样出来的return也可以用来估计它们的actionvalue。下面这个图就可以很好的解释了MC Basic的数据浪费：</p><p><img src="2.png" style="zoom: 67%;" /></p><p>看第一条episode，在MCBasic算法里那么一长条episode，我们只用它来估计了<spanclass="math inline">\((s_1, a_2)\)</span>的actionvalue。但其实，还可以用来估计<span class="math inline">\((s_2, a_4),\cdots\)</span>的action value，它们的return之间只差了一个<spanclass="math inline">\(\gamma\)</span>和reward。</p><p>所以MC ExploringStarts就是抓住了这点进行优化，就是类似记忆化搜索的思想 +dp填表法的思想。它的具体思想如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> T;                <span class="comment">// episode的长度</span></span><br><span class="line"><span class="type">int</span> q_sum[][];        <span class="comment">// (s,a)的action value的总和</span></span><br><span class="line"><span class="type">int</span> q_cnt[][];        <span class="comment">// (s,a)的action value的采样次数</span></span><br><span class="line"><span class="type">int</span> q[][];            <span class="comment">// (s,a)的action value</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> (<span class="number">1</span>) &#123;</span><br><span class="line">   <span class="keyword">if</span> (如果不想再迭代了) <span class="keyword">break</span>;</span><br><span class="line">   <span class="keyword">else</span> 确定起点(s0, a0)，按照当前policy生成一条长度为T的episode，将episode路上的(si, ai, ri)存到vector: path中</span><br><span class="line">       </span><br><span class="line">   <span class="type">int</span> G = <span class="number">0</span>;    <span class="comment">// episode的return</span></span><br><span class="line">   <span class="keyword">for</span> (<span class="type">int</span> i = path.<span class="built_in">size</span>() - <span class="number">1</span>; i &gt;= <span class="number">0</span>; i--) &#123;</span><br><span class="line">       G = gamma * G + path[i].r;</span><br><span class="line">       q_sum[s][a] += G;</span><br><span class="line">       q_cnt[s][a] += <span class="number">1</span>;</span><br><span class="line">       </span><br><span class="line">       q[s][a] = q_sum[s][a] / q_cnt[s][a];</span><br><span class="line">       更新<span class="built_in">pi</span>(a|s)</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>用数学语言描述如下： <span class="math display">\[\begin{align*}&amp;\textbf{Initialization: }\text{ Initial policy $\pi_0(a|s)$ andinitial value $q(s,a)$ for all $(s,a)$.} \\&amp;\quad\quad\quad\quad\quad\quad\quad~\text{Returns(s,a) = 0 andNum(s,a) = 0 for all $(s,a)$.} \\&amp;\text{For each episode, do} \\&amp;\quad\quad \text{Episode generation: Select a starting state-actionpair $(s_0, a_0)$}  \\&amp;\quad\quad \text{and ensure that all pairs can be possibly selected(this is the exploring-starts condition).} \\&amp;\quad\quad \text{Following the current policy, generate an episodeof length $T$: $s_0, a_0, r_1, \cdots, s_{T-1}, a_{T-1}, r_T$.} \\&amp;\quad\quad \text{Initialization for each episode: $g \gets 0$} \\&amp;\quad\quad \text{For each step of the episode, $t = T - 1, T - 2,\cdots, 0, $ do} \\&amp;\quad\quad\quad\quad g \gets \gamma g + r_{t+1} \\&amp;\quad\quad\quad\quad \text{Returns($s_t, a_t$) $\gets$Returns($s_t, a_t$) + $g$} \\&amp;\quad\quad\quad\quad \text{Policy evaluation:} \\&amp;\quad\quad\quad\quad q(s_t, a_t) \gets \text{Returns($s_t, a_t$) /Num($s_t,a_t$)} \\&amp;\quad\quad\quad\quad \text{Policy improvement:} \\&amp;\quad\quad\quad\quad \text{$\pi(a|s_t)=1$ if $a =\text{argmax}_aq(s_t, a)$ and $\pi(a|s_t)=0$ otherwise}\end{align*}\]</span> 最后我们来看看这个“ExploringStarts”是什么意思。Exploring我个人理解就是由于是dp填表法，所以episode就需要自己去生成，也就是exploring的过程。Starts是因为此时的算法不是像MCBasic样每个state-actionpair都去强制估计了，所以为了尽量确保每个state-action都被估计到，每个episode的起点的选法就很有讲究，最好每个state-action都被作为起点选择一次（当然这就退化为MCBasic了）</p><h4 id="mc-varepsilon-greedy">MC <spanclass="math inline">\(\varepsilon\)</span>-Greedy</h4><p>MC Exploring Starts算法很好，但是它不能保证每个state-actionpair都被估计到，所以选多少个episode，每个episode的起点是啥就很有讲究。可能起点选少了直接导致效果不好，选多了又速度慢。</p><p>为了解决上述问题，MC <spanclass="math inline">\(\varepsilon\)</span>-Greedy 算法应运而生。</p><p><span class="math inline">\(\varepsilon\)</span>-Greedy与MC ExploringStarts的区别就在Policy improvement这一步，MC ExploringStarts在这一步是直接将最大的actionvalue对应的action的概率设为1其余为0，但是<spanclass="math inline">\(\varepsilon\)</span>-Greedy是最大的actionvalue对应的action概率设为<span class="math inline">\(1 -\varepsilon\)</span>，其余的action概率设为<spanclass="math inline">\(\varepsilon\)</span>。</p><p>这样的好处就是不需要对全部的state-actionpair都作为起点生成episode，只要生成一些episode（起点随便），并且只要保证这条episode的长度T很长，那你在就几乎可以路过所有state-actionpairs，从而估计它们。就不需要从很多不同的起点开始了。</p><p>坏处就是通过<spanclass="math inline">\(\varepsilon\)</span>-Greedy得到的policy并不是最优的，因为它始终带着探索的概率。所以通常我们的做法是：</p><p>初始化policy为<spanclass="math inline">\(\varepsilon\)</span>策略，进行多次episode，每次的episode的<spanclass="math inline">\(\varepsilon\)</span>递减。这样就可以保证前面的episode随机性很强，从而可以覆盖到大多数state-actionpair，但是毕竟我们是要求最优解的，所以后期的episode的<spanclass="math inline">\(\varepsilon\)</span>就得减小。最后，在通过<spanclass="math inline">\(\varepsilon\)</span>-Greedy得到一个policy后，还是要将其转为确定性的策略（即不带概率的）。</p><p><spanclass="math inline">\(\varepsilon\)</span>-Greedy这个算法的目的就是为了避免要进行多次全部state-actionpair起点选择，仅此而已。它是如何做到的？尝试不是最优的策略，通过尝试不是最优的策略，从而尽可能的，覆盖所有的state-actionparis。所以从原理上，这个算法就是会降低准确率。但是它也很具有实际意义，因为你想啊，你在仿真中设计的算法是要用到实际环境中去的。你的机器人通常就从安全的起点出发开始探索，如果你要让它从全部的state-action开始，那显然不现实。例如深海作业，你都是让robot从浅水区开始，然后让它自己去探索，尽可能覆盖所有state-actionpairs。你总不可能让它从深海区开始吧，因为你人类到不了深海区。</p><p>下面是用数学描述： <span class="math display">\[\begin{align*}&amp;\textbf{Initialization: } \text{Initial policy $\pi_0(a|s)$ andinitial value $q(s,a)$ for all $(s,a)$. Returns(s,a)=0 and Num(s,a)=0}\\&amp;\text{for all $(s,a)$. $\varepsilon \in (0, 1]$} \\&amp;\text{For each episode, do} \\&amp;\quad\quad \text{Episode generation: Select a starting state-actionpair $(s_0, a_0)$. Following the current policy,} \\&amp;\quad\quad \text{generate an episode of length $T: s_0, a_0, r_1,\cdots, s_{T-1}, a_{T-1}, r_T$.} \\&amp;\quad\quad \text{Initialization for each episode: $g \gets 0$} \\&amp;\quad\quad \text{For each step of the episode, $t = T-1, T-2,\cdots, 0,$ do} \\&amp;\quad\quad\quad\quad g \gets \gamma g + r_{t+1} \\&amp;\quad\quad\quad\quad \text{Returns($s_t, a_t$) $\gets$Returns($s_t, a_t$) + $g$} \\&amp;\quad\quad\quad\quad \text{Num($s_t, a_t$) $\gets$ Num($s_t, a_t$)+ 1} \\&amp;\quad\quad\quad\quad \text{Policy evaluation:} \\&amp;\quad\quad\quad\quad q(s_t, a_t) \gets \text{Returns($s_t, a_t$) /Num($s_t, a_t$)} \\&amp;\quad\quad\quad\quad \text{Let } a^* = \text{argmax}_a q(s_t, a)\text{ and} \\&amp;\quad\quad\quad\quad\quad\quad \pi(a|s_t) = \begin{cases}1 -\frac{|\mathcal{A}(s_t)|-1}{|\mathcal{A}|}\varepsilon, \quad a = a^* \\\frac{1}{|\mathcal{A}(s_t)|}\varepsilon, \quad a \ne a^* \end{cases}\end{align*}\]</span> 个人觉得，<spanclass="math inline">\(\varepsilon\)</span>-Greedy的探索性与收敛性是严重矛盾的，因为明明已经通过采样得到state-actionvalue值了，只需要一直不断的更新deterministic的策略，就可以收敛了。但是<spanclass="math inline">\(\varepsilon\)</span>-Greedy为了探索性，会将"探索"这个元素，加入进自己的"策略"里。所以，就像一个明知道最优解的人，在做事情的时候，仍然小概率选择不是最优的东西。所以，<spanclass="math inline">\(\varepsilon\)</span>-Greedy这个算法收敛性，有些靠天。</p><h4 id="总结">总结</h4><p>我们从model-based的算法（值/策略迭代算法）开始说起，model-based算法的收敛性和最优性都是有保证的，在"强化学习1"中有提到证明。</p><p>随后我们进入了model-free算法，此时我们只能依靠采样来估计<spanclass="math inline">\(p(s,a)\)</span>，所以MCBasic只要采样数量无限大，那么其准确性和收敛性也是可以得到保证的。</p><p>但是MC Basic效率太慢了，为此MC ExploringStarts应运而生，运用了记忆化的思想加速了收敛。只要保证每个action-pair都被大量采样到，该算法也能保证准确性和收敛性。</p><p>但是问题就是为了保证“每个action-pair都被大量采样到”，MC ExploringStarts就需要从不同的action-pair起点去生成episode进行采样。而现实环境中这是有难度的，例如你不能让机器人从深海区开始，一般都是从浅水区开始。</p><p>所以MC <spanclass="math inline">\(\varepsilon\)</span>-Greedy算法应运而生，增加了"探索"机制，从而不必使每一个action-pair都要作为起点去生成episode进行采样。但是因为探索是直接加到policy里，所以该算法的准确率会下降，甚至收敛都不一定。不过这对未来的算法具有启发意义。</p><h3 id="三.-随机近似理论">三. 随机近似理论</h3><p>这一章的内容是为了下一章时序差分方法打基础。</p><h4 id="引入-1">引入</h4><p>以前我们用采样估计一个平均数的时候，都是收集m个样本，然后求它们的平均数作为估计值。</p><p>但这样做的话，你需要等到所有的样本都收集到了，才能进行估计。</p><p>所以我们可以用增量式的方法来解决这个问题：</p><p>令：<span class="math inline">\(w_{k+1} = \frac{1}{k}\sum_{i=1}^kx_i, k=1,2,\cdots\)</span></p><p>所以有：<span class="math inline">\(w_k =\frac{1}{k-1}\sum_{i=1}^{k-1}x_i, k=1,2,\cdots\)</span></p><p>那么：<span class="math inline">\(w_{k+1} = w_k - \frac{1}{k}(w_k -x_k)\)</span></p><p>所以来一个样本，就做一次迭代，最终估计的效果跟全部收集到是一样的。</p><p>其实，上面的算法可以进一步推广为： <span class="math display">\[w_{k+1} = w_k - \alpha_k(w_k - x_k)\]</span> 当<span class="math inline">\(\alpha_k &gt;0\)</span>并且满足一定条件时，上面的<spanclass="math inline">\(w_k\)</span>也能收敛于<spanclass="math inline">\(\mathbb{E}(X)\)</span></p><p>上面的算法其实就是一种特殊的Stochastic Approximation algorithm.</p><h4 id="rm">RM</h4><p>Stochastic Apporximation (SA)algorithm其实是一大类算法的总称，描述的是“涉及到随机变量采样”、"迭代式"的算法。</p><p>Robbins-Monro (RM)算法是SA algorithm算法里的一项开创性工作。</p><p>下面来思考这么一个问题，解方程： <span class="math display">\[g(w) = 0\]</span> 其中，<spanclass="math inline">\(g(\cdot)\)</span>未知，但是<spanclass="math inline">\(\nabla g\)</span>是正数且有上下界。</p><p>那么，RM algorithm可以解决这个问题： <span class="math display">\[w_{k+1} = w_k - a_k \tilde{g}(w_k, \eta_k), \quad k=1,2,3,\cdots\]</span></p><ul><li><span class="math inline">\(w_k\)</span> 是第k次迭代对<spanclass="math inline">\(g(w)=0\)</span>解的估计</li><li><span class="math inline">\(\tilde{g}(w_k, \eta_l)=g(w_k) +\eta_k\)</span>是第k次迭代的黑盒输出(这个输出跟真实的输出可能存在误差)</li><li><span class="math inline">\(a_k\)</span>是第k次迭代的正系数</li></ul><p>只要一直迭代下去，那么最终迭代到的<spanclass="math inline">\(w^*\)</span>就是<spanclass="math inline">\(g(w)=0\)</span>的解。</p><p>为啥成立呢？</p><p>从直观上很容易理解，因为<span class="math inline">\(\nablag\)</span>是正数且有上下界，所以如果<spanclass="math inline">\(w_k\)</span>越过了零点，那么其函数值就&gt;0了，那么<spanclass="math inline">\(w_k\)</span>就要往回走一点，也就是减去一个正数，用系数* 函数值刚好可以用作这个系数。如果<spanclass="math inline">\(w_k\)</span>还没到零点，那么其函数值&lt;0，那么<spanclass="math inline">\(w_k\)</span>就要前进一点，也就是减去一个负数，用系数* 函数值刚好可以用作这个系数。一直迭代下去，<spanclass="math inline">\(w_k\)</span>自然趋近零点。</p><p>以上，都是直观上的描述。现在，让我们给出Robbins-Monor (RM)算法的严谨数学表述： <span class="math display">\[\begin{align*}&amp;\text{In the Robbins-Monro algorithm: } w_{k+1} = w_k - a_k\tilde{g}(w_k, \eta_k), \quad k=1,2,3,\cdots\\&amp;if \\&amp;\quad\quad \text{1) $0 &lt; c_1 \le \nabla_w g(w) \le c_2$ for all$w$;} \\&amp;\quad\quad \text{2) $\sum_{k=1}^{\infty}a_k = \infty$ and$\sum_{k=1}^{\infty}a_k^2 &lt; \infty$;} \\&amp;\quad\quad \text{3) $\mathbb{E}[\eta_k | \mathcal{H}_k] = 0$ and$\mathbb{E}[\eta_k^2 | \mathcal{H}_k] &lt; \infty$}; \\&amp;\text{where $\mathcal{H}_k = \{w_k, w_{k-1}, \cdots, \}$, then$w_k$ converges with probability 1 (w.p.1) to the root $w^*$ satisfying$g(w^*)=0$.}\end{align*}\]</span></p><ul><li>这里用依概率收敛(w.p.1)是因为<spanclass="math inline">\(w_k\)</span>是涉及到随机变量采样的一个数，所以为了严谨，这里用了w.p.1</li><li>(1)是对梯度的要求，即要求梯度是大于0的且有上下界</li><li>(2)是对系数的要求，<spanclass="math inline">\(\sum_{k=1}^{\infty}a_k^2 &lt;\infty\)</span>保证了<spanclass="math inline">\(a_k\)</span>会收敛到0，<spanclass="math inline">\(\sum_{k=1}^{\infty}a_k =\infty\)</span>保证了<spanclass="math inline">\(a_k\)</span>收敛的速度不会很快</li><li>(3)是对测量误差的要求，就是说假设你采样的误差的期望要是0，且误差的平方的期望不能发散</li></ul><p>这里，我想讨论一下为什么第二个条件很重要：</p><p>若<span class="math inline">\(\sum_{k=1}^{\infty}a_k^2 &lt;\infty\)</span>，则会保证<spanclass="math inline">\(a_k\)</span>收敛到0，则会使得<spanclass="math inline">\(w_{\infty+1} = w_{\infty}\)</span>，即<spanclass="math inline">\(w_k\)</span>收敛。</p><p>若<span class="math inline">\(\sum_{k=1}^{\infty}a_k =\infty\)</span>呢？有啥用？不妨写出下列式子： <spanclass="math display">\[\begin{cases}&amp;w_2 = w_1 - a_1\tilde{g}(w_1, \eta_1) \\&amp;w_3 = w_2 - a_2\tilde{g}(w_2, \eta_2) \\&amp;\cdots\end{cases}\]</span> 将以上式子全加起来，可得到：<spanclass="math inline">\(w_{\infty} - w_1 = \sum_{k=1}^{\infty}a_k\tilde{g}(w_k, \eta_k)\)</span></p><p>如果<span class="math inline">\(\sum_{k=1}^{\infty}a_k =\infty\)</span>，就可以保证<spanclass="math inline">\(\sum_{k=1}^{\infty}a_k \tilde{g}(w_k,\eta_k)\)</span>发散，这样我们的<spanclass="math inline">\(w_1\)</span>就随便取都行了。如果有界的话，那我的<spanclass="math inline">\(w_1\)</span>的取值就被限定在一个范围了。</p><h4 id="sgd">SGD</h4><p>GD、BGD、SGD其实是一个系列的算法，它们的目的，都是解决下列这个优化问题：<span class="math display">\[\min_w J(w) = \mathbb{E}\left[ f(w, X) \right]\]</span> （即找到<span class="math inline">\(w\)</span>，使得<spanclass="math inline">\(J(w)\)</span>最小）</p><p>梯度下降大家都很熟悉了，这里直接给出定义和简单解释。</p><p>首先是gradient descent, GD, 梯度下降算法： <spanclass="math display">\[w_{k+1} = w_k - \alpha_k \mathbb{E}\left[ \nabla_wf(w_k, X) \right]\]</span>但是这个形式涉及到期望，是理想的式子，在现实中，我们往往用样本去估计这个期望，所以就有了batchgradient descent, BGD： <span class="math display">\[\mathbb{E}\left[ \nabla_wf(w_k, X) \right] \approx\frac{1}{n}\sum_{i=1}^{n} \nabla_wf(w_k, x_i) \\w_{k+1} = w_k - \alpha_k \frac{1}{n} \sum_{i=1}^{n} \nabla_w f(w_k, x_i)\]</span>但是毕竟还是要求出一个batch后才能迭代更新一次嘛，还是慢了，那就来一个样本就更新一次，于是就有了stochasticgradient descent, SGD, 随机梯度下降： <span class="math display">\[w_{k+1} = w_k - \alpha_k \nabla_w f(w_k, x_k)\]</span> 证明SGD收敛的过程我这里大概证一下：</p><p>首先，SGD可以写为RM算法形式： <span class="math display">\[w_{k+1} = w_k - \alpha_k \tilde{g}(w_k, \eta_k) \\\tilde{g}(w_k, \eta_k) = \nabla_wf(w_k, x_k) =\mathbb{E}[\nabla_wf(w,X)] + \left( \nabla_w f(w_k,x_k) -\mathbb{E}[\nabla_w f(w,X)] \right)\]</span> 令<span class="math inline">\(g(w,X) = \nabla_w f(w,X)\)</span>，其实我们就是想求解<span class="math inline">\(g(w, X) =0\)</span>这个方程。</p><p>那么上面的<span class="math inline">\(\tilde{g}(w_k,\eta_k)\)</span>就可写为<span class="math inline">\(g(w, X) +\eta\)</span>的形式，所以SGD被写为了RM算法的形式。</p><p>只要保证<span class="math inline">\(g(w,X)\)</span>的梯度是正数且有上下界（即<span class="math inline">\(f(w,X)\)</span>是凸的），且系数满足那俩条件，且误差<spanclass="math inline">\(\eta\)</span>满足那俩条件，那么SGD算法的收敛性就可以得到保证。</p><p>用数学语言描述SGD的收敛条件如下： <span class="math display">\[\begin{align*}&amp;\text{In the SGD algorithm, if} \\&amp;\quad\quad \text{1) } 0 &lt; c_1 \le \nabla_w^2 f(w, X) \le c_2 \\&amp;\quad\quad \text{2) } \sum_{k=1}^{\infty}a_k = \infty \text{ and }\sum_{k=1}^{\infty} a_k^2 &lt; \infty \\&amp;\quad\quad \text{3) } \{x_k\}_{k=1}^{\infty} \text{ is iid} \\&amp;\text{then $w_k$ converges to the root of $\nabla_w\mathbb{E}[f(w,X)] = 0$ with probability 1.}\end{align*}\]</span> SGD这个算法究竟好不好呢？其实是挺好的，当<spanclass="math inline">\(w_k\)</span>与<spanclass="math inline">\(w^*\)</span>相距较远时，它的表现和GD的表现差不多。我们可以通过误差来看看：<span class="math display">\[\delta_k\doteq\frac{|\nabla_wf(w_k,x_k)-\mathbb{E}[\nabla_wf(w_k,X)]|}{|\mathbb{E}[\nabla_wf(w_k,X)]|}\]</span> 那么通过理论分析，我们可以得到，SGD算法下，这个误差满足：<span class="math display">\[\delta_k\leq\frac{\mid\overbrace{\nabla_wf(w_k,x_k)}^\text{stochasticgradient}-\overbrace{\mathbb{E}[\nabla_wf(w_k,X)]}^\text{truegradient}\mid}{\underbrace{c|w_k-w^*|}_{\text{distance to the optimalsolution}}}\]</span> 所以当<span class="math inline">\(w_k\)</span>与<spanclass="math inline">\(w^*\)</span>相距较远时，它的表现和GD的表现差不多，这是个很不错的算法。</p><h4 id="总结-1">总结</h4><ul><li>这一章其实上是介绍了优化算法。</li><li>首先先介绍了解决<span class="math inline">\(g(w) =0\)</span>的RM算法：<span class="math inline">\(w_{k+1} = w_k - a_k\tilde{g}(w_k, \eta_k)\)</span>，它需要满足下列三个条件才能收敛：<ol type="1"><li><span class="math inline">\(0 &lt; c_1 \le \nabla_w g(w) \lec_2\)</span></li><li><span class="math inline">\(\sum_{k=1}^{\infty}a_k = \infty\)</span>and <span class="math inline">\(\sum_{k=1}^{\infty}a_k^2 &lt;\infty\)</span></li><li><span class="math inline">\(\mathbb{E}[\eta_k | \mathcal{H}_k] =0\)</span> and <span class="math inline">\(\mathbb{E}[\eta_k^2 |\mathcal{H}_k] &lt; \infty\)</span></li></ol></li><li>GD系列算法也属于RM算法，它们负责解决<spanclass="math inline">\(\min_w J(w) = \mathbb{E}\left[ f(w, X)\right]\)</span>问题，换句话说，就是解决<spanclass="math inline">\(\mathbb{E}\left[ \nabla_wf(w_k, X) \right] =0\)</span>问题，所以也可以转换为RM去证明收敛性。最终证明出需要满足下列三个条件才能收敛：<ol type="1"><li><span class="math inline">\(0 &lt; c_1 \le \nabla_w^2 f(w, X) \lec_2\)</span></li><li><span class="math inline">\(\sum_{k=1}^{\infty}a_k = \infty \text{and } \sum_{k=1}^{\infty} a_k^2 &lt; \infty\)</span></li><li><span class="math inline">\(\{x_k\}_{k=1}^{\infty} \text{ isiid}\)</span></li></ol></li></ul><h3 id="四.-时序差分方法">四. 时序差分方法</h3><h4 id="td-algorithm">TD algorithm</h4><p>TD算法通常是指一大类算法，但是这一小节的TD算法就是具体的一个小算法，它用来在已知一个策略<spanclass="math inline">\(\pi\)</span>下，来估计<spanclass="math inline">\(v_\pi\)</span>的值。</p><p>首先回想一下MC ExploringStarts的思路，进行很多次起点选择，每次选择一个起点后生成一条episode，然后倒着更新一路上的<spanclass="math inline">\(q(s,a)\)</span></p><p>是有点记忆化的味道了，不过还不够记忆化，因为</p><p>我直接给出TD algorithm： <span class="math display">\[\begin{cases}v_{t+1}(s_t) = v_t(s_t) - \alpha_t(s_t)\left[ v_t(s_t) - \left[ r_{t+1}+ \gamma v_t(s_{t+1}) \right] \right], \\v_{t+1}(s) = v_t(s), \forall s \ne s_t.\end{cases}\]</span>这就是完全的记忆化了。在程序上可以这么实现：进行若干次迭代，每次迭代循环所有的state:<spanclass="math inline">\(s_t\)</span>，根据policy可以找到它的下一个状态:<spanclass="math inline">\(s_{t+1}\)</span>，然后按照上面的迭代式更新一遍<spanclass="math inline">\(v(s_t)\)</span>。若干次迭代结束后，<spanclass="math inline">\(v(s) \to v_{\pi}(s), \forall s\)</span></p><p>上面的式子从直观上也很好理解，<spanclass="math inline">\(v_t(s_t)\)</span>就是对<spanclass="math inline">\(v_\pi(s_t)\)</span>的估计，所以<spanclass="math inline">\(v_t(s_t)\)</span>是不断在迭代更新的，咋更新的呢？就是不断的逼近<spanclass="math inline">\(r_{t+1} + \gammav_t(s_{t+1})\)</span>。这就是bellmanequation啊。所以本质上我觉得就是迭代法求bellmanequation，只不过与最开始那个迭代法求bellmanequation是在model-based的情况下，现在这个迭代法是无需知道model的。</p><p>我们来从数学上证明一下上面那个算法为什么会收敛到<spanclass="math inline">\(r_{t+1} + \gamma v_t(s_{t+1})\)</span>：</p><p>令：<span class="math inline">\(y = r_{t+1} + \gammav_t(s_{t+1})\)</span></p><p>则迭代式可写为：<span class="math inline">\(v_{t+1}(s_t) = v_t(s_t) -\alpha_t(s_t)[v_t(s_t) - y]\)</span></p><p>两边同减y：<span class="math inline">\(v_{t+1}(s_t) - y = (v_t(s_t) -y) - \alpha_t(s_t)[v_t(s_t) - y]\)</span></p><p>整理：<span class="math inline">\(v_{t+1}(s_t) - y = [1 -\alpha_t(s_t)](v_t(s_t) - y)\)</span></p><p><span class="math inline">\(\therefore \|v_{t+1}(s_t) - y\| \le \| 1- \alpha_t(s_t) \| \cdot \|v_t(s_t) - y\|\)</span></p><p>所以最终<span class="math inline">\(v_{t}(s_t) \to y\)</span></p><p>当<span class="math inline">\(v_t(s_t) \to r_{t+1} + \gammav_t(s_{t+1})\)</span>时其实就是bellman equation了。</p><p>（你可能会问，你这个bellmanequation没带概率啊。别急，概率在迭代过程中policy选择当前state下一个状态<spanclass="math inline">\(s_{t+1}\)</span>进行更新的时候用到了，所以估计出来的这个<spanclass="math inline">\(v_t(s)\)</span>，就是<spanclass="math inline">\(v_\pi(s)\)</span>）</p><p>（数学证明出，当<span class="math inline">\(\sum_{t} \alpha_t(s) =\infty\)</span>, <span class="math inline">\(\sum_{t} \alpha_t^2(s) &lt;\infty\)</span>, <span class="math inline">\(\foralls\)</span>时，上述TD算法就能使<span class="math inline">\(v_t(s) \tov_\pi(s), \forall s\)</span>）</p><h4 id="sarsa">Sarsa</h4><p>前面的TD是在model-free的情况下估计出statevalue。Sarsa就是在model-free的情况下估计出action value。</p><p>直接给出算法，非常好理解： <span class="math display">\[\begin{cases}q_{t+1}(s_t, a_t) = q_t(s_t, a_t) - \alpha_t(s_t, a_t)\left[ q_t(s_t,a_t) - [r_{t+1} + \gamma q_t(s_{t+1}, a_{t+1})] \right], \\q_{t+1}(s,a) = q_t(s,a), \quad \forall (s,a) \ne (s_t, a_t)\end{cases}\]</span>有意思的小故事，为啥叫Sarsa？其实就是state-action-reward-state-action的缩写，难蚌。</p><p>程序也很好写，进行若干次迭代，每次迭代双重循环枚举state、action，然后按照上面迭代式更新就好了。最终<spanclass="math inline">\(q_t(s,a) \to q_\pi(s,t)\)</span></p><p>其收敛条件为：<span class="math inline">\(\sum_{t} \alpha_t(s,a) =\infty\)</span>, <span class="math inline">\(\sum_{t}\alpha_t^2(s,a)&lt; \infty\)</span>, <span class="math inline">\(\forall(s,a)\)</span></p><p>action value都求出来了，后续你是greedy还是<spanclass="math inline">\(\varepsilon\)</span>-Greedy去update你的policy都可以。</p><h4 id="q-learning">Q-learning</h4><p>跟Sarsa不同，Sarsa是估计action value，结合policyimprovement才可以得到最优policy。而Q-learing是一步到位直接估计optimalaction value。</p><p>其实就修改了Sarsa迭代式里的一个地方，直观感觉就是把policyimprovement这一步直接换成在更新时就优化actionvalue了。这样直接可以得到最优action value，再greedy的求出最优策略即可。<span class="math display">\[\begin{cases}q_{t+1}(s_t, a_t) = q_t(s_t, a_t) - \alpha_t(s_t, a_t)\left[ q_t(s_t,a_t) - [r_{t+1} + \gamma \max_{a \in \mathcal{A}}q_t(s_{t+1}, a)]\right], \\q_{t+1}(s,a) = q_t(s,a), \forall (s,a) \ne (s_t, a_t)\end{cases}\]</span>因为Q-learning用的很多，所以这里我给出其正式的流程描述，分为on-policy和off-policy两个版本：</p><p><strong>On-policy version：</strong> <span class="math display">\[\begin{align*}&amp;\text{For each episode, do} \\&amp;\quad\quad \text{If the current $s_t$ is not the target state, do}\\&amp;\quad\quad\quad\quad \text{Collect the experience $(s_t, a_t,r_{t+1}, s_{t+1})$: In particular, take action $a_t$ follwing} \\&amp;\quad\quad\quad\quad \pi_t(s_t), \text{ generate } r_{t+1},s_{t+1}. \\&amp;\quad\quad\quad\quad \text{Update q-value:} \\&amp;\quad\quad\quad\quad\quad\quad q_{t+1}(s_t, a_t) = q_t(s_t, a_t) -\alpha_t(s_t, a_t) - \alpha_t(s_t, a_t)\left[ q_t(s_t, a_t) - \left[r_{t+1} + \gamma \max_{a}q_t(s_{t+1}, a) \right] \right] \\&amp;\quad\quad\quad\quad \text{Update policy:} \\&amp;\quad\quad\quad\quad\quad\quad \pi_{t+1}(a|s_t) = 1 -\frac{\varepsilon}{|\mathcal{A}|}(|\mathcal{A}| - 1) \text{ if } a =\text{argmax}_a q_{t+1}(s_t, a) \\&amp;\quad\quad\quad\quad\quad \quad \pi_{t+1}(a|s_t) =\frac{\varepsilon}{|\mathcal{A}|} \text{ otherwise}\end{align*}\]</span> <strong>Off-policy version：</strong> <spanclass="math display">\[\begin{align*}&amp;\text{For each episode $\{s_0, a_0, r_1, s_1, a_1, r_2, \cdots \}$generated by $\pi_b$, do} \\&amp;\quad\quad \text{For each step $t = 0,1,2,\cdots$ of the episode,do} \\&amp;\quad\quad\quad\quad \text{Update q-value:} \\&amp;\quad\quad\quad\quad\quad\quad q_{t+1}(s_t, a_t) = q_t(s_t, a_t) -\alpha_t(s_t, a_t) - \alpha_t(s_t, a_t)\left[ q_t(s_t, a_t) - \left[r_{t+1} + \gamma \max_{a}q_t(s_{t+1}, a) \right] \right] \\&amp;\quad\quad\quad\quad \text{Update target policy:} \\&amp;\quad\quad\quad\quad\quad\quad \pi_{T,t+1}(a|s_t) = 1 \text{ of } a= \text{argmax}_a q_{t+1}(s_t, a) \\&amp;\quad\quad\quad\quad\quad\quad \pi_{T,t+1}(a|s_t) = 0 \text{otherwise}\end{align*}\]</span>可以发现，off-policy与on-policy的区别就是，off-policy生成experience数据的policy与优化出的policy不同。</p><p>所以off-policy的一个特点就是最终的优化结果跟生成数据的policy相关性很大，因为它是在生成experience数据的policy基础上优化的。</p><p>但是on-policy，即使初始策略很烂，但是因为是持续优化，最终仍可以收敛到全局最优policy。</p><p>那么off-policy就没有好处了吗？并不是的，它有着自身的优点，在后续与DL结合的时候你就知道了。</p><h4 id="总结-2">总结</h4><p>这一章叫时序差分方法，但是我更愿意把其叫做model-free的迭代法求statevalue(TD)、action value(Sarsa)、optimal actionvalue(Q-learing)。与MC系列算法相比，我觉得是MC系列的完全记忆化版本，即MCExploring Starts算法的优化版本。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;值/策略迭代算法、蒙特卡洛、随机近似理论、时序差分方法&lt;/p&gt;</summary>
    
    
    
    <category term="1. 科研" scheme="http://error666.top/categories/1-%E7%A7%91%E7%A0%94/"/>
    
    <category term="强化学习" scheme="http://error666.top/categories/1-%E7%A7%91%E7%A0%94/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>强化学习1</title>
    <link href="http://error666.top/2024/10/03/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A01/"/>
    <id>http://error666.top/2024/10/03/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A01/</id>
    <published>2024-10-03T14:35:02.000Z</published>
    <updated>2024-10-05T17:22:49.659Z</updated>
    
    <content type="html"><![CDATA[<p>基本概念、MDP、贝尔曼公式、贝尔曼最优公式</p><span id="more"></span><p>Follow的西湖大学赵老师的<ahref="https://www.bilibili.com/video/BV1sd4y167NS/?p=2&amp;vd_source=ca9a71bb3c1806ce48ae27d95e4e8bd0">B站课程</a></p><hr /><h3 id="一.-基本概念">一. 基本概念</h3><p>考虑一个在强化学习里很常见的例子“Grid-world”，网格世界：</p><p><img src="1.png" style="zoom:67%;" /></p><p>网格有以下几种类型：</p><ol type="1"><li>Acessible cell：可到达，即白色</li><li>Forbidden cell：进去会得到惩罚的地方，即黄色</li><li>Target cell：希望到达的地方，即蓝色</li></ol><p>网格世界有边界，且机器人只能在相邻块间移动。</p><p>网格世界的任务就是：找到一个“好”的路径到达target cell。</p><p>下面借用grid-world例子，来介绍几个在RL里常见的名词</p><ul><li><p><code>state</code>：</p><ul><li>符号：<span class="math inline">\(s_i\)</span></li><li>解释：agent的可能会处于的状态，在grid-world里robot的state就是它的location</li></ul></li><li><p><code>state-space</code>：</p><ul><li>符号：<span class="math inline">\(\mathcal{S} =\{s_i\}\)</span></li><li>解释：所有state的集合，在grid-world里state-space就是全部的location</li></ul></li><li><p><code>action</code>：</p><ul><li>符号：<span class="math inline">\(a_i\)</span></li><li>解释：action的概念都是基于state，即当前state为了达到下一个state所采取的一个动作，在grid-world里任意一个state的action都是上/下/左右/不动</li></ul></li><li><p><code>action-space</code>：</p><ul><li>符号：<span class="math inline">\(\mathcal{A}(s_i) =\{a_i\}\)</span></li><li>解释：action-space的概念也是基于state，即当前state所有action的集合，在grid-world里所有state的action-space都是上下左右不动五个动作</li></ul></li><li><p><code>state-transition</code>：</p><ul><li><p>符号：<span class="math inline">\(s_1 \stackrel{a_3}{\to}s_2\)</span></p></li><li><p>解释：从某个state，通过其某个action，转移到另一个state的过程</p></li><li><p>简单的state-transition可以用表格的形式表现出来：</p><p><img src="2.png" /></p></li><li><p>还可以用条件概率来描述state-transition：<spanclass="math inline">\(p(s_2 | s_1, a_3) = 1\)</span>（当前在<spanclass="math inline">\(s_1\)</span>通过<spanclass="math inline">\(a_3\)</span>，跳到<spanclass="math inline">\(s_2\)</span>的概率是1）</p></li></ul></li><li><p><code>policy</code>：</p><ul><li><p>解释：告诉agent当它处于某state时，应该采取什么action</p></li><li><p>符号：</p><ul><li><span class="math inline">\(\pi(a_1 | s_1) = 0\)</span>：在<spanclass="math inline">\(s_1\)</span>下，采取<spanclass="math inline">\(a_1\)</span>策略的概率是0</li><li><span class="math inline">\(\pi(a_2 | s_1) = 0.5\)</span>：在<spanclass="math inline">\(s_1\)</span>下，采取<spanclass="math inline">\(a_2\)</span>策略的概率是0.5</li><li><span class="math inline">\(\sum_{i=1}^{m} \pi(a_i | s_1) =1\)</span></li></ul></li><li><p>简单的policy也可以用表格的形式表现出来：</p><p><img src="3.png" /></p></li></ul></li><li><p><code>reward</code>：</p><ul><li>解释：在agent处于某个state，采取某个action后得到的一个real number<ul><li>A positive reward represents encouragement to take suchactions.</li><li>A negative reward represents punishment to take such actions.</li><li>reward依赖于state + action</li></ul></li><li>符号：<span class="math inline">\(r_i\)</span></li><li>同样可以用条件概率来表达reward：<span class="math inline">\(p(r=1 |s_1, a_1)\)</span> = 1</li></ul></li><li><p><code>trajectory</code>：</p><ul><li>解释：一条“state-action-reward”的链</li><li>符号：<span class="math inline">\(s_1 \xrightarrow[r=0]{a_2} s_2\xrightarrow[r=0]{a_3} s_5 \xrightarrow[r=0]{a_3} s_8\xrightarrow[r=1]{a_2} s_9\)</span></li></ul></li><li><p><code>return</code>：</p><ul><li>解释：沿着一条trajectory的reward总和</li></ul></li><li><p><code>discount rate</code>：</p><ul><li>符号：<span class="math inline">\(\gamma, 0 &lt; \gamma &lt;1\)</span></li><li>解释： 一个控制着agent策略“近视”/“远视”的参数<ul><li>在trajectory路上每一个新得到一个reward都叠加一个<spanclass="math inline">\(\gamma\)</span>，即<spanclass="math inline">\(\gamma r_1 + \gamma^2 r_2 + \cdots + \gamma^kr_k\)</span></li><li>若<spanclass="math inline">\(\gamma\)</span>越接近0，则agent的策略会更加关注眼前结果</li><li>若<spanclass="math inline">\(\gamma\)</span>越接近1，则agent的策略会考虑的更长远</li></ul></li></ul></li><li><p><code>discount return</code>：</p><ul><li>解释：沿着一条trajectory的叠加过discount rate后的reward总和</li></ul></li><li><p><code>episode</code>：</p><ul><li>解释：一段有terminal state的trajectory</li></ul></li></ul><h3 id="二.-mdp">二. MDP</h3><p>MDP，Markov decisionprocess，马尔可夫决策过程，是一个描述RL的框架。</p><p>MDP的组成如下：</p><ul><li>Sets（集合）<ul><li>state-space：<span class="math inline">\(\mathcal{S}\)</span></li><li>action-space：<spanclass="math inline">\(\mathcal{A}(s)\)</span></li><li>reward-space：<span class="math inline">\(\mathcal{R}(s,a)\)</span></li></ul></li><li>Probability distribution（概率分布）<ul><li><p>state transition probability：<spanclass="math inline">\(p(s&#39; | s, a)\)</span></p></li><li><p>reward probability：<span class="math inline">\(p(r | s,a)\)</span></p></li><li><p>policy：<span class="math inline">\(\pi(a | s)\)</span></p></li></ul></li><li>Markov property（马尔可夫性质）<ul><li><span class="math inline">\(p(s_{t+1} | a_{t + 1}, s_t, \cdots, a_1,s_0) = p(s_{t+1} | a_{t+1}, s_t)\)</span></li><li><span class="math inline">\(p(r_{t+1} | a_{t+1}, s_t, \cdots, a_1,s_0) = p(r_{t+1} | a_{t+1}, s_t)\)</span></li><li>即每一步transition后的state和reward的概率都是与历史无关的，只与action的当前这一步有关</li></ul></li></ul><h3 id="三.-贝尔曼公式">三. 贝尔曼公式</h3><h4 id="引入">引入</h4><p>前面我们学到了<code>return</code>这个概念，<code>return</code>其实就是起点到最终一路上的reward之和。那么记忆化的思想，我们可以给每个state定义一个return，不妨叫做<spanclass="math inline">\(v_i\)</span>。在grid-world例子里，每个格子都有自己的<spanclass="math inline">\(v\)</span>。</p><p><img src="4.png" style="zoom:67%;" /></p><p>对于上图，很容易可以写出下列的递推式： <span class="math display">\[\begin{cases}    &amp;v_1 = r_1 + \gamma v_2 \\    &amp;v_2 = r_2 + \gamma v_3 \\    &amp;v_3 = r_3 + \gamma v_4 \\    &amp;v_4 = r_4 + \gamma v_1 \\\end{cases}\]</span> 不妨写成矩阵形式： <span class="math display">\[v = r + \gamma \begin{bmatrix} 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0&amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 1 \\ 1 &amp; 0 &amp; 0 &amp;0\end{bmatrix} v \\ \therefore (I - \gamma P)v = r\]</span> 若<span class="math inline">\((I - \gammaP)\)</span>可逆，则可求出<span class="math inline">\(v\)</span>。</p><p>上面的公式其实就是贝尔曼公式，只不过是非常简单的对于确定性问题的bellmanequation.</p><h4 id="state-value">state value</h4><p>回顾一下单步转移的过程： <span class="math display">\[S_t \xrightarrow{A_t} R_{t+1}, S_{t+1}\]</span></p><ul><li><span class="math inline">\(S_t\)</span>：当前的状态</li><li><span class="math inline">\(A_t\)</span>：当前采取的action</li><li><span class="math inline">\(R_{t+1}\)</span>：在<spanclass="math inline">\(S_t\)</span>采取<spanclass="math inline">\(A_t\)</span>后获得的reward。这里写<spanclass="math inline">\(R_t\)</span>也行，但是习惯写为<spanclass="math inline">\(R_{t+1}\)</span>。</li><li><span class="math inline">\(S_{t+1}\)</span>：下一步的状态</li><li>Note：这里的<span class="math inline">\(S_t, R_{t},A_t\)</span>均为随机变量</li></ul><p>再回顾一下MDP里的Probability distribution：</p><ul><li><span class="math inline">\(S_t \to A_t\)</span>由<spanclass="math inline">\(\pi(A_t = a | S_t = s)\)</span>决定</li><li><span class="math inline">\(S_t, A_t \to R_{t+1}\)</span>由<spanclass="math inline">\(p(R_{t+1} = r | S_t = s, A_t =a)\)</span>决定</li><li><span class="math inline">\(S_t, A_t \to S_{t+1}\)</span>由<spanclass="math inline">\(p(S_{t+1}=s&#39; | S_t = s, A_t =a)\)</span>决定</li></ul><p>再回顾一下"引入"里<spanclass="math inline">\(v_i\)</span>里的概念，这里我们给它一个具体的符号：<spanclass="math inline">\(G_t\)</span></p><ul><li>对于一个<span class="math inline">\(S_t\)</span>，其discountedreturn，即<span class="math inline">\(G_t := R_{t+1} + \gamma R_{t+2} +\gamma^2 R_{t+3} + \cdots\)</span></li><li><spanclass="math inline">\(G_t\)</span>也是一个随机变量。因为因为policy的随机性，以及<spanclass="math inline">\(R_{t}\)</span>同为随机变量。</li></ul><p>现在，可以引出state value的定义了。</p><p>state value就是某个状态的discountedreturn的期望值，用数学符号表示就是：<span class="math inline">\(v_\pi(s)= \mathbb{E}(G_t | S_t = s)\)</span></p><p>Hummm，statevalue是个很好的衡量工具，它能告诉我一个state的期望discountedreturn是多少，这样就可以衡量一个state是否具有价值了。</p><p>那它跟return有什么区别？return是针对一个确定性problem来说的，但是statevalue套了概率与期望。</p><p><img src="5.png" style="zoom:50%;" /></p><p>上面是一个state value的例子，三个图代表三种policy。每种policy下<spanclass="math inline">\(s_1\)</span>的state value计算如上。</p><h4 id="bellman-equation-derivation">Bellman equation: Derivation</h4><p>贝尔曼公式用一句话来概括，就是它描述了不同state value之间的关系。</p><p>下面我们来推导一下贝尔曼公式： <span class="math display">\[\begin{align*}    v_\pi(s)&amp;=\mathbb{E}(G_t | S_t = s) \\            &amp;=\mathbb{E}(R_{t+1} + \gamma G_{t+1} | S_t = s) \\            &amp;=\mathbb{E}(R_{t+1} | S_t = s) +\gamma\mathbb{E}(G_{t+1} | S_t = s)\end{align*}\]</span> 先分析第一项： <span class="math display">\[\begin{align*}    \mathbb{E}(R_{t+1} | S_t = s) &amp;= \sum_{a} \left( \pi(a |s)\left( \sum_{r}p(r | s, a)r \right) \right)\end{align*}\]</span> 上面推导的思路就是你的<spanclass="math inline">\(R_{t+1}\)</span>是依赖于<spanclass="math inline">\(S_t, A_t\)</span>的，所以先把<spanclass="math inline">\(A_t\)</span>搞出来，然后有了<spanclass="math inline">\(a, s\)</span>后，再把<spanclass="math inline">\(r\)</span>搞出来</p><p>再分析第二项： <span class="math display">\[\begin{align*}    \mathbb{E}(G_{t+1} | S_t = s) &amp;= \sum_{s&#39;}\left( p(s&#39; |s)\mathbb{E}(G_{t+1} | S_{t+1}=s&#39;) \right) \\    &amp;=\sum_{s&#39;}\left( p(s&#39; | s)v_\pi(s&#39;)\right) \\    &amp;=\sum_{s&#39;}\left( v_\pi(s&#39;) \cdot \sum_{a}\left( \pi(a |s)p(s&#39; | s, a) \right) \right)\end{align*}\]</span> 上面的推导思路就是首先你要走到<spanclass="math inline">\(s&#39;\)</span>，然后<spanclass="math inline">\(p(s&#39;|s)\)</span>又可以展开，即先要有<spanclass="math inline">\(a\)</span>，才能基于<span class="math inline">\(s,a\)</span>走到<span class="math inline">\(s&#39;\)</span></p><p>第二项其实还可以这样推导： <span class="math display">\[\begin{align*}    \mathbb{E}(G_{t+1} | S_t = s) &amp;= \sum_{a}\left( \pi(a|s) \cdot\sum_{s&#39;}\left( p(s&#39;|s,a) \cdot \mathbb{E}(G_{t+1} |S_{t+1}=s&#39;) \right) \right) \\    &amp;=\sum_{a}\left( \pi(a|s) \cdot \sum_{s&#39;}\left(p(s&#39;|s,a) \cdot v_\pi(s&#39;) \right) \right)\end{align*}\]</span> 上面的思路是首先要有<spanclass="math inline">\(a\)</span>，这样才可以走到某个<spanclass="math inline">\(s&#39;\)</span></p><p>那么合并，即可得到： <span class="math display">\[\begin{align*}v_\pi(s) &amp;= \sum_{a} \left( \pi(a | s)\left( \sum_{r}p(r | s, a)r\right) \right) + \gamma \sum_{a}\left( \pi(a|s) \cdot\sum_{s&#39;}\left( p(s&#39;|s,a) \cdot v_\pi(s&#39;) \right) \right) \\    &amp;=\sum_{a}\left( \pi(a|s)\left[ \sum_{r}p(r|s,a)r + \gamma\sum_{s&#39;}p(s&#39;|s,a)v_\pi(s&#39;) \right] \right), \quad s \in\mathcal{S}.\end{align*}\]</span> 上面就是<strong>贝尔曼公式</strong>。</p><p>推导出来后我们来直观理解下这个式子，首先，当前state的statevalue是多少呢？</p><p>因为statevalue是期望，所以就要考虑到所有策略，每个策略会有一个value，所以statevalue就是： <span class="math display">\[\sum_{a}(\pi(a|s) \cdot \text{value})\]</span>那么value是多少呢？用记忆化的思想，就是当前这一步的reward期望，加上<spanclass="math inline">\(\gamma\)</span>乘后续的reward期望。</p><p>当前这一步的reward期望就是： <span class="math display">\[\sum_{r}(p(r|s,a)r)\]</span> 后续的reward期望是多少呢？其实就是下一步的statevalue，那么就要确定下一步的<spanclass="math inline">\(s&#39;\)</span>，所以后续的reward期望就是： <spanclass="math display">\[\sum_{s&#39;}(p(s&#39;|s,a)v_\pi(s&#39;))\]</span> Interesting，right？</p><p>观察bellman equation，可以发现<spanclass="math inline">\(v_\pi(s)\)</span>由三个东西决定：<spanclass="math inline">\(\pi(a|s), p(r|s,a), p(s&#39;|s,a)\)</span></p><p>这仨恰好是MDP里的probabilitydistribution，这就是为什么MDP框架里要抽象出这仨，因为它们很关键。</p><h4 id="bellman-equation-matrix-vector-form">Bellman equation:Matrix-vector form</h4><p>上一个小节我们已经求出了bellman equation： <spanclass="math display">\[v_\pi(s) = \sum_{a}\pi(a|s)\left[ \sum_{r}p(r|s,a)r +\gamma\sum_{s&#39;}p(s&#39;|s,a)v_\pi(s&#39;) \right]\]</span>这是易于理解的，但是做算法/求解的时候，我们需要将其写为矩阵形式。</p><p>首先可以把<spanclass="math inline">\(\sum_{a}\pi(a|s)\)</span>乘进去，得到 <spanclass="math display">\[v_\pi(s) = r_\pi(s) + \gamma \sum_{s&#39;}p_\pi(s&#39; | s)v_\pi(s&#39;)\\r_\pi(s) := \sum_{a}\pi(a|s)\sum_{r}p(r|s,a)r, \quad\quadp_\pi(s&#39;|s) := \sum_{a}\pi(a|s)p(s&#39;|s,a)\]</span></p><ul><li><spanclass="math inline">\(r_\pi(s)\)</span>表示在当前state走一步所能得到的reward的期望</li><li><span class="math inline">\(p_\pi(s&#39;|s)\)</span>表示在当前<spanclass="math inline">\(s\)</span>走到下一步状态<spanclass="math inline">\(s&#39;\)</span>的概率</li></ul><p>Suppose the states could be indexed as <spanclass="math inline">\(s_i~(i=1,2,\cdots,n)\)</span></p><p>For state <span class="math inline">\(s_i\)</span>, the Bellmanequation is <span class="math display">\[v_\pi(s_i) = r_\pi(s_i) + \gamma \sum_{s_j}p_\pi(s_j | s_i)v_\pi(s_j).\]</span> Then, put all these equations for all the states together andrewrite to a matrix-vector form: <span class="math display">\[v_\pi = r_\pi + \gamma P_\pi v_\pi,\]</span> where</p><ul><li><span class="math inline">\(v_\pi = \left[ v_\pi(s_1), \cdots,v_\pi(s_n) \right]^\mathrm{T} \in \mathbb{R}^n\)</span></li><li><span class="math inline">\(r_\pi = \left[ r_\pi(s_1), \cdots,r_\pi(s_n) \right]^\mathrm{T} \in \mathbb{R}^n\)</span></li><li><span class="math inline">\(P_\pi \in \mathbb{R}^{n \timesn}\)</span>, where <span class="math inline">\([P_\pi]_{ij} = p_\pi(s_j| s_i)\)</span></li></ul><p>熟悉的感觉right？回到了推第一篇论文时候的感觉。</p><p>上面的实际意义很好理解，就是一个状态的statevalue等于走一步的reward期望，加上<spanclass="math inline">\(\gamma\)</span>乘<spanclass="math inline">\(\sum\)</span>(走到第j个点的概率 <spanclass="math inline">\(\cdot\)</span> 第j个点出发的state value)</p><p><span class="math inline">\(v_\pi\)</span>就是state values，<spanclass="math inline">\(r_\pi\)</span>叫当前期望reward们，<spanclass="math inline">\(P_\pi\)</span>叫状态转移矩阵</p><p>下面是一个展开形式的矩阵形式：</p><p><img src="6.png" style="zoom: 67%;" /></p><p>所以只要确定了MDP里的probability distribution：<spanclass="math inline">\(\pi(a|s), p(r|s,a),p(s&#39;|s,a)\)</span>，我们就可求出<span class="math inline">\(r_\pi,P_\pi\)</span>，则可以用算出<spanclass="math inline">\(v_\pi\)</span></p><p>来看一个例子：</p><p><img src="7.png" style="zoom: 50%;" /></p><p>OK继续，如果用线代的方法求<spanclass="math inline">\(v_\pi\)</span>，需要算矩阵的逆，这个计算量是很大的，所以通常我们用迭代法来求state-values：</p><p>We have <span class="math inline">\(v_{k+1} = r_\pi + \gamma P_\piv_k\)</span>，we can show that <span class="math inline">\(v_k \to v_\pi= (I - \gamma P_\pi)^{-1}r_\pi, k \to \infty.\)</span></p><blockquote><p>Proof：</p><p>因为<spanclass="math inline">\(P_\pi\)</span>是一个马尔可夫矩阵，所以收敛性是显然可证的。</p><p>先列出已有的条件：</p><p>递推式：<span class="math inline">\(v_{k+1} = r_\pi + \gamma P_\piv_k \quad (1)\)</span></p><p>bellman equation：<span class="math inline">\(v_\pi = r_\pi + \gammaP_\pi r_\pi \quad (2)\)</span></p><p>令<span class="math inline">\(\delta_k = v_k -v_\pi\)</span>，则<span class="math inline">\(v_k = \delta_k + v_\pi\quad (3)\)</span></p><p>将(3)代入(1)，得：<span class="math inline">\(\delta_{k+1} + v_\pi =r_\pi + \gamma P_\pi(\delta_k + v_\pi)\)</span></p><p><span class="math inline">\(\therefore \delta_{k+1} = -v_\pi + (r_\pi+ \gamma P_\pi v_\pi) + \gamma P_\pi \delta_k\)</span></p><p>将(2)代入上式，得：<span class="math inline">\(\delta_{k+1} = \gammaP_\pi \delta_k\)</span></p><p><span class="math inline">\(\therefore \delta_k = \gamma^k P_\pi^k\delta_0, k \to \infty\)</span></p><p>因为<spanclass="math inline">\(P_\pi\)</span>是马尔可夫矩阵，所有其幂次同样是马尔可夫矩阵，所以其每个元素均<spanclass="math inline">\(\in [0, 1]\)</span>，又<spanclass="math inline">\(0 &lt; \gamma &lt; 1\)</span>，所以<spanclass="math inline">\(\gamma^k P_\pi^k, k \to\infty\)</span>是一个零矩阵，所以<span class="math inline">\(\delta_k =\textbf{0}, k \to \infty\)</span></p><p>证毕</p></blockquote><p>OK！现在我们知道了bellman equation的矩阵形式，并通过迭代法算出了statevalues。那么state values有什么用呢？</p><p>答案：用来评估我们的策略表现是否优秀。</p><p>来看下面这个例子：</p><p><img src="8.png" /></p><p>上图有三个子图，每个子图对应一种policy。图1的policy是很好的按计划走到targetcell，图2的policy是一直往右走，图3的policy是随机生成的。</p><p>人眼可以知道，图1的policy最好，图3的policy很一般，图2的policy很差。</p><p>通过确定policy，也就是<spanclass="math inline">\(\pi(a|s)\)</span>，通常<spanclass="math inline">\(p(s&#39;|s,a),p(r|s,a)\)</span>是模型已知的，那么就可以通过bellman equation算出statevalues。已标注在图上。</p><p>可以发现，图1的state values都是正数而且比较大，图2的statevalues都是负的，图3的state values有正有负。</p><p>所以从state values，我们就可以看出一个policy好不好。</p><h4 id="action-value">Action value</h4><p>action value和state value的区别是什么？</p><ul><li>state value：从一个state出发，所得到的average discounted return</li><li>action value：从一个state出发，执行一个action后，所得到的averagediscounted return<ul><li>符号：<span class="math inline">\(q_\pi(s, a) = \mathbb{E}(G_t | S_t= s, A_t = a)\)</span></li></ul></li></ul><p>state value与action value的联系： <span class="math display">\[v_\pi(s) = \sum_{a}\pi(a|s)q_\pi(s, a)\]</span> 上式与bellman equation中<spanclass="math inline">\(v_\pi(s)\)</span>表达式对比，可发现： <spanclass="math display">\[q_\pi(s, a) = \sum_{r}p(r|s, a)r + \gamma \sum_{s&#39;}p(s&#39; | s,a)v_\pi(s&#39;)\]</span> 所以当我们计算出state values后，可以通过上式来算出actionvalues</p><p>上式的实际意义也很好理解，就是一个actionvalue等于走一步的reward期望<spanclass="math inline">\(\sum_{r}p(r|s,a)r\)</span>，加上<spanclass="math inline">\(\gamma\)</span>乘后续的reward期望<spanclass="math inline">\(\sum_{s&#39;}p(s&#39;|s,a)v_\pi(s&#39;)\)</span></p><h4 id="总结">总结</h4><p>这一章最重要的概念就是state value和action value。</p><p>符号表示分别为：<span class="math inline">\(v_\pi(s) = \mathbb{E}(G_t| S_t = s)\)</span>，<span class="math inline">\(q_\pi(s, a) =\mathbb{E}(G_t | S_t = s, A_t = a)\)</span></p><p>state value就是从一个state出发，所得到的average discountedreturn；actionvalue就是从一个state出发，执行一个action后，所得到的average discountedreturn。</p><p>想求解state values需通过bellman equation，bellmanequation的单点形式如下： <span class="math display">\[v_\pi(s_i) = r_\pi(s_i) + \gamma \sum_{s_j}p_\pi(s_j | s_i)v_\pi(s_j)\]</span> 矩阵形式如下： <span class="math display">\[v_\pi = r_\pi + \gamma P_\pi v_\pi \\v_\pi = \left[ v_\pi(s_1), \cdots, v_\pi(s_n) \right]^\mathrm{T} \in\mathbb{R}^n \\r_\pi = \left[ r_\pi(s_1), \cdots, r_\pi(s_n) \right]^\mathrm{T} \in\mathbb{R}^n \\P_\pi \in \mathbb{R}^{n \times n}, \text{ where }[P_\pi]_{ij} =p_\pi(s_j | s_i)\]</span> 所以可解出：<span class="math inline">\(v_\pi = (I - \gammaP_\pi)^{-1}r_\pi\)</span></p><p>但是通常为了节约时间开销选用迭代法求解state values： <spanclass="math display">\[v_{k+1} = r_\pi + \gamma P_\pi v_k \\v_k \to v_\pi = (I - \gamma P_\pi)^{-1}r_\pi, k \to \infty.\]</span> 求解出state values有什么用呢？可以评估policy是好是坏。</p><p>为什么这么说，因为bellman equation本质就是通过MDP里Probabilitydistribution的<span class="math inline">\(\pi(a|s), p(r|s,a),p(s&#39;|s,a)\)</span>求出<spanclass="math inline">\(v_i\)</span>-s。</p><p>而通常<span class="math inline">\(p(r|s,a),p(s&#39;|s,a)\)</span>是已知的（即模型已知），所以<spanclass="math inline">\(v_i\)</span>-s就可以反映出<spanclass="math inline">\(\pi(a|s)\)</span>（即policy）表现如何。</p><h3 id="四.-贝尔曼最优公式">四. 贝尔曼最优公式</h3><h4 id="引入-1">引入</h4><p>对于一个模型确定的问题（即<span class="math inline">\(p(s&#39;|s,a),p(r|s,a)\)</span>已知），我们可以在确定一种policy（<spanclass="math inline">\(\pi(a|s)\)</span>）情况下，算出基于当前policy的statevalues，进而算出action values。</p><p>假设我当前处于<spanclass="math inline">\(s\)</span>，因为我已经知道了actionvalues，所以我已经知道了<span class="math inline">\(q_\pi(s, a_1),q_\pi(s,a_2),q_\pi(s,a_3)\)</span>（假设只有3个策略）。那么从直觉上，我肯定选择最大的<spanclass="math inline">\(q_\pi\)</span>对应的action作为我当前这一步的action。</p><p>你的直觉是对的。通常我们的做法是：</p><p>算出action values后，对于每一个state: <spanclass="math inline">\(s\)</span>，都选择其最大的<spanclass="math inline">\(q_\pi(s, a_*)\)</span>对应的<spanclass="math inline">\(a_*\)</span>作为它新的policy，即<spanclass="math inline">\(\pi(a_{old}|s)\)</span>变为<spanclass="math inline">\(\pi(a_*|s)\)</span>。相当于得到了一个新的policy'，那么一直这样迭代下去，最后迭代出的policy就是最优的。</p><p>这就是直观上的理解，那么如果用数学语言解释上面的现象的话，就需要用到bellmanoptimality equation（BOE）。</p><h4 id="boe">BOE</h4><p>之前我们反复提到了一个policy是否好坏是由statevalue来衡量的，这里我们给出数学定义：</p><p>If <span class="math inline">\(v_{\pi_1}(s) \ge v_{\pi_2}(s)\)</span>for all <span class="math inline">\(s \in \mathcal{S}\)</span>，then<span class="math inline">\(\pi_1\)</span> is "better" than <spanclass="math inline">\(\pi_2\)</span>.</p><p>A policy <span class="math inline">\(\pi^*\)</span> is optimal if<span class="math inline">\(v_{\pi^*}(s) \ge v_\pi(s)\)</span> for all<span class="math inline">\(s\)</span> and for any other policy <spanclass="math inline">\(\pi\)</span>.</p><p>那么我就要提出许多问题了：</p><ol type="1"><li>最优策略 <span class="math inline">\(\pi^*\)</span>是否存在？</li><li>最优策略是否unique？</li><li>最优策略是stochastic的还是deterministic的？</li><li>如何得到最优策略</li></ol><p>不急，慢慢来，这些问题都会得到解决。</p><p>我先给出bellman optimality equation： <span class="math display">\[\begin{align*}v(s) &amp;= \max_{\pi} \sum_{a} \pi(a|s) \left( \sum_{r}p(r|s,a)r +\gamma\sum_{s&#39;}p(s&#39;|s,a)v(s&#39;) \right) \\&amp;= \max_{\pi} \sum_{a} \pi(a|s) q(s,a)\end{align*}\]</span> 上面的式子其实就是bellmanequation发现没？但是有两点不同，第一，符号由<spanclass="math inline">\(v_\pi(s)\)</span>改为<spanclass="math inline">\(v(s)\)</span>了；第二，前面加了个<spanclass="math inline">\(max_\pi\)</span>前缀。</p><p>第一点不同，其实没什么原因，因为前人愿意。</p><p>第二点不同，你需要理解<spanclass="math inline">\(max_{\pi}(\cdots)\)</span>的意思。它意思就是说你的<spanclass="math inline">\(\pi\)</span>随便取，然后给我搞出<spanclass="math inline">\(\cdots\)</span>最大就行。</p><p>所以<span class="math inline">\(v(s) =max_{\pi}\sum_{a}\pi(a|s)q(s,a)\)</span>这一个式子应该这样理解：</p><p>就是首先你得找到一个<spanclass="math inline">\(\pi\)</span>，使得右边这一坨<spanclass="math inline">\(\sum_{a} \pi(a|s)q(s,a)\)</span>最大，也就是最大化<spanclass="math inline">\(v(s)\)</span>。而非常巧的是，你找到的这个<spanclass="math inline">\(\pi\)</span>，它刚好是一个最优policy。因为根据前面最优policy的定义：<spanclass="math inline">\(v_{\pi^*}(s) \gev_\pi(s)\)</span>，此时我们的<spanclass="math inline">\(v(s)\)</span>已经是最大了，所以此时的policy就是最优的。</p><p>上面的BOE可以写为矩阵形式： <span class="math display">\[v = \max_{\pi}(r_\pi + \gamma P_\pi v)\]</span> （<span class="math inline">\(r_\pi + \gamma P_\piv\)</span>是一个向量，给其套一个<spanclass="math inline">\(\max_{\pi}\)</span>的意思是给其每一个分量都套一个<spanclass="math inline">\(\max_{\pi}\)</span>）</p><p>不妨令<span class="math inline">\(f(v) = max_\pi(r_\pi + \gamma P_\piv)\)</span></p><p>那么BOE就可写成： <span class="math display">\[v = f(v)\]</span></p><blockquote><p>补充三个数学小知识：</p><ol type="1"><li>The definition of fixed point：<ul><li><span class="math inline">\(x \in X\)</span> is a fixed point of<span class="math inline">\(f\)</span>: <span class="math inline">\(X\to X\)</span> if <span class="math inline">\(f(x)=x\)</span></li></ul></li><li>Contraction mapping：<ul><li><span class="math inline">\(f\)</span> is a contraction mapping if<span class="math inline">\(\| f(x_1) - f(x_2) \| \le \gamma \| x_1 -x_2 \|\)</span>, where <span class="math inline">\(\gamma \in (0,1)\)</span>.</li></ul></li><li>Contraction Mapping Theorem：<ul><li>For any equation that has the form of <spanclass="math inline">\(x=f(x)\)</span>, if <spanclass="math inline">\(f\)</span> is a contraction mapping, then:<ul><li>Existence: there exists a fixed point <spanclass="math inline">\(x^*\)</span> satisfying <spanclass="math inline">\(f(x^*)=x^*\)</span></li><li>Uniqueness: The fixed point <span class="math inline">\(x^*\)</span>is unique.</li><li>Algorithm: Consider a sequence <spanclass="math inline">\(\{x_k\}\)</span> where <spanclass="math inline">\(x_{k+1}=f(x_k)\)</span>, then <spanclass="math inline">\(x_k \to x^*\)</span> as <spanclass="math inline">\(k \to \infty\)</span>. Moreover, the convergencerate is exponentially fast.</li></ul></li></ul></li></ol></blockquote><p>OK，我们其实可以证明出<spanclass="math inline">\(f(v)\)</span>是一个contractionmapping（证明见书），那么通过contraction mappingtheorem，我们就可以知道，它必然存在不动点解，而且通过迭代法迭代出的<spanclass="math inline">\(v^*\)</span>就是不动点解（即收敛），且不动点唯一。</p><p>那么现在就剩两个问题了：</p><ol type="1"><li><span class="math inline">\(f(v)\)</span>如何求解，即<spanclass="math inline">\(\max_{\pi}(r_\pi + \gamma P_\piv)\)</span>如何求解，进一步说，即<span class="math inline">\(\max_{\pi}\sum_{a} \pi(a|s) q(s,a)\)</span>如何求解？</li><li>如何证明这个unique不动点解<spanclass="math inline">\(v^*\)</span>就是最优的？</li></ol><p>先解决第二个问题，可以由下面这个Theorem解决：</p><blockquote><p>Policy optimality theorem：</p><p>Suppose that <span class="math inline">\(v^*\)</span> is the uniquesolution to <span class="math inline">\(v = \max_\pi(r_\pi + \gammaP_\pi v)\)</span>, and <span class="math inline">\(v_\pi\)</span> is thestate value function satisfying <span class="math inline">\(v_\pi =r_\pi + \gamma P_\pi v_\pi\)</span> for any given policy <spanclass="math inline">\(\pi\)</span>, then: <spanclass="math inline">\(v^* \ge v_\pi, \forall \pi\)</span></p><p>Proof:</p><p>见书</p></blockquote><p>那么只剩一个问题了，<span class="math inline">\(\max_{\pi} \sum_{a}\pi(a|s) q(s,a)\)</span>如何求？</p><p>因为我是要用迭代法求<span class="math inline">\(v =f(v)\)</span>的，所以初始的state values: <spanclass="math inline">\(v_0\)</span>我是已知的。所以相当于初始的actionvalues: <span class="math inline">\(q_0\)</span>我是已知的，将<spanclass="math inline">\(\sum_{a} \pi(a|s) q(s,a)\)</span>展开：<spanclass="math inline">\(\pi(a_1|s)q_0(s,a_1) + \pi(a_2|s)q_0(s,a_2) +\cdots\)</span>。</p><p>显然要使上面那一坨最大，我就要使最大那个q值最大的action: <spanclass="math inline">\(a&#39;\)</span>，让其权重分配为1: <spanclass="math inline">\(\pi(a&#39;|s)=1\)</span>。其余权重都为0即可。</p><p>那么这样就求出了<span class="math inline">\(\max_{\pi} \sum_{a}\pi(a|s) q(s,a)\)</span>，问题解决。</p><p>至此，我们已经可以通过BOE：<span class="math inline">\(v =f(v)\)</span>，求出最优state value: <spanclass="math inline">\(v^*\)</span>以及对应的最优policy: <spanclass="math inline">\(\pi^*\)</span>，算法如下：</p><ol type="1"><li>设定好<span class="math inline">\(\gamma\)</span>，<spanclass="math inline">\(r\)</span>，<spanclass="math inline">\(p(r|s,a)\)</span>，<spanclass="math inline">\(p(s&#39;|s,a)\)</span></li><li>随意取一个<span class="math inline">\(v_0\)</span>，然后通过<spanclass="math inline">\(q_\pi(s, a) = \sum_{r}p(r|s, a)r + \gamma\sum_{s&#39;}p(s&#39; | s, a)v_\pi(s&#39;)\)</span>算出对应的<spanclass="math inline">\(q_0\)</span></li><li>For each state <span class="math inline">\(s_i\)</span>, at time<span class="math inline">\(k\)</span>：<ul><li>算出<span class="math inline">\(q_{k}(s_i, a)\)</span></li><li>Find the <span class="math inline">\(a_k^*(s_i)\)</span>, s.t, <spanclass="math inline">\(q_k(s_i, a_k^*(s_i))\)</span>最大</li><li><span class="math inline">\(\pi_{k+1}(a|s_i)=\begin{cases} 1 \quada=a_k^*(s_i) \\ 0 \quad a \ne a_k^*(s_i) \end{cases}\)</span></li><li><span class="math inline">\(v_{k+1}(s_i) =\sum_{a}\pi_{k+1}(a|s)q_k(s,a)\)</span></li></ul></li></ol><p>这一小节的内容非常非常重要，对后续做数值仿真以及理解RL的基本思想很有意义。其实不难理解，静下心，多看几遍。</p><p>至此我们可以回答开头的那几个问题：</p><ol type="1"><li>最优policy当然存在。因为根据fixed point theorem，我们知道<spanclass="math inline">\(v^*\)</span>存在且唯一，那么其对应的policy就是<spanclass="math inline">\(\pi^*\)</span></li><li>不一定唯一。我们只能保证<spanclass="math inline">\(v^*\)</span>存在且唯一，但是其对应的policy不一定唯一。因为初始<spanclass="math inline">\(v_0\)</span>会不同</li><li>根据我们的algorithm，显然<spanclass="math inline">\(\pi^*\)</span>是deterministic的</li><li>根据上面给出的algorithm，即可得到最优策略</li></ol><h4 id="gamma-r的选择"><span class="math inline">\(\gamma,r\)</span>的选择</h4><p>通过前面的所学，我们知道我们需要确定好<spanclass="math inline">\(\gamma\)</span>, <spanclass="math inline">\(r\)</span>, <spanclass="math inline">\(p(r|s,a)\)</span>, <spanclass="math inline">\(p(s&#39;|s,a)\)</span>, <spanclass="math inline">\(v_0\)</span>，即可通过BOE求解出<spanclass="math inline">\(v^*, \pi^*\)</span></p><p><span class="math inline">\(v_0\)</span>是随便设定的，不用管；<spanclass="math inline">\(p(r|s,a)\)</span>, <spanclass="math inline">\(p(s&#39;|s,a)\)</span>通常是先验已知的，所以也不用管。</p><p>所以关键就是<span class="math inline">\(\gamma,r\)</span>这俩参数的选择会给我们的<span class="math inline">\(v_*,\pi^*\)</span>带来什么样的影响。</p><p><img src="9.png" style="zoom:67%;" /></p><p>看上面这个参数设置，算出来的<spanclass="math inline">\(\pi^*\)</span>左图，<spanclass="math inline">\(v^*\)</span>右图。可以发现，对于红色的几个点，它们都选择穿过forbidden到达targetcell。这是因为<spanclass="math inline">\(\gamma\)</span>的取值比较大，policy会比较远视。不会太计较短期的得失，而是考虑长期的回报。</p><p><img src="10.png" style="zoom:67%;" /></p><p>看上面这个参数设置，<spanclass="math inline">\(\gamma\)</span>变小了，所以policy会比较计较近处的得失，所以会绕过forbiddencell。</p><p><img src="11.png" style="zoom:67%;" /></p><p>再来看这个图，它的参数设置与第一幅图一样，但是把forbidden的惩罚增加了。这样同样可以达到绕过forbiddencell的效果。</p><p>这里补充一个知识，就是<spanclass="math inline">\(r\)</span>如果统一进行线性变化：<spanclass="math inline">\(r&#39; = ar +b\)</span>，其余条件不变的情况下，<spanclass="math inline">\(\pi^*\)</span>是不会有任何改变的。具体证明见书。</p><h4 id="总结-1">总结</h4><p>这一节所说的BOE就是在bellman equation前套了个<spanclass="math inline">\(\max_\pi\)</span></p><p>最重要的是要掌握fixed point theorem + Policy optimalitytheorem，这样就知道了为啥通过迭代即可算出来最优<spanclass="math inline">\(v^*, \pi^*\)</span>了</p><p>然后还需要掌握迭代算法的流程：</p><ol type="1"><li>设定好<span class="math inline">\(\gamma\)</span>，<spanclass="math inline">\(r\)</span>，<spanclass="math inline">\(p(r|s,a)\)</span>，<spanclass="math inline">\(p(s&#39;|s,a)\)</span></li><li>随意取一个<span class="math inline">\(v_0\)</span>，然后通过<spanclass="math inline">\(q_\pi(s, a) = \sum_{r}p(r|s, a)r + \gamma\sum_{s&#39;}p(s&#39; | s, a)v_\pi(s&#39;)\)</span>算出对应的<spanclass="math inline">\(q_0\)</span></li><li>For each state <span class="math inline">\(s_i\)</span>, at time<span class="math inline">\(k\)</span>：<ul><li>算出<span class="math inline">\(q_{k}(s_i, a)\)</span></li><li>Find the <span class="math inline">\(a_k^*(s_i)\)</span>, s.t, <spanclass="math inline">\(q_k(s_i, a_k^*(s_i))\)</span>最大</li><li><span class="math inline">\(\pi_{k+1}(a|s_i)=\begin{cases} 1 \quada=a_k^*(s_i) \\ 0 \quad a \ne a_k^*(s_i) \end{cases}\)</span></li><li><span class="math inline">\(v_{k+1}(s_i) =\sum_{a}\pi_{k+1}(a|s)q_k(s,a)\)</span></li></ul></li></ol><p>最后，要知道参数<span class="math inline">\(\gamma,r\)</span>的选择会对policy产生什么样的影响。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;基本概念、MDP、贝尔曼公式、贝尔曼最优公式&lt;/p&gt;</summary>
    
    
    
    <category term="1. 科研" scheme="http://error666.top/categories/1-%E7%A7%91%E7%A0%94/"/>
    
    <category term="强化学习" scheme="http://error666.top/categories/1-%E7%A7%91%E7%A0%94/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>微分方程2</title>
    <link href="http://error666.top/2024/10/02/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B2/"/>
    <id>http://error666.top/2024/10/02/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B2/</id>
    <published>2024-10-02T07:17:35.000Z</published>
    <updated>2024-10-04T15:52:49.915Z</updated>
    
    <content type="html"><![CDATA[<p>复数、复化解一阶常系数线性ODE、二阶常系数齐次线性ODE、二阶齐次线性ODE相关理论、二阶非齐次线性ODE相关理论</p><span id="more"></span><hr /><h3 id="一.-复数和复指数">一. 复数和复指数</h3><p>”数学就是一场符号的游戏，实际问题都是运用符号建的模。“</p><p>复数同理，虚部有什么实际含义吗？我想，目前暂且无需关心，当实际遇到需要建模的时候再谈。</p><p>那么，虚数就是<span class="math inline">\(i\)</span>，且满足<spanclass="math inline">\(i^2 = 1\)</span>。</p><p>所以复数可以这么表达：<span class="math inline">\(a +bi\)</span>，其中<span class="math inline">\(a\)</span>是实部，<spanclass="math inline">\(bi\)</span>是虚部。这是复数的代数表达形式。</p><p>聪明的人类希望可视化复数，于是用复平面（其实就是坐标系）来描述复数：</p><p><img src="1.png" style="zoom: 50%;" /></p><p>当我们把复数放到复平面的时候，我们就可以用坐标来描述一个复数：（a,b），<del>虽然没有啥人这么描述就是了。</del></p><p>人们更喜欢用极坐标去描述，因为在复平面上，如果知道了<spanclass="math inline">\(\theta\)</span>和<spanclass="math inline">\(r\)</span>，那么相当于就知道了一个复数。</p><p>可以发现，对于复数<span class="math inline">\(a +bi\)</span>，其模<span class="math inline">\(r = \sqrt{a^2 +b^2}\)</span>，其辐角<span class="math inline">\(\theta = \arctan\frac{b}{a}\)</span></p><p>所以，假设我知道了一个复数的模和辐角，那么其可以表示为：<spanclass="math inline">\(r(\cos \theta + i\sin\theta)\)</span>，这就是复数的极坐标表达形式。</p><p>Hummm，仍然不够简洁，right？</p><p>欧拉站出来了，给出了欧拉公式：<span class="math inline">\(\cos \theta+ i\sin \theta = e^{i\theta}\)</span></p><p>我觉得欧拉公式不要从等价推导的角度去理解它，而是要从映射的角度去理解它。如果两个东西，不管外界对它们对什么样的刺激，它们产生的影响都是相同的，那从抽象的角度我们就可以认为这两个东西是等价的。上面那个公式说的就是这么个事情。</p><p>欧拉公式的右侧，也就是指数，我们很熟悉了right？其至少满足两个性质：</p><ol type="1"><li><span class="math inline">\(e^x \cdot e^y = e^{x + y}\)</span></li><li><span class="math inline">\(\frac{dy}{dx} = ay, \quad y =e^{ax}\)</span></li></ol><p>那么假设欧拉公式是正确的，去推一推指数的这两个性质，发现也是满足的，那么我们就有理由的认为，这两者在抽象层面上是“等价的”，即这个公式是正确的。<span class="math display">\[\begin{align*}&amp;~~~~(\cos \theta_1 + i\sin \theta_1)(\cos \theta_2 + i\sin\theta_2) \\&amp;=\cos \theta_1 \cos \theta_2 - \sin \theta_1 \sin \theta_2 + i(\sin\theta_1 \cos \theta_2 + \cos \theta_1 \sin \theta_2) \\&amp;=\cos(\theta_1 + \theta_2) + i\sin(\theta_1 + \theta_2) \\&amp;=e^{i(\theta_1 + \theta_2)} \\\end{align*}\]</span>所以我们有理由说明，这个公式是正确的（严谨的证明这里没必要讨论）。</p><p>所以，有了欧拉公式的加持，我们可以将复数的极坐标形式改写，得到：<spanclass="math inline">\(re^{i\theta}\)</span>，这就是复数的指数表达形式。</p><p>至此，对于一个复数，我们有了三种表达方式：</p><ol type="1"><li><span class="math inline">\(a + bi\)</span></li><li><span class="math inline">\(r(\cos \theta + i \sin \theta), r =\sqrt{a^2 + b^2}, \theta = \arctan \frac{b}{a}\)</span></li><li><span class="math inline">\(re^{i\theta}, r = \sqrt{a^2 + b^2},\theta = \arctan \frac{b}{a}\)</span></li></ol><hr /><p>复数很有用，我们来看两个例子。</p><p>第一个是求解<span class="math inline">\(\int e^{-x}\cosx\mathrm{d}x\)</span></p><p>常规做法是两次分部积分法，但是可以将积分”复化“来做。</p><p>将<span class="math inline">\(\cos x\)</span>看作是复数<spanclass="math inline">\(e^{i\theta}\)</span>的实部，不妨记为<spanclass="math inline">\(Re(e^{ix})\)</span></p><p>复数如果乘一个实数，那么就是实部虚部分别乘这个实数，所以<spanclass="math inline">\(e^{-x}\cos x\)</span>，其实就是<spanclass="math inline">\(e^{-x} \cdote^{ix}\)</span>这个复数的实部，即<span class="math inline">\(Re(e^{-x +ix})\)</span></p><p>那么积分可写为：<span class="math inline">\(\int Re(e^{-x + ix})\mathrm{d}x\)</span></p><p><span class="math inline">\(Re\)</span>可提到积分号外面：<spanclass="math inline">\(Re \int e^{-x + ix} \mathrm{d}x\)</span></p><p>于是可得：<span class="math inline">\(Re(\frac{1}{i-1}e^{-x + ix} +c)\)</span></p><p>即求复数<span class="math inline">\(\frac{1}{i-1}e^{-x +ix}\)</span>的实部，（最后记得加个c），整理：<spanclass="math inline">\(e^{-x} \cdot \frac{\cos x + i\sin x}{i-1} = e^{-x}\cdot \frac{\cos x - \sin x + i(\cos x + \sin x)}{-2}\)</span></p><p>由于我们只需要实部，所以答案就是：<span class="math inline">\(e^{-x}\cdot \frac{\cos x - \sin x}{-2} + c\)</span></p><p>Humm，巧妙。</p><hr /><p>再来看一个例子，我们知道，对于<spanclass="math inline">\(\sqrt[n]{1}\)</span>，在实数范围内，如果n是奇数，那么只有一个解1，如果是正数，那么解为<spanclass="math inline">\(\pm1\)</span>。</p><p>但是在复数域，<spanclass="math inline">\(\sqrt[n]{1}\)</span>有n个解，这是很容易解释的，用复平面就可以很好的解释。</p><p>因为对于俩复数相乘，即<span class="math inline">\(r_1e^{i\theta_1}\cdot r_2e^{i\theta_2} =r_1r_2e^{i(\theta_1+\theta_2)}\)</span>，在复平面上来看，其实就是模相乘作为新的模，然后辐角相加作为新的辐角。</p><p>所以<spanclass="math inline">\(\sqrt[n]{1}\)</span>的解，其实就是n个自己相乘，最后在复平面上落到（1，0）处。</p><p>因为单位圆上的复数的模都是1，所以无需考虑模了。只需考虑辐角，哪些辐角的单位复数，n次自乘后会落到（1，0）？</p><p>答案是：<span class="math inline">\(e^{i \cdot 2\pi \cdot\frac{k}{n}}, k = 1,2,\cdots,n\)</span></p><p>即这n个复数，它们的n次方就是实数1。（从复平面角度考虑，模永远是1，但是辐角相加n次后都为<spanclass="math inline">\(2\pi\)</span>的倍数）</p><p>这n个复数恰好是单位圆上的n等分点。</p><h3 id="二.-复化解带三角函数的一阶常系数线性ode">二.复化解带三角函数的一阶常系数线性ODE</h3><p>在“微分方程1”中“一阶ODE解析法”的例3中，我们介绍了这种特殊的一阶线性ODE，其系数是常数，即一阶常系数线性ODE：<span class="math display">\[y&#39; + ky = kq(t)\]</span> 因为它毕竟是一阶线性ODE，所以可以用通法去解它。</p><p>在例3中，我们已经解出其通解为：<span class="math inline">\(\thereforeT = ke^{-kt}\int q(t) e^{kt} \mathrm{d}t + ce^{-kt}\)</span></p><p>但是当<spanclass="math inline">\(q(t)\)</span>为三角函数的时候，其实还可以将其“复化”去解决。</p><p>下面做一道例题：<span class="math inline">\(y&#39; + ky = k\coswt\)</span></p><p>看到三角函数，直接把它复化了：<span class="math inline">\(y&#39; + ky= k Re(e^{iwt})\)</span></p><p>一直带着<spanclass="math inline">\(Re\)</span>有点烦，所以不妨将方程左侧的解先换为“复数解”，则有：<spanclass="math inline">\(\tilde{y}&#39; + k\tilde{y} =ke^{iwt}\)</span></p><p>利用解一阶ODE的通法，解出<span class="math inline">\(u = e^{\int kdx}= e^{kt}\)</span></p><p><span class="math inline">\(\therefore (u\tilde{y})&#39; = ke^{iwt +kt}\)</span></p><p><span class="math inline">\(\therefore e^{kt} \cdot \tilde{y} =\frac{k}{iw + k}e^{iwt + kt} + c\)</span></p><p><span class="math inline">\(\therefore \tilde{y} = \frac{1}{1 +i(\frac{w}{k})}e^{iwt} + c\)</span></p><p>所以要不然就把<span class="math inline">\(\frac{1}{1 +i(\frac{w}{k})}\)</span>转为指数形式，要不然就把<spanclass="math inline">\(e^{iwt}\)</span>转为<span class="math inline">\(a+ bi\)</span>形式。</p><p>这里我们选用前者去做，<span class="math inline">\(1 +i\frac{w}{t}\)</span>是复数，<span class="math inline">\(\frac{1}{1 +i\frac{w}{t}}\)</span>显然也是复数，问题是它的模和辐角是多少？</p><p>因为复数相乘本质就是模相乘，辐角相加，而我们又知道<spanclass="math inline">\(\frac{1}{1 + i\frac{w}{t}} \cdot (1 +i\frac{w}{t}) = 1\)</span> <span class="math display">\[\therefore \begin{cases}    \arg(\alpha) + \arg(\frac{1}{\alpha}) = \arg(1) = 0 \\    \mod(\alpha) \cdot \mod(\frac{1}{\alpha}) = \mod(1) = 1 \\    \alpha = 1 + i\frac{w}{t}\end{cases}\]</span></p><p><span class="math display">\[\therefore \begin{cases}    \arg(\frac{1}{\alpha}) = -\arctan\frac{w}{t} = - \phi \\    \mod(\frac{1}{\alpha}) = \frac{1}{\sqrt{1 + (\frac{w}{t})^2}}\end{cases}\]</span></p><p><span class="math inline">\(\therefore \frac{1}{1 + i(\frac{w}{t})} =\frac{1}{\sqrt{1 + (\frac{w}{t})^2}}e^{-i\phi}, \phi =\arctan\frac{w}{t}\)</span></p><p><span class="math inline">\(\therefore \tilde{y} = \frac{1}{\sqrt{1 +(\frac{w}{t})^2}}e^{i(wt-\phi)} + c\)</span></p><p><span class="math inline">\(\therefore y = Re(\tilde{y}) =\frac{1}{\sqrt{1 + (\frac{w}{t})^2}} \cdot \cos(wt - \phi) + c, \quad\phi = \arctan\frac{w}{t}\)</span></p><hr /><p>ok，那现在换一种做法，也就是把<spanclass="math inline">\(e^{iwt}\)</span>转为<span class="math inline">\(a+ bi\)</span>形式。</p><p>回到这一步：<span class="math inline">\(\therefore \tilde{y} =\frac{1}{1 + i(\frac{w}{k})}e^{iwt} + c\)</span></p><p><span class="math inline">\(\therefore \tilde{y} = \frac{1 -i(\frac{w}{k})}{1 + (\frac{w}{k})^2} \cdot (\cos wt + i\sin wt) +c\)</span></p><p><span class="math inline">\(\therefore \tilde{y} = \frac{1}{1 +(\frac{w}{k})^2} \cdot (\cos wt + \frac{w}{k}\sin wt) + c\)</span></p><blockquote><p>补充：辅助角公式</p><p><span class="math inline">\(a\cos \alpha + b\sin \alpha = \sqrt{a^2 +b^2}\cos(\alpha - \phi), \phi = \arctan\frac{b}{a}\)</span></p><p>证明：</p><p><span class="math inline">\(a\cos \alpha + b\sin\alpha\)</span>可以表示为<span class="math inline">\((a - bi)(\cos\alpha + i\sin \alpha)\)</span>的实部</p><p>将这个复数指数化：<span class="math inline">\(\sqrt{a^2 +b^2}e^{-i\theta} \cdot e^{i\alpha} = \sqrt{a^2 + b^2} \cdot e^{i(\alpha- \theta)}\)</span></p><p>所以这个复数的实部就是：<span class="math inline">\(\sqrt{a^2 +b^2}\cos(\alpha - \theta), \theta =\arctan\frac{n}{a}\)</span>，证毕。</p><p>为什么复数指数化的时候是<spanclass="math inline">\(-\theta\)</span>？因为我们考虑符号的正负很烦，所以通常我们都假设<spanclass="math inline">\(a, b&gt;0\)</span>去做，那么复数<spanclass="math inline">\(a -bi\)</span>就在复平面的下方，那么对应的辐角就是一个负的，因为<spanclass="math inline">\(\phi =\arctan\frac{b}{a}\)</span>在假设下为正，所以要给它加个负号</p></blockquote><p><span class="math inline">\(\therefore \tilde{y} = \frac{1}{1 +(\frac{w}{k})^2} \cdot \sqrt{1 + (\frac{w}{k})^2} \cdot \cos(wt - \phi)+ c, \phi = \arctan\frac{w}{k}\)</span></p><p><span class="math inline">\(\therefore \tilde{y} = \frac{1}{\sqrt{1 +(\frac{w}{k})^2}} \cdot \cos(wt - \phi) + c, \quad \phi =\arctan\frac{w}{t}\)</span></p><h3 id="三.-二阶常系数齐次线性ode">三. 二阶常系数齐次线性ODE</h3><p>前面学习了：</p><ol type="1"><li>一阶线性ODE：<span class="math inline">\(y&#39; + p(x)y =q(x)\)</span></li><li>伯努利方程：<span class="math inline">\(y&#39; = p(x)y +q(x)y^n\)</span></li><li>一阶齐次ODE：<span class="math inline">\(y&#39; =f(\frac{y}{x})\)</span></li><li>一阶自治ODE：<span class="math inline">\(y&#39; = f(y)\)</span></li><li>一阶常系数线性ODE：<span class="math inline">\(y&#39; + ky =kq(x)\)</span></li></ol><p>今天来学习形如<span class="math inline">\(y&#39;&#39; + Ay&#39; + By=0\)</span>的方程，因为方程右边是0，所以叫齐次，所以这类方程叫：“二阶常系数齐次线性ODE”</p><p>首先先说结论，对于二阶ODE，其通解必然为：<spanclass="math inline">\(y = c_1y_1 + c_2y_2\)</span>，<spanclass="math inline">\(y_1,y_2\)</span>线性无关。想起了线性代数right？Ahahah</p><p>那么想解<span class="math inline">\(y&#39;&#39; + Ay&#39; + By =0\)</span>，我们的任务就是找出<span class="math inline">\(y_1,y_2\)</span></p><p>欧拉已经发现了，对于二阶常系数齐次线性ODE，其解形如：<spanclass="math inline">\(e^{rx}\)</span></p><p>将其代入试一下：<span class="math inline">\(r^2 \cdot e^{rx} + Ar\cdot e^{rx} + B \cdot e^{rx} = 0\)</span></p><p><span class="math inline">\(\therefore r^2 + Ar + B = 0\)</span></p><p>所以解出<spanclass="math inline">\(r\)</span>就行了，上面这个方程也叫二阶常系数齐次线性ODE的“特征方程”。</p><p>那么其实有几种情况：</p><ol type="1"><li><span class="math inline">\(r_1 \ne r_2\)</span>且都是realnumber</li><li><span class="math inline">\(r1, r2\)</span>都是复数</li><li><span class="math inline">\(r1 = r2\)</span>且是real number</li></ol><p>我们一个一个来分析。</p><p><strong>Case 1：</strong></p><p>首先先讨论第一种情况，此时俩特解就是<spanclass="math inline">\(e^{r_1x}, e^{r_2x}\)</span>，那么通解就是<spanclass="math inline">\(y = c_1e^{r_1x} + c_2e^{r_2x}\)</span></p><p><strong>Case 2：</strong></p><p>然后讨论第二种情况，此时不妨设特征方程的根是<spanclass="math inline">\(a \pm bi\)</span>，那么俩特解就是<spanclass="math inline">\(e^{(a+bi)x},e^{(a-bi)x}\)</span>，但是显然我们不希望通解里包含复数啊，怎么办呢？用下面这个定理</p><blockquote><p>定理：若<span class="math inline">\(u + vi\)</span>是<spanclass="math inline">\(y&#39;&#39; + Ay&#39; + By =0\)</span>的解，那么<span class="math inline">\(u,v\)</span>都是此方程的解</p><p>证明：</p><p><span class="math inline">\(\because (u+vi)&#39;&#39; + A(u+vi)&#39;+ B(u+vi) = 0\)</span></p><p><span class="math inline">\(\therefore (u&#39;&#39; + Au&#39; + Bu) +i(v&#39;&#39; + Av&#39; + Bv) = 0\)</span></p><p><span class="math inline">\(\therefore u, v\)</span> are thesolutions of the equation.</p></blockquote><p>而<span class="math inline">\(e^{(a\pmbi)x}\)</span>对应的复数是<span class="math inline">\(e^{ax} \cdot (\cosbx \pm i\sin bx)\)</span></p><p>所以<span class="math inline">\(e^{ax}\cos bx\)</span>和<spanclass="math inline">\(e^{ax}\sin bx\)</span>也是方程的俩特解（其实<spanclass="math inline">\(-e^{ax}\sinbx\)</span>也是，不过只需要俩线性无关的就行，所以任选一个）</p><p>所以通解为：<span class="math inline">\(y = e^{ax}(c_1\cos bx +c_2\sin bx)\)</span></p><p>（这里教授用弹簧-阻尼-木块模型描述了这个方程的物理现象，就是在不断震荡，趋近于稳态但不会到稳态。即震荡现象与特征方程复数根联系在一起）</p><p>除了用定理外，还有另一种方法同样可以得到实数解，回到得到俩特解<spanclass="math inline">\(e^{(a\pmbi)x}\)</span>这一步，那么通解可写为：<span class="math inline">\(y =c_1e^{(a+bi)x} + c_2e^{(a-bi)x}\)</span></p><p>这确实是通解，但是我们研究的问题是在实数域中的，所以我们希望求出实数通解，也就是令<spanclass="math inline">\(c_1, c_2\)</span>取某些值时，s,t, <spanclass="math inline">\(y\)</span>为实数</p><p>Well，这里用一个小trick，即实数的共轭复数就是它自己。所以假定<spanclass="math inline">\(y\)</span>为实数，然后取其共轭： <spanclass="math display">\[\overline{y} = \overline{c_1e^{(a+bi)x} + c_2e^{(a-bi)x}} =\overline{c_1e^{(a+bi)x}} + \overline{c_2e^{(a-bi)x}} =\overline{c_1}e^{(a-bi)x} + \overline{c_2}e^{(a+bi)x} = y\]</span> <span class="math inline">\(\therefore\overline{c_1}=c_2,\overline{c_2} = c_1\)</span></p><p>所以通解即为：<span class="math inline">\(y = (u + iv)e^{(a+bi)x} +(u - iv)e^{(a-bi)x}\)</span></p><p>工程领域的人很多人喜欢写成上面这个形式。</p><p>但是hummm，我还是觉得写为三角函数会更优雅直观些，我们来看看上面的形式如何转为三角形式</p><blockquote><p>补充：逆欧拉公式</p><p><span class="math inline">\(\cos \alpha = \frac{e^{i\alpha} +e^{-i\alpha}}{2}\)</span></p><p><span class="math inline">\(\sin \alpha = \frac{e^{i\alpha} -e^{-i\alpha}}{2i}\)</span></p></blockquote><p><span class="math display">\[\begin{align*}    y&amp;=(u + iv)e^{(a+bi)x} + (u - iv)e^{(a-bi)x} \\     &amp;=e^{ax}(ue^{ibx} + ive^{ibx} + ue^{-ibx} - ive^{-ibx}) \\     &amp;=e^{ax}\left(u(e^{ibx} + e^{-ibx}) + iv(e^{ibx} -e^{-ibx})\right) \\     &amp;=e^{ax}\left(2u\cos bx - 2v\sin bx\right)\end{align*}\]</span></p><p><strong>Case 3：</strong></p><p>最后讨论第三种情况，此时不妨设特征方程的重根为<spanclass="math inline">\(r\)</span>，那么特解就是<spanclass="math inline">\(e^{rx}\)</span>，另一个特解是啥呢？继续用一个定理</p><blockquote><p>定理：若知道了<span class="math inline">\(y&#39;&#39; + p(x)y&#39; +q(x)y = 0\)</span>的一个解<spanclass="math inline">\(y_1\)</span>，那么另一个解必然可以写成<spanclass="math inline">\(u(x)y_1\)</span></p></blockquote><p>ok，来找一下这个u吧！</p><p>首先我们的方程是<span class="math inline">\(y&#39;&#39; + Ay&#39; +By = 0\)</span>，然后其中一个解<spanclass="math inline">\(y_1\)</span>为<spanclass="math inline">\(e^{rx}\)</span>，另一个解<spanclass="math inline">\(y_2\)</span>为<spanclass="math inline">\(ue^{rx}\)</span></p><p>求出<span class="math inline">\(y, y_2&#39;, y_2&#39;&#39;\)</span><span class="math display">\[\begin{cases}    &amp;y_2 = ue^{rx} \\    &amp;y_2&#39; = u&#39;e^{rx} + ure^{rx} \\    &amp;y_2&#39;&#39; = u&#39;&#39;e^{rx} + 2u&#39;re^{rx} + ur^2e^{rx}\end{cases}\]</span> 因为是重根，所以<span class="math inline">\(A = -2r, B =r^2\)</span></p><p><span class="math inline">\(\therefore y_2&#39;&#39; + Ay_2&#39; +By_2 = u&#39;&#39;e^{rx} + 2u&#39;re^{rx} + ur^2e^{rx} - 2ru&#39;e^{rx}- 2r^2ue^{rx} + r^2ue^{rx} = 0\)</span></p><p><span class="math inline">\(\therefore u&#39;&#39;e^{rx} =0\)</span></p><p><span class="math inline">\(\therefore u = c_1x + c_2\)</span></p><p>因为我们只需要求出<spanclass="math inline">\(y_2\)</span>的一个特解，所以<spanclass="math inline">\(u\)</span>不妨取<spanclass="math inline">\(x\)</span>，这样即得到<spanclass="math inline">\(y_2 = xe^{rx}\)</span></p><p>所以通解为：<span class="math inline">\(y = c_1e^{rx} +c_2xe^{rx}\)</span></p><h3 id="四.-二阶齐次线性ode相关理论">四. 二阶齐次线性ODE相关理论</h3><p>本节讨论的方程形如：<span class="math inline">\(y&#39;&#39; +p(x)y&#39; + q(x)y = 0\)</span></p><p>这节课教授介绍了为什么二阶齐次线性ODE的通解是<spanclass="math inline">\(c_1y_1 +c_2y_2\)</span>，证明过程我这里就略了。</p><p>以及还提到了对通解的正交化。</p><p>什么意思呢？就是当你求出俩通解<spanclass="math inline">\(y\)</span>后，可以令<spanclass="math inline">\(y(0) = 1, y&#39;(0) =0\)</span>，解出一个特解，记为<spanclass="math inline">\(Y_1\)</span>；然后再令<spanclass="math inline">\(y(0) = 0, y&#39;(0) =1\)</span>，解出一个特解，记为<spanclass="math inline">\(Y_2\)</span>。</p><p>那么<span class="math inline">\(Y_1,Y_2\)</span>就是正交的，通解可以重新写为<span class="math inline">\(y =c_1Y_1 + c_2Y_2\)</span></p><p>这样有什么好处呢？</p><p>当你给出一个初始条件<span class="math inline">\(y(0) = a, y&#39;(0) =b\)</span>时，那么解就是<span class="math inline">\(aY_1 +bY_2\)</span></p><p>除了正交化，教授还讲了一个存在和唯一性定理：</p><blockquote><p>存在和唯一性定理：</p><p>对于二阶齐次线性ODE：<span class="math inline">\(y&#39;&#39; +p(x)y&#39; + q(x)y = 0\)</span>，若<span class="math inline">\(p,q\)</span>对<spanclass="math inline">\(x\)</span>连续，则当给定一组初始条件时，有且仅有一个解。</p></blockquote><h3 id="五.-二阶非齐次线性ode相关理论">五.二阶非齐次线性ODE相关理论</h3>]]></content>
    
    
    <summary type="html">&lt;p&gt;复数、复化解一阶常系数线性ODE、二阶常系数齐次线性ODE、二阶齐次线性ODE相关理论、二阶非齐次线性ODE相关理论&lt;/p&gt;</summary>
    
    
    
    <category term="4. 大学" scheme="http://error666.top/categories/4-%E5%A4%A7%E5%AD%A6/"/>
    
    <category term="数学" scheme="http://error666.top/categories/4-%E5%A4%A7%E5%AD%A6/%E6%95%B0%E5%AD%A6/"/>
    
    <category term="微分方程" scheme="http://error666.top/categories/4-%E5%A4%A7%E5%AD%A6/%E6%95%B0%E5%AD%A6/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/"/>
    
    
  </entry>
  
  <entry>
    <title>微分方程1</title>
    <link href="http://error666.top/2024/09/18/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B1/"/>
    <id>http://error666.top/2024/09/18/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B1/</id>
    <published>2024-09-18T10:05:34.000Z</published>
    <updated>2024-10-02T07:17:10.580Z</updated>
    
    <content type="html"><![CDATA[<p>几何法解一阶ODE、欧拉数值法解一阶ODE、分离变量法解一阶ODE、解标准一阶线性ODE、换元法解伯努利方程+一阶齐次ODE、一阶自治ODE图像分析</p><span id="more"></span><p>mit 18.03 Arthur Mattuck教授讲的微分方程。</p><hr /><h3 id="导言">导言</h3><p>常微分方程（ordinary differential equation,ODE）：函数的自变量只有一个，通常是时间</p><p>举个最简单的例子，考虑平抛问题中的垂直方向，向下为正。</p><p><span class="math inline">\(\because \dot{v} = g \quad \therefore v =gt + v_0\)</span></p><p><span class="math inline">\(\because \dot{y} = v = gt + v_0 \quad\therefore y = \frac12gt^2 + v_0t + y_0\)</span></p><p>上面这个简单的例子，其实就是在求解微分方程：<spanclass="math inline">\(\ddot{y} = g\)</span></p><p>这个微分方程非常简单，且求解非常容易。但往往实际中的大部分问题，你只能列出微分方程，但是却无法精确求解它。例如下面单摆这个例子：</p><p><img src="1.png" style="zoom:50%;" /></p><p>规定以中间垂直线为标准，向右边的角度为正，左边为负。考虑切向加速度<spanclass="math inline">\(a\)</span>，加速度前的负号表示它总指向与位移相反的方向。</p><p>可以根据图片写出一些公式： <span class="math display">\[\begin{align*}    \because x &amp;= L\theta \\    \because a &amp;= -g\sin \theta \\    \therefore \ddot{x} &amp;= -g\sin\theta \\    \therefore L\ddot{\theta} &amp;= -g\sin\theta \\    \therefore \ddot{\theta} &amp;= -\frac{g}{L}\sin\theta\end{align*}\]</span> 我们成功写出了一个关于<spanclass="math inline">\(\theta(t)\)</span>的微分方程。为了更加负号实际，我们引入空气阻力，于是微分方程变为：<span class="math display">\[\ddot{\theta} = -\mu\dot{\theta} - \frac{g}{L}\sin\theta\]</span>很好。单摆这个微分方程，是十分难求解的。既然我们求不出它的解析解，那我们如果仅通过这个方程窥探摆运动的规律呢？</p><p>既然上面那个微分方程体现了摆的运动，那我们先将这个微分方程可视化出来，具体来说，我们可以以x轴为<spanclass="math inline">\(\theta\)</span>，y轴为<spanclass="math inline">\(\dot{\theta}\)</span>。即有向量坐标<spanclass="math inline">\((\theta,\dot{\theta})\)</span>。对向量坐标求导，得到<spanclass="math inline">\((\dot{\theta},\ddot{\theta})\)</span>向量，它表示了当前该坐标在图上变化的方向和大小。</p><p>这样子的话，我们对平面上所有坐标<span class="math inline">\((\theta,\dot{\theta})\)</span>，把对应的<spanclass="math inline">\((\dot{\theta},\ddot{\theta})\)</span>向量平移到以<span class="math inline">\((\theta,\dot{\theta})\)</span>为起点上来。就可以得到一副这样的图：</p><p><img src="2.png" style="zoom:50%;" /></p><p>（为了保持美观，向量的长度保持了一样，通过颜色来区分向量的长度）</p><p>这幅图会引发我们很多的思考，可以发现，原点代表着摆的角度和角加速度都为0，即静止状态。通过旋流来看显然最后的状态都会回归静止状态，这是符合实际的。我们还会发现，在<spanclass="math inline">\((\pi, 0)\)</span>位置的旋流是静止的，说明<spanclass="math inline">\((\pi,0)\)</span>是个不动点。那它代表什么物理含义呢？它代表小球在正上方保持平衡。well，根据实际我们可以知道，这确实是个可保持静止的地方，但但凡受到一点扰动，就会打破这个平衡。</p><p>所以我们可以得出结论：<span class="math inline">\((0, 0), (\pi,0)\)</span>都是不动点，但<span class="math inline">\((0,0)\)</span>是稳定点，而<span class="math inline">\((\pi,0)\)</span>不是。</p><p>非常有趣！问题来了，如何绘制出这一幅图？</p><p>我们已知的是微分方程为<span class="math inline">\(\ddot{\theta} =-\mu\dot{\theta} - \frac{g}{L}\sin\theta\)</span>，且坐标<spanclass="math inline">\((\theta, \dot{\theta})\)</span>上的向量为<spanclass="math inline">\((\dot{\theta}, \ddot{\theta})\)</span>。</p><p>那么我们可以任意选取某点<span class="math inline">\((\theta_0,\dot{\theta}_0)\)</span>，然后根据微分方程算出<spanclass="math inline">\(\ddot{\theta}\)</span>。即可得到当前坐标上的向量<spanclass="math inline">\((\dot{\theta}_0,\ddot{\theta}_0)\)</span>。这个向量就体现了该坐标的移动趋势，所以我们将该坐标朝着对应向量方向移动一个小距离<spanclass="math inline">\(\Delta t\)</span>，即可得到新点<spanclass="math inline">\((\theta_1,\dot{\theta_1})\)</span>。重复这个步骤，即可画出一条轨迹。多次取不同的初始点进行绘制，即可得到若干条轨迹。全部的轨迹合起来就是上述那张图。</p><p>Well，讲到这里，相信你已经感受到微分方程的魅力了。通过小小方程，即可窥探事物运行的规律，没有比这更令人兴奋的事情了！</p><h3 id="一.-ode几何方法">一. ODE几何方法</h3><p>如何用作图描述ODE呢？</p><p>使用方向场。跟前言中使用的方法类似，假设目前我们有：<spanclass="math inline">\(y&#39; = f(x, y)\)</span></p><p>那我们可以作一幅二维图，x轴就是x，y轴就是y，坐标<spanclass="math inline">\((x,y)\)</span>上是一个短线（称为"线素"），这条短线的斜率就是<spanclass="math inline">\(y&#39;\)</span></p><p>那么，画出一条与所有线素相切的曲线，就是微分方程<spanclass="math inline">\(y&#39;=f(x,y)\)</span>的一个解，称之为"积分曲线"</p><p>这显然是正确的，因为对于一条画出来的曲线，其任意一点都与线素相切，说明其斜率也就是<spanclass="math inline">\(y&#39;\)</span>跟给定的<spanclass="math inline">\(y&#39;=f(x,y)\)</span>一致，也就是这条曲线符合微分方程，所以它自然就是解。</p><p>例如这个微分方程：<span class="math inline">\(y&#39; =-\frac{x}{y}\)</span>。它用方向场画出来的积分曲线如下：</p><p><img src="3.png" style="zoom:67%;" /></p><p>（<span class="math inline">\(C = 0\)</span> means <spanclass="math inline">\(y&#39; = 0\)</span>）</p><p>再来一个例子：<span class="math inline">\(y&#39; = 1 + x -y\)</span></p><p><img src="4.png" /></p><p>注意到在<span class="math inline">\(C=2\)</span>和<spanclass="math inline">\(C=0\)</span>两条之间的区域，当积分曲线进入这个区域后就再也出不去了，解函数无法逃逸。另一个需要注意的要点是，积分曲线永远不会相交。如果两曲线相交的话，则在交点处就会有两条切线、两个斜率，这与微分方程不符。因此进入此区域的曲线，无法逃逸也无法相交，只能够互相靠近，朝向<spanclass="math inline">\(y=x\)</span>的直线靠拢。</p><p>事实上，在方向场上的积分曲线满足"存在与唯一性定理"：</p><ul><li>存在性：若<span class="math inline">\(y\)</span>在<spanclass="math inline">\((x_0, y_0)\)</span>的领域内连续，则通过<spanclass="math inline">\((x_0, y_0)\)</span>的<spanclass="math inline">\(y&#39;=f(x, y)\)</span>有解。</li><li>唯一性：若<span class="math inline">\(y&#39;\)</span>在<spanclass="math inline">\((x_0, y_0)\)</span>的领域为连续，则通过<spanclass="math inline">\((x_0, y_0)\)</span>的<spanclass="math inline">\(y&#39; = f(x, y)\)</span>有且仅有唯一解。</li></ul><h3 id="二.-ode欧拉数值法及推广">二. ODE欧拉数值法及推广</h3><p>欧拉数值法解微分方程其实就是导言里用的方法。对于微分方程：<spanclass="math inline">\(y&#39; = f(x,y)\)</span>，如何在图上求出其中某条曲线(解)呢？首先你先得确定一个初始点<spanclass="math inline">\((x_0,y_0)\)</span>，然后从初始点出发，一步一步画出曲线。</p><p>对于坐标<span class="math inline">\((x,y)\)</span>，求导后可得到<span class="math inline">\((1,y&#39;)\)</span>向量，这个向量就是从<span class="math inline">\((x,y)\)</span>出发的移动趋势向量。朝着这个向量的方法走一个步长，一直迭代下去，就可以得到用欧拉数值法拟合出来的曲线。</p><p>那么移动后的坐标是多少呢？假设步长为<spanclass="math inline">\(\alpha\)</span>，那么可以得到公式<spanclass="math inline">\(f(x_k, y_k)\)</span>为函数在<spanclass="math inline">\(x_k\)</span>处的导数： <spanclass="math display">\[\begin{cases}    x_{k+1} = x_k + \alpha \\    y_{k+1} = y_k + \alpha \cdot f(x_k, y_k)\end{cases}\]</span>上面的方法就叫一阶欧拉数值法。（一阶的意思是你求了一次导数）</p><p>即如果你确定了初始点<span class="math inline">\((x_0,y_0)\)</span>和微分方程<span class="math inline">\(y&#39;=f(x,y)\)</span>，那么通过此方法就可求出以<span class="math inline">\((x_0,y_0)\)</span>为初始点且满足微分方程的曲线。</p><hr /><p>这个方法好不好呢？</p><p>当然是有的，显然我们知道，步长越小越精确。但再怎么精确，如果真正的函数是曲线的话，欧拉数值法也是得不到真正解的。</p><p>例如下面这个图，在<span class="math inline">\((x_k,y_k)\)</span>处是凸的(因为<span class="math inline">\(y&#39;&#39; &gt;0\)</span>)，所以在此点迭代的下一个<spanclass="math inline">\(y_{k+1}\)</span>是小于真正的<spanclass="math inline">\(y\)</span>的。</p><p><img src="5.png" /></p><p>同理，如果某点的<span class="math inline">\(y&#39;&#39; &lt;0\)</span>，那么在<span class="math inline">\((x_k,y_k)\)</span>便是凹的，因此在该点迭代的下一个<spanclass="math inline">\(y_{k+1}\)</span>是大于真正的<spanclass="math inline">\(y\)</span>的。</p><p>我们知道了数值法并不能求精确解，但是我们会希望误差最小。</p><p>所以步长与误差之间的关系是什么呢？答案是<span class="math inline">\(e\sim c\alpha\)</span>，<spanclass="math inline">\(c\)</span>是一个常数，即误差近似与步长为线性关系。</p><hr /><p>那么能不能使这个误差<spanclass="math inline">\(e\)</span>小点呢？</p><p>有的，求多几次导数就好了，例如我现在在<spanclass="math inline">\((x_k, y_k)\)</span>，然后通过该点的导数<spanclass="math inline">\(y&#39;_1\)</span>可迭代处<spanclass="math inline">\((\hat{x}_{k+1},\hat{y}_{k+1})\)</span>，然后我可以得到<spanclass="math inline">\((\hat{x}_{k+1},\hat{y}_{k+1})\)</span>的导数<spanclass="math inline">\(y&#39;_2\)</span>。</p><p>于是我将<span class="math inline">\(\frac{y&#39;_1 +y&#39;_2}{2}\)</span>作为<spanclass="math inline">\(y&#39;\)</span>，然后在<spanclass="math inline">\((x_k, y_k)\)</span>基础上迭代出<spanclass="math inline">\((x_{k+1}, y_{k+1})\)</span>。</p><p>这样得到的<span class="math inline">\((x_{k+1},y_{k+1})\)</span>会更加接近真实解。</p><p>从几何上理解也很直观，相当于取了两次方向向量的中间作为新的方向向量，然后朝着新方向向量移动一小步，这样的偏差会比原先的一阶欧拉数值法小。</p><p>因为在这个方法中，我们算了两次导数，所以该方法叫做二阶欧拉数值法。其误差与步长的关系是：<spanclass="math inline">\(e \sim c\alpha^2\)</span></p><hr /><p>同理，我们还可以求三步、四步的信息(导数)，这样误差就会进一步缩小。但是由此带来的问题是计算复杂度变大。所以天下没有免费的午餐。通常在计算机绘制微分方程解时，都是采用的四阶欧拉数值法。</p><hr /><p>欧拉数值法有没有局限性呢？显然是有的，当解函数不是连续的，而存在“奇点”时，欧拉数值法在越过奇点之后将无法拟合出正确的曲线，例如下图这个真实解函数：</p><p><img src="6.png" style="zoom: 50%;" /></p><h3 id="三.-一阶ode解析法">三. 一阶ODE解析法</h3><p>前面两节我们分别通过几何法（画线素）和欧拉数值法求解了ODE。这一节我们将用解析的方式求解一阶线性ODE。</p><h4 id="可分离变量的一阶ode">可分离变量的一阶ODE</h4><p>我们把能写为：<span class="math inline">\(y&#39; =f(x)g(y)\)</span>形式的微分方程，称为一阶ODE。</p><p>那么，x放一边，y放一边，两边同时积分，即可求出答案： <spanclass="math display">\[\because \frac{dy}{dx} = f(x)g(y) \\\therefore \frac{1}{g(y)}dy = f(x)dx \\\therefore \int \frac{1}{g(y)}dy = \int f(x)dx \\\]</span> 例题：求<span class="math inline">\(y&#39; = y\sinx\)</span>的通解 <span class="math display">\[\text{when } y \ne 0, \text{one has } \frac{1}{y}dy = y\sin x \\\therefore \int \frac{1}{y}dy = \int \sin x dx \\\therefore \ln |y| = -\cos x + c \\\therefore |y| = e^{-\cos x} \cdot e^c \\\therefore y = \pm c \cdot e^{-\cos x} \\\therefore y = c \cdot e^{-\cos x}, c \ne 0 \\\text{when } y = 0, \text{it can pass the check.} \\\therefore y = c \cdot e^{-\cos x}, c \ne 0\]</span></p><h4 id="一阶线性ode">一阶线性ODE</h4><p>可以写成：<span class="math inline">\(a(x)y&#39; + b(x)y =c(x)\)</span>，的方程叫做一阶ODE。</p><p>为什么叫“线性”呢？因为<spanclass="math inline">\(y&#39;\)</span>和<spanclass="math inline">\(y\)</span>呈线性关系，所以这样叫了。跟这种方程的感觉很像：<spanclass="math inline">\(ax + by = c\)</span></p><p>如果<spanclass="math inline">\(c(x)\)</span>为0那么上面的方程可以称为“齐次方程”</p><p>但上面的形式是一阶线性ODE的通式，其标准形式如下： <spanclass="math display">\[y&#39; + p(x)y = q(x)\]</span></p><hr /><p>一阶线性ODE在实际应用中很广泛，例如传导—扩散模型。</p><p>这个模型名字的来源是来自于两个物理现象，首先是温度传导现象：</p><p><img src="8.png" style="zoom:50%;" /></p><p>外头是某种液体，中间可能是某种介质，外边套了层铁皮。那么如果<spanclass="math inline">\(T\)</span>与<spanclass="math inline">\(T_e\)</span>不同，则会发生温度传导现象，由牛顿温度传导定律，可得到如下方程：<spanclass="math inline">\(\frac{dT}{dt} = k(T_e - T), k &gt; 0\)</span></p><p>以及浓度扩散现象：</p><p><img src="7.png" style="zoom:50%;" /></p><p>外头是某种液体，中间也是某种液体，外边套了层半透膜。那么如果<spanclass="math inline">\(C\)</span>与<spanclass="math inline">\(C_e\)</span>不同，则会发生浓度扩散现象，同样可得到方程：<spanclass="math inline">\(\frac{dC}{dt} = k(C_e - C), k &gt; 0\)</span></p><p>这就是传导—扩散模型，让我们用一个一般的数学式子来描述它： <spanclass="math display">\[\begin{cases}&amp;T&#39; + kT = kT_e \\&amp;k = p(t) \\&amp;kT_e = q(t)\end{cases}\]</span>显然，上面的式子显然是标准一阶线性ODE的写法，我们可以将其视为：<spanclass="math inline">\(y&#39; + p(x)y = q(x)\)</span></p><hr /><p>知道了一阶线性ODE的定义，以及标准形式，以及实际生活中的建模运用。现在我们想知道的，就是如何求解它。<span class="math display">\[y&#39; + py = q \quad(1)\]</span> <span class="math inline">\(p, q\)</span>均为关于<spanclass="math inline">\(x\)</span>的函数，我想找到一个关于<spanclass="math inline">\(x\)</span>的函数<spanclass="math inline">\(u\)</span>，使得(1)同时左乘<spanclass="math inline">\(u\)</span>后，能使得左边的部分可以化简为<spanclass="math inline">\((uy)\)</span>的导数，let's try： <spanclass="math display">\[uy&#39; + upy = uq \quad (2)\]</span> <span class="math inline">\(\because (uy)&#39; = uy&#39; +u&#39;y\)</span></p><p><span class="math inline">\(\therefore u&#39; = up \quad(3)\)</span></p><p>也就是我们想到的这个<spanclass="math inline">\(u\)</span>，满足(3)</p><p>(3)是可通过分离变量解的，因为<spanclass="math inline">\(\frac{du}{dx} = up(x)\)</span>，整理得：<spanclass="math inline">\(\frac{1}{u}du = p(x)dx\)</span></p><p>可解出：<span class="math inline">\(u = e^{\int p(d)dx}\)</span></p><p>ok，将算出的这个<spanclass="math inline">\(u\)</span>回代进(2)里，我们则可以得到： <spanclass="math display">\[(uy)&#39; = uq\]</span> 于是我们可以解出<spanclass="math inline">\(uy\)</span>为多少，然后方程就没有求导项了，整理下即可求出<spanclass="math inline">\(y\)</span></p><p>思路总结：</p><ol type="1"><li>求出<span class="math inline">\(u = e^{\int p(x)dx}\)</span></li><li>把<span class="math inline">\(u\)</span>左乘<spanclass="math inline">\(y&#39; + py = q\)</span>，方程左边可变为<spanclass="math inline">\((uy)&#39;\)</span></li><li>解<span class="math inline">\((uy)&#39; = uq\)</span></li></ol><hr /><p><strong>例1.</strong> <span class="math inline">\(xy&#39; - y =x^3\)</span></p><p>首先先化为标准形式：<span class="math inline">\(y&#39; - \frac1x y =x^2\)</span></p><p>然后求出<span class="math inline">\(u = e^{\int p(d)dx} = e^{\int-\frac1x dx} = e^{-\ln x} = \frac1x\)</span></p><p>对标准形式左乘<span class="math inline">\(u\)</span>：<spanclass="math inline">\((uy)&#39; = (\frac1x \cdot y)&#39; = \frac1x \cdotx^2 = x\)</span></p><p><span class="math inline">\(\therefore \frac{y}{x} = \frac12 x^2 +c\)</span></p><p><span class="math inline">\(\therefore y = \frac12 x^3 +cx\)</span></p><p><strong>例2.</strong> <span class="math inline">\((1 + \cos x)y&#39;- (\sin x)y = 2x\)</span></p><p>首先先化为标准形式：<span class="math inline">\(y&#39; - \frac{\sinx}{1 + \cos x}y = \frac{2x}{1 + \cos x}\)</span></p><p>然后求出<span class="math inline">\(u = e^{-\int \frac{\sin x}{1 +\cos x}dx} = 1 + \cos x\)</span></p><p>对标准形式左乘<span class="math inline">\(u\)</span>：<spanclass="math inline">\((uy)&#39; = ((1 + \cos x)y)&#39; = 2x\)</span></p><p><span class="math inline">\(\therefore (1 + \cos x)y = x^2 +c\)</span></p><p><span class="math inline">\(\therefore y = \frac{x^2 + c}{1 + \cosx}\)</span></p><p><strong>例3.</strong> <span class="math inline">\(T&#39; + kT = kT_e,k &gt; 0\text{ is a constant. } T_e\text{ is a function of}x\)</span></p><p>已经是标准形式了</p><p>然后求出<span class="math inline">\(u = e^{\int k dt} =e^{kt}\)</span></p><p>对标准形式左乘<span class="math inline">\(u\)</span>：<spanclass="math inline">\((uT)&#39; = (e^{kt}T)&#39; = kT_ee^{kt}\)</span></p><p><span class="math inline">\(\therefore e^{kt}T = k\int T_e e^{kt}\mathrm{d}t + c\)</span></p><p><span class="math inline">\(\therefore T = ke^{-kt}\int T_e e^{kt}\mathrm{d}t + ce^{-kt}\)</span></p><p>如果有实际物理意义，即<spanclass="math inline">\(t\)</span>从0开始，并且给定<spanclass="math inline">\(T(0) = T_0\)</span>。那么<spanclass="math inline">\(T\)</span>的积分下限就是0，上限就是<spanclass="math inline">\(t\)</span>。而且还可得到<spanclass="math inline">\(c = T_0\)</span>。则：</p><p><span class="math inline">\(T(t) = ke^{-kt}\int_0^t T_e(x) e^{kx}\mathrm{d}x + T_0e^{-kt}\)</span></p><p>可以发现，如果<span class="math inline">\(t \to\infty\)</span>时，因为<span class="math inline">\(k &gt;0\)</span>，所以<spanclass="math inline">\(T_0e^{-kt}\)</span>会收敛到0。</p><p>因此<spanclass="math inline">\(T_0e^{-kt}\)</span>叫做“暂态解”，<spanclass="math inline">\(ke^{-kt}\int_0^tT_e(x)e^{kx}dx\)</span>叫做“稳态解”</p><p>而且我们发现，当<span class="math inline">\(t \to\infty\)</span>时，<span class="math inline">\(T\)</span>与初始状态<spanclass="math inline">\(T_0\)</span>无关。</p><h3 id="四.-一阶方程换元法">四. 一阶方程换元法</h3><p>这类方程： <span class="math display">\[y&#39;=p(x)y + q(x)y^{n}, n \ne 0\]</span> 叫做“伯努利方程”。解它用换元法，如下：</p><p>同除<span class="math inline">\(y^n\)</span>，得：<spanclass="math inline">\(\frac{y&#39;}{y^n} =\frac{p(x)}{y^{n-1}}+q(x)\)</span></p><p>令<span class="math inline">\(v =\frac{1}{y^{n-1}}\)</span>，则有：<span class="math inline">\(v&#39; =(1-n)y^{-n} \cdot y&#39;\)</span></p><p><span class="math inline">\(\therefore \frac{v&#39;}{1-n} = p(x)v +q(x)\)</span></p><p>发现，线性方程出现了，那么可先解出v，然后回代解出y即可</p><p><strong>例题. </strong><span class="math inline">\(y&#39; =\frac{y}{x} - y^2\)</span></p><p>同除<span class="math inline">\(y^2\)</span>，得：<spanclass="math inline">\(\frac{y&#39;}{y^2} = x^{-1}y^{-1} - 1\)</span></p><p>令<span class="math inline">\(v = y^{-1}\)</span>，则：<spanclass="math inline">\(v&#39; = (-1)y^{-2} \cdot y&#39;\)</span></p><p><span class="math inline">\(\therefore -v&#39; = \frac{v}{x} -1\)</span></p><p><span class="math inline">\(\therefore v&#39; + \frac{v}{x} =1\)</span></p><p><span class="math inline">\(\therefore u = e^{\int \frac1x dx} =x\)</span></p><p>左乘<span class="math inline">\(u\)</span>，得：<spanclass="math inline">\((uv)&#39; = (xv)&#39; = x\)</span></p><p><span class="math inline">\(\therefore xv = \frac12x^2 +c\)</span></p><p><span class="math inline">\(\therefore v = \frac12x +\frac{c}{x}\)</span></p><p><span class="math inline">\(\therefore \frac1y = \frac{x^2 +2c}{2x}\)</span></p><p><span class="math inline">\(\therefore y =\frac{2x}{x^2+2c}\)</span></p><hr /><p>第二类用换元法解的ODE，叫一阶齐次ODE，形如： <spanclass="math display">\[y&#39; = f(\frac{y}{x})\]</span> 即等式右边的基本原子都是<spanclass="math inline">\(\frac{y}{x}\)</span></p><p>一阶齐次ODE的套路是先换元然后分离变量解决。</p><p><strong>例题. </strong>有一个贩毒船，还有一个灯塔，灯塔会对船射出光线，但是船不想被照到，于是船始终保持与光线成45°角一直逃窜，请求出船的运行轨迹。</p><p><img src="9.png" style="zoom:50%;" /></p><p>根据图，可以列出方程： <span class="math display">\[y&#39; = \tan (\frac{\pi}{4} + \alpha) = \frac{\tan \frac{\pi}{4} + \tan\alpha}{1 - \tan \frac{\pi}{4}\tan \alpha} = \frac{1 + \tan \alpha}{1 -\tan \alpha} = \frac{1 + y/x}{1 - y/x}\]</span> 令<span class="math inline">\(v =\frac{y}{x}\)</span>，则<span class="math inline">\(y =xv\)</span>，<span class="math inline">\(y&#39; = v +xv&#39;\)</span></p><p><span class="math inline">\(\therefore v + xv&#39; = \frac{1 + v}{1 -v}\)</span></p><p><span class="math inline">\(\therefore v + x\frac{dv}{dx} = \frac{1 +v}{1 - v}\)</span></p><p><span class="math inline">\(\therefore x\frac{dv}{dx} = \frac{v^2 +1}{1 - v}\)</span></p><p><span class="math inline">\(\therefore \frac{x}{dx} = \frac{v^2 +1}{(1 - v)dv}\)</span></p><p><span class="math inline">\(\therefore \frac{1}{x}dx = \frac{1 -v}{v^2 + 1}dv\)</span></p><p><span class="math inline">\(\therefore \int \frac1x dx = \int \frac{1- v}{v^2 + 1} dv\)</span></p><p><span class="math inline">\(\therefore \ln |x| = \arctan v - \frac12\ln(v^2 + 1) + c\)</span></p><p><span class="math inline">\(\therefore \ln |x| = \arctan \frac{y}{x}- \frac12\ln ((\frac{y}{x})^2 + 1) + c\)</span></p><p><span class="math inline">\(\therefore \arctan \frac{y}{x} = \ln\sqrt{x^2 + y^2} + c\)</span></p><p><span class="math inline">\(\therefore \theta = \ln r +c\)</span></p><p><span class="math inline">\(\therefore r = ce^\theta, c &gt;0\)</span></p><p>上述方程又称为“指数螺旋线”，优美。</p><h3 id="五.-一阶自治ode">五. 一阶自治ODE</h3><p>回顾一下，前面我们已经学会了用几何法一阶ODE（画线素），然后又学了数值法求一阶ODE；</p><p>然后，学了用分离变量法解一阶ODE，以及学会了用通法解决形如<spanclass="math inline">\(y&#39; + p(x)y = q(x)\)</span>的一阶ODE；</p><p>以及，学会了用换元法解决“伯努利ODE“和“一阶齐次ODE”。</p><p>现在我们再来学一种形如：<span class="math inline">\(y&#39; =f(y)\)</span>的一阶ODE解法，这种方程我们称为自治的，因为右侧仅仅由y组成不涉及x。</p><p>当然分离变量法可以解这个方程，但是我们不是想解它，只是想分析一下它的图像性质。</p><p>核心思路就是：画两张图，第一张图是“x-y图”，第二张图是“y-f(y)”图。</p><hr /><p>下面举一个Logistic equation withharvesting的例子（例如渔场养鱼捕鱼，<spanclass="math inline">\(y\)</span>是鱼量，<spanclass="math inline">\(t\)</span>是时间，<spanclass="math inline">\(h\)</span>是捕获量）： <spanclass="math display">\[\frac{dy}{dt} = (a - by)y - h\]</span> 首先令右边为：<span class="math inline">\(f(y) = ay - by^2 -h\)</span></p><p>然后画出<span class="math inline">\(f(y)-y\)</span>图：</p><p><img src="10.png" style="zoom:80%;" /></p><p>可以看出，与x轴有俩交点，分别令值为<span class="math inline">\(a,b\)</span></p><p>然后可以继续画出<span class="math inline">\(y-t\)</span>图：</p><p><img src="11.png" style="zoom: 67%;" /></p><p>可以看出，a和b都是“临界点”。因为一旦<span class="math inline">\(y =a\)</span>或<span class="math inline">\(b\)</span>，那么<spanclass="math inline">\((x, y)\)</span>将一直在<spanclass="math inline">\(y=a\)</span>或<spanclass="math inline">\(b\)</span>这条线上移动，用数值法的作图方法简单即可知道。</p><p>那么，我们会抛出疑问，虽然a和b都是临界点，但是谁是稳定的呢？</p><p>显然可以看出，b是稳定的，因为上下都趋向于它，a是不稳定的，因为上下都远离它。</p><p>对于实际意义来说，就是如果维持当前的捕获量<spanclass="math inline">\(h\)</span>，那么只需要保证初始鱼量&gt;a，即可保证一定时间后，鱼量收敛于b。</p><p>那么，如果我是农场主，我肯定希望捕获量<spanclass="math inline">\(h\)</span>尽可能高，当<spanclass="math inline">\(h\)</span>高时，上面那个二次函数会下移，如果移的太多了，那么整个函数在定义域都是下降的了，那么鱼量会一直下降到负无穷。所以我们要提高<spanclass="math inline">\(h\)</span>，使得二次函数图像刚刚好与x轴有一个交点，值记为<spanclass="math inline">\(c\)</span>的时候停止。此时的<spanclass="math inline">\(y-t\)</span>图如下：</p><p><img src="12.png" style="zoom:67%;" /></p><p>可以看出，只要你保证最开始的鱼量&gt;c，那么最终你的鱼量会收敛于c，而且你的捕获量<spanclass="math inline">\(h\)</span>此时是最高的。</p><hr /><p>OK，上面那个例子非常有趣right？</p><p>但是你可能会注意到一点，为什么图像都是平移相同的？</p><p>因为对于自治方程，等号右边等于<spanclass="math inline">\(y&#39;\)</span>，而且等号右边仅仅与<spanclass="math inline">\(y\)</span>有关，如果我们确定了一个<spanclass="math inline">\(y_0\)</span>，那么这条水平线上的线素都相同。所以说，你自己画画就知道，一阶自治ODE的图像是这种平移相同的了。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;几何法解一阶ODE、欧拉数值法解一阶ODE、分离变量法解一阶ODE、解标准一阶线性ODE、换元法解伯努利方程+一阶齐次ODE、一阶自治ODE图像分析&lt;/p&gt;</summary>
    
    
    
    <category term="4. 大学" scheme="http://error666.top/categories/4-%E5%A4%A7%E5%AD%A6/"/>
    
    <category term="数学" scheme="http://error666.top/categories/4-%E5%A4%A7%E5%AD%A6/%E6%95%B0%E5%AD%A6/"/>
    
    <category term="微分方程" scheme="http://error666.top/categories/4-%E5%A4%A7%E5%AD%A6/%E6%95%B0%E5%AD%A6/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/"/>
    
    
  </entry>
  
  <entry>
    <title>线性代数4</title>
    <link href="http://error666.top/2024/09/15/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B04/"/>
    <id>http://error666.top/2024/09/15/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B04/</id>
    <published>2024-09-15T15:26:20.000Z</published>
    <updated>2024-09-16T15:45:43.936Z</updated>
    
    <content type="html"><![CDATA[<p>总结、知识易混点整理、补充知识</p><span id="more"></span><hr /><h3 id="总结">总结</h3><p>前前后后花了一个月左右把mit 18.06学完了，收获颇丰，感谢教授。</p><p>这门课第一章从（方程组 + 矩阵 +四个基本子空间）出发，讨论了线性代数的基本元素：向量、矩阵、空间。最后用所学知识对电势差问题建模，得到了许多优美的结论。</p><p>第二章开始研究矩阵各种性质，例如正交、投影、行列式、特征值、对角化。并在结尾用马尔可夫矩阵寻找稳态，用正交性对傅里叶级数建模。</p><p>第三章仍然是研究矩阵的各种性质，例如对称、正定、相似、SVD分解。以及将线性变换和矩阵统一起来。利用基变换实现图像压缩展示了线性变换的优美之处。</p><p>总之，这门课不仅帮我扎实的打好了线代基础，而且给了我理解线代的上层视角。以及：马尔可夫矩阵、对角化、SVD分解、基变换压缩这三个知识点也给了我科研上的启发，说不定哪天就可以作为trick来优化我的算法。</p><h3 id="section"></h3><h3 id="知识点易混点整理">知识点易混点整理</h3><ol type="1"><li>如何理解矩阵<spanclass="math inline">\(A\)</span>可对角化的条件是：“有n个线性无关的特征向量”?<ul><li>因为我们在推对角化公式的时候，用的是<span class="math inline">\(AS =S\Lambda\)</span>。<spanclass="math inline">\(S\)</span>是n个特征向量排成的方阵</li><li>因为要右乘<spanclass="math inline">\(S^{-1}\)</span>，所以就要保证<spanclass="math inline">\(S\)</span>的n个列向量线性无关，即<spanclass="math inline">\(A\)</span>有n个线性无关的特征向量</li><li>这样才能推出：<span class="math inline">\(A = S\LambdaS^{-1}\)</span></li></ul></li><li>上面那个判据太困难了，有什么等价判据？<ul><li>若<spanclass="math inline">\(A\)</span>有n个互不相同的特征值，则<spanclass="math inline">\(A\)</span>有n个线性无关的特征向量</li><li>但若没有，则不能说<spanclass="math inline">\(A\)</span>一定没有n个线性无关的特征向量</li><li>所以我们判断一个矩阵是否可对角化，可转换为求其特征值的问题。</li></ul></li><li><span class="math inline">\(A\)</span>有n个线性无关的特征向量 和<span class="math inline">\(A\)</span>的各列线性无关有什么关系？<ul><li>前者可推后者，后者不可推前者</li><li>我来证一下前者可推后者，因为对于特征向量x，有<spanclass="math inline">\(Ax = \lambda x\)</span>，所以<spanclass="math inline">\(\lambda x\)</span>是通过<spanclass="math inline">\(A\)</span>进行列变换得到的，那仅仅通过列变换就可以得到一组线性无关的向量，相当于变换后的列空间就是<spanclass="math inline">\(\mathrm{R}^n\)</span>。而列变换不改变列空间，所以<spanclass="math inline">\(C(A) = \mathrm{R}^n\)</span>，所以<spanclass="math inline">\(A\)</span>的各列线性无关。</li></ul></li><li>对称矩阵一定可以对角化吗？若可以，它的对角化有什么特别之处？<ul><li>是的一定可以。</li><li>对称矩阵<spanclass="math inline">\(A\)</span>可以对角化。那么可写为：<spanclass="math inline">\(A = S\Lambda S^{-1}\)</span></li><li>因为对称矩阵有个很好的性质就是：可选出一组正交特征向量。所以<spanclass="math inline">\(S\)</span>可以是完全正交的，再将其标准化一下，即可得到正交阵<spanclass="math inline">\(Q\)</span>。</li><li>正交阵有一个很好的性质：<span class="math inline">\(Q^{-1} =Q^T\)</span></li><li>所以可对角化的对称矩阵<spanclass="math inline">\(A\)</span>可对角化为：<spanclass="math inline">\(A = Q\Lambda Q^\mathrm{T}\)</span></li></ul></li><li>正定矩阵有什么好处吗？<ul><li>首先正定矩阵是对称矩阵的一个子集，对称矩阵已经有一些很好的性质了，正定矩阵除了有对称矩阵的性质，还有其余很好的性质。比如一定可逆而且<spanclass="math inline">\(x^\mathrm{T}Ax &gt; 0\)</span>。</li><li>如果快速获得一个对称矩阵？<spanclass="math inline">\(A^\mathrm{T}A\)</span>，<spanclass="math inline">\(xx^\mathrm{T} / AA^\mathrm{T}\)</span></li><li>如果快速获得一个正定矩阵？<span class="math inline">\(A^\mathrm{T}A,r(A) = n\)</span></li></ul></li><li></li></ol><h3 id="补充知识">补充知识</h3>]]></content>
    
    
    <summary type="html">&lt;p&gt;总结、知识易混点整理、补充知识&lt;/p&gt;</summary>
    
    
    
    <category term="4. 大学" scheme="http://error666.top/categories/4-%E5%A4%A7%E5%AD%A6/"/>
    
    <category term="数学" scheme="http://error666.top/categories/4-%E5%A4%A7%E5%AD%A6/%E6%95%B0%E5%AD%A6/"/>
    
    <category term="线性代数" scheme="http://error666.top/categories/4-%E5%A4%A7%E5%AD%A6/%E6%95%B0%E5%AD%A6/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>线性代数3</title>
    <link href="http://error666.top/2024/09/14/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B03/"/>
    <id>http://error666.top/2024/09/14/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B03/</id>
    <published>2024-09-14T15:31:27.000Z</published>
    <updated>2024-09-16T15:11:39.326Z</updated>
    
    <content type="html"><![CDATA[<p>对称矩阵、复数矩阵、FFT、正定矩阵、相似矩阵、SVD分解、线性变换、图像压缩、左右逆/伪逆</p><span id="more"></span><hr /><h3 id="一.-对称矩阵及其正定性">一. 对称矩阵及其正定性</h3><p>在这里我们只讨论实对称矩阵。</p><p>我们很喜欢对称矩阵，因为它具有很好的性质。就拿实对称矩阵来举例，它具有下列两个性质：</p><ol type="1"><li>其特征值均为实数</li><li>其一定可选出具有正交的特征向量。这里的“有”，是指可以选出一套完全正交的特征向量（例如在重特征值条件下，可能存在一个平面内向量都可以作为特征向量）</li><li>特征值的符号与主元的符号相同，即正数的个数相同，负数的个数也相同</li></ol><p>上一章我们学过，若方阵<spanclass="math inline">\(A\)</span>具有n个线性无关的特征向量，那么其可以对角化为<spanclass="math inline">\(S\Lambda S^{-1}\)</span>。</p><p>对于有n个线性无关特征向量的对称矩阵来说，因为性质2，所以它的特征向量矩阵可化为一个正交阵<spanclass="math inline">\(Q\)</span>，正交阵满足<spanclass="math inline">\(Q^\mathrm{T} = Q^{=1}\)</span></p><p>所以对于具有n个线性无关特征向量的对称矩阵<spanclass="math inline">\(A\)</span>来说，其可对角化为<spanclass="math inline">\(Q\Lambda Q^T\)</span>。</p><p>把上面的式子进一步展开：</p><p><span class="math inline">\(A = Q\Lambda Q^\mathrm{T} =\begin{bmatrix} q_1 &amp; q_2 &amp; \cdots &amp; q_n \end{bmatrix}\begin{bmatrix} \lambda_1 &amp; &amp; &amp; \\ &amp; \lambda_2 &amp;&amp; \\ &amp; &amp; \cdots &amp; \\ \end{bmatrix} \begin{bmatrix}q_1^\mathrm{T} \\ q_2^\mathrm{T} \\ \cdots \\ \end{bmatrix}\)</span></p><p>利用"线性代数1"讲的用拆分矩阵乘法为加法去理解这个式子，可以写成：</p><p><span class="math inline">\(A = \lambda_1q_1q_1^\mathrm{T} +\lambda_2q_2q_2^\mathrm{T} + \cdots +\lambda_nq_nq_n^\mathrm{T}\)</span></p><p>上面这个式子发现了吗，其实每一项都是一个系数乘一个投影矩阵，因为<spanclass="math inline">\(Q\)</span>是正交矩阵，所以<spanclass="math inline">\(q_i^\mathrm{T}q_i = 1\)</span>。</p><p>所以<spanclass="math inline">\(A\)</span>可以理解为投影矩阵的线性组合，且投影方向都是互相正交的。</p><hr /><p>下面来介绍正定矩阵。</p><p>正定矩阵是对称矩阵的一个子类，且所有特征值&gt;0</p><p>而且它的“子行列式”均&gt;0，子行列式指的是n阶矩阵左上角的所有<spanclass="math inline">\(k \times k, 1 \le k \len\)</span>子行列式数值均为正。这很好理解，由对称矩阵的性质3，我们知道，其所有主元都&gt;0。而行列式就等于主元之积，所以子行列式们自然都大于0。这就是用行列式判定矩阵正定的判据。</p><h3 id="二.-复数矩阵-快速傅里叶变换">二. 复数矩阵, 快速傅里叶变换</h3><p>对不起我的高数很垃圾，这节我也听不懂。等我学完mit18.03再来听这一节课</p><h3 id="三.-正定矩阵">三. 正定矩阵</h3><p>正定矩阵是很好的矩阵，它是对称的而且所有特征值都大于零。那么如何判定一个方阵是正定矩阵呢？这里我给出几种方法</p><ol type="1"><li>所有特征值<span class="math inline">\(\lambda_i &gt; 0\)</span></li><li>所有主元大于零</li><li>所有子行列式们大于零</li><li><span class="math inline">\(x^\mathrm{T}Ax &gt; 0, x \ne\textbf{0}\)</span></li></ol><p>第一个是定义，第二个是因为对称矩阵有一个性质就是特征值正负号与主元相同，所以可根据(1)得到第二条等价条件。第三个是用行列式判断正定性(前面讲过)。第四个是新加的，我们很喜欢用第四个判据。让我们来看看为什么这个判据可以推出矩阵是正定的。</p><p>对于<spanclass="math inline">\(x^\mathrm{T}Ax\)</span>，我们将其展开： <spanclass="math display">\[x^\mathrm{T}Ax = \begin{bmatrix}x_1, x_2, \cdots,x_n\end{bmatrix}\begin{bmatrix} a_{11}x_1+a_{12}x_2+\cdots+a_{1n}x_n \\a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n \\ a_{31}x_1 + a_{32}x_2 +\cdots + a_{3n}x_n \\ \cdots \\ a_{n1}x_1 + a_{n2}x_2 + \cdots +a_{nn}x_n\end{bmatrix} = \\ x_1(a_{11}x_1+a_{12}x_2+\cdots+a_{1n}x_n)+x_2(a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n) + \cdots + x_n(a_{n1}x_1+ a_{n2}x_2 + \cdots + a_{nn}x_n)\]</span>可以发现，每一项都是二次的。其实如果用图像去研究这个函数：<spanclass="math inline">\(f(x) =x^\mathrm{T}Ax\)</span>，（这个函数也叫二次型）</p><p>矩阵为正定时二次型图像见左上角，为半正定时图像见右下角，为非正定/半正定时图像见左下角，负定时图像见右上角</p><p>这些图像研究的是二维方阵的二次型，xy轴是<spanclass="math inline">\(x\)</span>的俩分量<span class="math inline">\(x_1,x_2\)</span>，z轴是<spanclass="math inline">\(f(x)=x^\mathrm{T}Ax\)</span></p><p><img src="1.png" /></p><hr /><p>下面继续讨论下正定矩阵还有哪些性质，假设<spanclass="math inline">\(A, B\)</span>为正定矩阵，<spanclass="math inline">\(C\)</span>为矩阵。</p><p>那么<spanclass="math inline">\(A\)</span>是否可逆呢？答案是肯定的，因为我们知道<spanclass="math inline">\(A\)</span>的子行列式们都大于0，所以<spanclass="math inline">\(det(A) &gt; 0\)</span>，所以<spanclass="math inline">\(A\)</span>是非奇异的，即满秩可逆的。</p><p>那么<spanclass="math inline">\(A^{-1}\)</span>是不是正定矩阵呢？答案是肯定的。因为我们知道<spanclass="math inline">\(A^{-1}\)</span>的特征值们就是<spanclass="math inline">\(A\)</span>的特征值取倒数，而<spanclass="math inline">\(A\)</span>的特征值都大于0，所以<spanclass="math inline">\(A^{-1}\)</span>的特征值也都大于0，所以<spanclass="math inline">\(A^{-1}\)</span>也为正定矩阵。</p><p>那么<span class="math inline">\(A +B\)</span>是不是正定矩阵呢？答案是肯定的。我们来看看<spanclass="math inline">\(x^\mathrm{T}(A+B)x = x^\mathrm{T}Ax +x^\mathrm{T}Bx &gt; 0\)</span>，所以<span class="math inline">\(A +B\)</span>也是正定矩阵。</p><p>那么<spanclass="math inline">\(C^\mathrm{T}C\)</span>是不是正定矩阵呢？答案是不一定。我们来看看<spanclass="math inline">\(x^\mathrm{T}(C^\mathrm{T}C)x = (Cx)^\mathrm{T}(CX)= \|Cx\|^2 \ge 0\)</span>。</p><p>所以<spanclass="math inline">\(C^\mathrm{T}C\)</span>至少是半正定的，那什么时候是正定的呢？只要没有非零向量使得<spanclass="math inline">\(Cx = \textbf{0}\)</span>，那么就可以保证：<spanclass="math inline">\(x^\mathrm{T}(C^\mathrm{T}C)x &gt; 0\)</span>。</p><p>即要保证<span class="math inline">\(N(C) =\{\textbf{0}\}\)</span>，即要保证<span class="math inline">\(r(C) =n\)</span>，即可保证<spanclass="math inline">\(C^\mathrm{T}C\)</span>是正定矩阵。</p><p>还记得<spanclass="math inline">\(C^\mathrm{T}C\)</span>在哪用到吗？即最小二乘求最优近似解那里，最后方程为：<spanclass="math inline">\(C\hat{x}=Pb =C(C^\mathrm{T}C)^{-1}C^\mathrm{T}b\)</span></p><p>只要保证<span class="math inline">\(C\)</span>各列线性无关，则<spanclass="math inline">\(C^\mathrm{T}C\)</span>是正定矩阵，所以<spanclass="math inline">\(C^\mathrm{T}C\)</span>可逆，则上述方程成立，同左乘<spanclass="math inline">\(C^\mathrm{T}\)</span>，然后再左乘<spanclass="math inline">\((C^\mathrm{T}C)^{-1}\)</span>，即可求出<spanclass="math inline">\(\hat{x}\)</span>。</p><h3 id="四.-相似矩阵">四. 相似矩阵</h3><p><span class="math inline">\(A, B\)</span>均为<spanclass="math inline">\(n \times n\)</span>的方阵，那么<spanclass="math inline">\(A\)</span>与<spanclass="math inline">\(B\)</span>相似，用数学语言表达为：存在可逆矩阵<spanclass="math inline">\(M\)</span>，使得<span class="math inline">\(B =M^{-1}AM\)</span></p><p>它具有一个性质：相似的矩阵拥有相同的特征值。</p><blockquote><p>证明：</p><p><span class="math inline">\(Ax = \lambda x\)</span></p><p><span class="math inline">\(A(MM^{-1})x = \lambda x\)</span></p><p><span class="math inline">\(M^{-1}AMM^{-1}x = \lambdaM^{-1}x\)</span></p><p><span class="math inline">\(BM^{-1}x = \lambda M^{-1}x\)</span></p><p><span class="math inline">\(B(M^{-1}x) = \lambda(M^{-1}x)\)</span></p><p>证毕。且可看出特征值虽然不变，但是特征向量由<spanclass="math inline">\(x\)</span>变为了<spanclass="math inline">\(M^{-1}x\)</span>。</p></blockquote><p>举个例子，例如可对角化的矩阵<spanclass="math inline">\(A\)</span>，其可分解为：<spanclass="math inline">\(A = S^{-1}\Lambda S\)</span>。其中<spanclass="math inline">\(S\)</span>是可逆的，因为各特征向量线性无关。所以<spanclass="math inline">\(A\)</span>与<spanclass="math inline">\(\Lambda\)</span>就相似。而且我们会发现，<spanclass="math inline">\(A\)</span>与<spanclass="math inline">\(\Lambda\)</span>的特征值一样。</p><h3 id="五.-奇异值分解">五. 奇异值分解</h3><p>SVD分解也是一种矩阵分解的形式。至今为止，我们已经学过了许多矩阵分解方法了：<spanclass="math inline">\(LU\)</span>、<span class="math inline">\(S\LambdaS^{-1}\)</span>（可对角矩阵）、<span class="math inline">\(Q\LambdaQ^\mathrm{T}\)</span>（对称矩阵）、<spanclass="math inline">\(QR\)</span>（满秩矩阵）。</p><p>可以发现，除了<spanclass="math inline">\(LU\)</span>分解，其余分解都对矩阵有限制条件。但是这节我要讲的SVD分解，对任意矩阵都成立。</p><p>SVD用数学语言描述如下：</p><p><span class="math inline">\(A = U\Sigma V^\mathrm{T}\)</span></p><p><span class="math inline">\(A \in \mathbb{R}^{m \timesn}\)</span>是任意矩阵，<span class="math inline">\(U \in \mathbb{R}^{m\times m}\)</span>是正交阵，<span class="math inline">\(\Sigma \in\mathbb{R}^{m \times n}\)</span>是"对角阵"，<spanclass="math inline">\(V^\mathrm{T} \in \mathbb{R}^{n \timesn}\)</span>是正交阵。</p><p>我们来看下在已知<spanclass="math inline">\(A\)</span>下，如何计算出<spanclass="math inline">\(U, \Sigma, V\)</span></p><p><span class="math inline">\(\because A = U\SigmaV^\mathrm{T}\)</span></p><p><span class="math inline">\(\therefore A^\mathrm{T}A =(V\Sigma^\mathrm{T}U^\mathrm{T})(U\Sigma V^\mathrm{T}) =V\Sigma^\mathrm{T}\Sigma V^\mathrm{T}\)</span></p><p><span class="math inline">\(\because A^\mathrm{T}A \text{ is asymmetric matrix}\)</span></p><p><span class="math inline">\(\therefore V \text{ is } Q,\Sigma^\mathrm{T}\Sigma \text{ is } \Lambda, \text{ where }A^\mathrm{T}A = Q\Lambda Q^{\mathrm{T}}\)</span></p><p>所以，对于<span class="math inline">\(A\)</span>，求出<spanclass="math inline">\(A^\mathrm{T}A\)</span>的特征值开根和对应的相互正交的标准特征向量，对应排好就是<spanclass="math inline">\(\Sigma, V\)</span></p><p>那有了<span class="math inline">\(\Sigma, V\)</span>后，如何求<spanclass="math inline">\(U\)</span>呢？</p><p><span class="math inline">\(\because A = U\SigmaV^\mathrm{T}\)</span></p><p><span class="math inline">\(\therefore AV = U\Sigma V^\mathrm{T}V =U\Sigma\)</span></p><p>即再算出<spanclass="math inline">\(AV\)</span>，然后每一列除对应的奇异值，即可得到<spanclass="math inline">\(U\)</span>。</p><hr /><p>多说无益，举个例子来看下如何将矩阵<spanclass="math inline">\(A\)</span>分解为<spanclass="math inline">\(U\Sigma V^\mathrm{T}\)</span></p><p>已知<span class="math inline">\(A = \begin{bmatrix} 4 &amp; 4 \\ -3&amp; 3 \end{bmatrix}\)</span></p><p>先算出<span class="math inline">\(A^\mathrm{T}A = \begin{bmatrix} 25&amp; 7 \\ 7 &amp; 25 \end{bmatrix}\)</span></p><p>然后求其特征值和对应的互相正交的标准特征向量：</p><p><span class="math inline">\(\lambda_1 = 18, x_1 =\frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ -1 \end{bmatrix}\)</span></p><p><span class="math inline">\(\lambda_2 = 32, x_2 =\frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ 1 \end{bmatrix}\)</span></p><p>即已经求出<span class="math inline">\(\Sigma = \begin{bmatrix}\sqrt{18} &amp; \\ &amp; \sqrt{32} \end{bmatrix}, V =\frac{1}{\sqrt{2}}\begin{bmatrix} 1 &amp; 1 \\ -1 &amp; 1\end{bmatrix},V^\mathrm{T} = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 &amp; -1 \\ 1 &amp; 1\end{bmatrix}\)</span></p><p>然后算出<span class="math inline">\(AV\)</span>，即<spanclass="math inline">\(U\Sigma\)</span>：<spanclass="math inline">\(\begin{bmatrix} 0 &amp; \frac{8}{\sqrt{2}} \\-\frac{6}{\sqrt{2}} &amp; 0\end{bmatrix}\)</span></p><p>每一列除对应的<spanclass="math inline">\(\sqrt{\lambda_i}\)</span>，即第一列除<spanclass="math inline">\(\sqrt{18}\)</span>，第二列除<spanclass="math inline">\(\sqrt{32}\)</span>，得到<spanclass="math inline">\(U\)</span>：<spanclass="math inline">\(\begin{bmatrix} 0 &amp; 1 \\ -1 &amp; 0\end{bmatrix}\)</span></p><p>现在所有东西都已求出来： <span class="math display">\[U = \begin{bmatrix} 0 &amp; 1 \\ -1 &amp; 0 \end{bmatrix}, \Sigma =\begin{bmatrix} \sqrt{18} &amp; \\ &amp; \sqrt{32} \end{bmatrix},V^\mathrm{T} = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 &amp; -1 \\ 1 &amp; 1\end{bmatrix}\]</span> 我们来验证一下，将这仨乘起来，发现等于<spanclass="math inline">\(A\)</span>，check！</p><hr /><p>SVD有什么用呢？我来举一个例子，例如某件事物需要用两个维度来刻画，即<spanclass="math inline">\(f: (x, y) \toz\)</span>。那么适合用一个矩阵来表示，不妨表示为<spanclass="math inline">\(A\)</span>。</p><p>如果我们将<span class="math inline">\(A\)</span>进行SVD分解为<spanclass="math inline">\(U\SigmaV^\mathrm{T}\)</span>，进一步展开可以得到： <spanclass="math display">\[A = U\Sigma V^\mathrm{T} = \begin{bmatrix}u_1, u_2, \cdots,u_n\end{bmatrix} \cdot \begin{bmatrix}\sigma_1 &amp; &amp; &amp; \\&amp; \sigma_2 &amp; &amp; \\ &amp; &amp; \cdots &amp; \\ &amp; &amp;&amp; \sigma_n\end{bmatrix} \cdot \begin{bmatrix}v_1^\mathrm{T} \\v_2^\mathrm{T} \\ \cdots \\ v_n^\mathrm{T}\end{bmatrix} =\sigma_1u_1v_1^\mathrm{T} + \sigma_2u_2v_2^\mathrm{T} + \cdots +\sigma_nu_nv_n^\mathrm{T}\]</span> 可以发现，一个矩阵被拆解为若干矩阵相加，奇异值<spanclass="math inline">\(\sigma_i\)</span>可以理解为矩阵所占的权重。那些<spanclass="math inline">\(\sigma_i\)</span>大的矩阵说明对整体矩阵的影响越大。</p><p>相当于对于一个模式<spanclass="math inline">\(A\)</span>，我可以知道<spanclass="math inline">\(A\)</span>重点体现在哪些子模式上，进而重点去优化那些子模式。</p><h3 id="六.-线性变换">六. 线性变换</h3><p>什么是坐标？</p><p>其实，世界上坐标不是天生存在的东西。而是人类以某些东西为标准，去测量其它东西的一种度量。</p><p>例如我们通常认知的坐标系，其实上就是以俩正交的标准向量基为标准，坐标就是用这俩去线性组合的系数。</p><p>所以如何理解一个线性变化？</p><p>我这么说吧，假设我们已经确定好一组基(n个)，那么这组基作为我们的“观测基准”，可以以它们为标准观测出万物的状态<spanclass="math inline">\((c_1, c_2, \cdots, c_n)\)</span>。（<spanclass="math inline">\(c_i\)</span>跟第i个基向量有关）</p><p>但是现在，我想换一套观察工具，也就是换一组基(m个)，那么换完之后，<strong>同样</strong>的一个事物，在之前用旧基观测的时候，它的状态是<spanclass="math inline">\((c_1, c_2, \cdots,c_n)\)</span>，用新基观测的时候，它的状态变为<spanclass="math inline">\((d_1, d_2, \cdots, d_m)\)</span>。</p><p>那么有没有一种映射，可以直接让我从<span class="math inline">\((c_1,c_2, \cdots, c_n)\)</span>直接可以得到<span class="math inline">\((d_1,d_2, \cdots, d_m)\)</span>？有，它就是——线性变换。</p><p>如何构造这个线性变换呢？如下：</p><p>对于旧基(<spanclass="math inline">\(v\)</span>)的每个基向量，我们都先用新基(<spanclass="math inline">\(w\)</span>)去观测它，并得到观测状态<spanclass="math inline">\(a_{ij}\)</span>： <span class="math display">\[\begin{align*}    v_1 &amp;= a_{11}w_1 + a_{21}w_2 + \cdots + a_{m1}w_m \\    v_2 &amp;= a_{12}w_1 + a_{22}w_2 + \cdots + a_{m2}w_m \\    &amp;\cdots \\    v_n &amp;= a_{1n}w_1 + a_{2n}w_2 + \cdots + a_{mn}w_m\end{align*}\]</span> 对于一件事物<spanclass="math inline">\(x\)</span>，用旧基观测可以得到：<spanclass="math inline">\(x = c_1v_1 + c_2v_2 + \cdots + c_nv_n\)</span></p><p>继续展开： <span class="math display">\[\begin{align*}    x &amp;= c_1v_1 + c_2v_2 + \cdots + c_nv_n \\      &amp;= c_1a_{11}w_1 + c_1a_{21}w_2 + \cdots + c_1a_{m1}w_m + \\      &amp;~~~~~c_2a_{12}w_1 + c_2a_{22}w_2 + \cdots + c_2a_{m2}w_m + \\      &amp;~~~~~\cdots \\      &amp;~~~~~c_na_{1n}w_1 + c_na_{2n}w_2 + \cdots + c_na_{mn}w_m \\      &amp;= (c_1a_{11}+c_2a_{12}+\cdots+c_na_{1n})w_1 + \\      &amp;~~~~~(c_1a_{21}+c_2a_{22}+\cdots+c_na_{2n})w_2 + \\      &amp;~~~~~\cdots \\      &amp;~~~~~(c_1a_{m1}+c_2a_{m2}+\cdots+c_na_{mn})w_m \\      &amp;=d_1w_1 + d_2w_2 + \cdots + d_mw_m\end{align*}\]</span> 用矩阵形式表达： <span class="math display">\[Ac = \begin{bmatrix}a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n} \\a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \\\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn}\end{bmatrix}\begin{bmatrix}c_1 \\ c_2 \\ \vdots \\ c_n\end{bmatrix}=\begin{bmatrix}a_{11}c_1 + a_{12}c_2 + \cdots + a_{1n}c_n \\a_{21}c_1 + a_{22}c_2 + \cdots + a_{2n}c_n \\\vdots \\a_{m1}c_1 + a_{m2}c_2 + \cdots + a_{mn}c_n\end{bmatrix}=\begin{bmatrix}d_1 \\d_2 \\\vdots \\d_m\end{bmatrix}\]</span> 所以我们从旧观测状态<span class="math inline">\((c_1, c_2,\cdots, c_n)\)</span>得到了新观测状态<span class="math inline">\((d_1,d_2, \cdots, d_m)\)</span>。</p><p>问题的关键就是确定这个<spanclass="math inline">\(A\)</span>矩阵，从旧基到新基的线性变换也就体现在这个<spanclass="math inline">\(A\)</span>矩阵上。</p><p>那这个<span class="math inline">\(A\)</span>咋求呢？<spanclass="math inline">\(A\)</span>的每一列其实就是每一个旧基的基向量用新基去观测得到的状态。</p><p>至此，我们有了一种全新的视角去看待矩阵乘法<spanclass="math inline">\(Ax = b\)</span></p><p><strong><spanclass="math inline">\(x\)</span>是旧基下对某事物的观测状态，<spanclass="math inline">\(b\)</span>是新基下对某事物的观测状态。而实现基转换功能的东西，就是<spanclass="math inline">\(A\)</span>。</strong></p><p><strong><spanclass="math inline">\(A\)</span>的每一列其实就是每一个旧基的基向量用新基去观测得到的状态。</strong></p><hr /><p>至此，线性变换与矩阵乘法彻底联系了起来。线性变换就是矩阵，矩阵就是线性变换。</p><p>而我们知道，矩阵满足<span class="math inline">\((A + B)x = Ax + Bx,(cA)x = c(Ax)\)</span>。所以线性变换<spanclass="math inline">\(T\)</span>是满足加法和数乘的，即：</p><ol type="1"><li><span class="math inline">\(T(v + w) = T(v) + T(w)\)</span></li><li><span class="math inline">\(T(cv) = cT(v)\)</span></li></ol><p>其实线性变换一定满足<span class="math inline">\(T(0) =0\)</span>，因为当<span class="math inline">\(v = w =0\)</span>时，<span class="math inline">\(T(0) =2T(0)\)</span>，所以<span class="math inline">\(T(0) =0\)</span>。所以可以通过<spanclass="math inline">\(T(0)\)</span>是否等于0来快速排除一些不是线性变换的映射。</p><hr /><p>告诉你三个有趣的事实：</p><ol type="1"><li>矩阵的逆就是线性变换的逆变换</li><li>进行多次线性变换就是多个矩阵连乘，这样子矩阵乘法就有了几何上的直观理解</li><li>正交阵相当于对空间进行旋转，对角阵相当于对空间进行拉伸</li></ol><h3 id="七.-图像压缩">七. 图像压缩</h3><p>思考一个<span class="math inline">\(512 \times512\)</span>的图像，我们需要<span class="math inline">\(512 \times512\)</span>个数来保存图像的状态。</p><p>令<span class="math inline">\(n = 512 \times 512\)</span></p><p>具体来说，一个状态可以用一个<spanclass="math inline">\(\mathbb{R}^{n}\)</span>向量<spanclass="math inline">\(x\)</span>来描述（把矩阵拉成一个向量）</p><p>而我们存储的<span class="math inline">\(512 \times512\)</span>个数，其实是标准基的系数： <span class="math display">\[x = c_1\begin{bmatrix}1\\ \\ \\ \\ \\ \end{bmatrix} +c_2\begin{bmatrix}\\ 1 \\ \\ \\ \\ \end{bmatrix} + \cdots +c_n\begin{bmatrix} \\ \\ \\ \\ 1\end{bmatrix}\]</span> 那如果换一组基呢？</p><p>假设我们的旧基（即标准基）为<spanclass="math inline">\(v\)</span>，新基为<spanclass="math inline">\(w\)</span>。旧的观测状态为<spanclass="math inline">\((c_1, c_2, ...,c_n)\)</span>，那么新的观测状态<span class="math inline">\((d_1, d_2,..., d_n)\)</span>需要通过<spanclass="math inline">\(Ac\)</span>才能得到，<spanclass="math inline">\(A\)</span>的每一列为每一个旧基的基向量用新基去观测得到的状态。</p><p><span class="math inline">\(A^{-1} =B\)</span>的每一列为每一个新基的基向量用旧基去观测的状态，所以<spanclass="math inline">\(B\)</span>其实就是把新基的基向量排成一排。</p><p>所以我们新的观测状态：<span class="math inline">\(d =B^{-1}c\)</span>。</p><p>用新观测状态表示图片向量：<span class="math inline">\(x = d_1w_1 +d_2w_2 + \cdots + d_nw_n\)</span></p><p>我们可以设定一个阈值，对于那些很小的<spanclass="math inline">\(d_i\)</span>那一项，我们就直接丢弃它，不存储了。例如我只要前5大的<spanclass="math inline">\(d_i\)</span>，那么我原本要存<spanclass="math inline">\(n\)</span>个数，现在我只需要存5个数了。</p><p>我还原出来的图片向量即为：<span class="math inline">\(\hat{x} =d_1w_1 + d_2w_2 + \cdots + d_5w_5\)</span>。</p><p>非常巧妙，嗯哼？</p><hr /><p>相当于用时间换空间，因为压缩和还原的过程我都需要进行矩阵运算。所以想加速压缩/还原速度我需要保证挑选出来的新基组成的<spanclass="math inline">\(B\)</span>的逆好求。</p><p>同时，为了尽可能压缩空间，我要使得挑选出来的新基的观测状态<spanclass="math inline">\((d_1, d_2, \cdots, d_n)\)</span>尽可能多的使<spanclass="math inline">\(d_i &lt;\text{阈值}\)</span>。这样我就可能少保存系数，从而达到压缩存储空间的效果。</p><p>注意，为啥我新基的个数要和旧基保持一样（即都有<spanclass="math inline">\(n\)</span>个基向量）？因为图片大小为<spanclass="math inline">\(512 \times512\)</span>，你不能直接把人家图片大小给改了啊，直接通过砍图片大小来达到的压缩不叫压缩。</p><hr /><p>多说一嘴，基的选择要结合实际情况分析，例如这张图片色彩很单调，且一大片区域都是同一种颜色的情况。那么基向量里必有<spanclass="math inline">\(\begin{bmatrix}1 \\ 1 \\ \vdots \\ 1\end{bmatrix}\)</span>。因为这个基向量表达的图片状态就是纯单色图片。当然了，我意思是这个基向量肯定起着主导作用（也就是系数<spanclass="math inline">\(d_i\)</span>大），但是仍要结合使用别的基向量，要不你还原出来的图片就是纯单色图片。</p><h3 id="八.-左右逆-伪逆">八. 左右逆, 伪逆</h3><p>对于矩阵<span class="math inline">\(A \in \mathbb{R}^{m \timesn}\)</span>，若<spanclass="math inline">\(A\)</span>的各列线性无关。则<spanclass="math inline">\(A\)</span>存在左逆：<spanclass="math inline">\((A^\mathrm{T}A)^{-1}A^\mathrm{T}\)</span>。</p><p>拿这玩意左乘<span class="math inline">\(A\)</span>可得到<spanclass="math inline">\(I\)</span>，所以叫左逆。</p><p>为啥条件是各列线性无关呢？因为我们知道<spanclass="math inline">\(A^\mathrm{T}A\)</span>一定是半正定矩阵。且当<spanclass="math inline">\(N(A) ={\textbf{0}}\)</span>时，升级为正定矩阵，而正定矩阵一定可逆。所以才能有<spanclass="math inline">\((A^\mathrm{T}A)^{-1}\)</span>。</p><hr /><p>对应的，我可以得到右逆的定义，若<spanclass="math inline">\(A\)</span>的各行线性无关，则<spanclass="math inline">\(A\)</span>存在右逆：<spanclass="math inline">\(A^\mathrm{T}(AA^\mathrm{T})^{-1}\)</span>。</p><hr /><p>下面来讨论一下伪逆。</p><p>我们知道，如果矩阵<span class="math inline">\(r(A) &lt; \min(m, n), A\in \mathbb{R}^{m \times n}\)</span>，那么对于无解的最小二乘方程：<spanclass="math inline">\(Ax = b\)</span>是求不出最优近似解的。但是伪逆<spanclass="math inline">\(A^+\)</span>却可以求到"最优稳定解"<spanclass="math inline">\(\hat{x}\)</span>，满足：<spanclass="math inline">\(\| A\hat{x}-b \| \le \| Ax - b \|\)</span>。</p><p>ok，那如何求伪逆呢？用SVD。这里我直接给出公式：</p><p><span class="math inline">\(A^+ = V\Sigma^+U^\mathrm{T}\)</span>，<spanclass="math inline">\(\Sigma^+\)</span>就是<spanclass="math inline">\(\Sigma\)</span>的每一个奇异值取倒数。</p><p>（伪逆这里讲的很浅，因为目前还不怎么用得着，等以后需要学习原理的时候再补充）</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;对称矩阵、复数矩阵、FFT、正定矩阵、相似矩阵、SVD分解、线性变换、图像压缩、左右逆/伪逆&lt;/p&gt;</summary>
    
    
    
    <category term="4. 大学" scheme="http://error666.top/categories/4-%E5%A4%A7%E5%AD%A6/"/>
    
    <category term="数学" scheme="http://error666.top/categories/4-%E5%A4%A7%E5%AD%A6/%E6%95%B0%E5%AD%A6/"/>
    
    <category term="线性代数" scheme="http://error666.top/categories/4-%E5%A4%A7%E5%AD%A6/%E6%95%B0%E5%AD%A6/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>线性代数2</title>
    <link href="http://error666.top/2024/09/12/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B02/"/>
    <id>http://error666.top/2024/09/12/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B02/</id>
    <published>2024-09-12T13:56:55.000Z</published>
    <updated>2024-10-04T13:33:17.253Z</updated>
    
    <content type="html"><![CDATA[<p>正交向量/空间、对向量/空间投影、正交矩阵、正交化、行列式、特征值/特征向量、(特征值)对角化、马尔可夫矩阵/傅里叶级数</p><span id="more"></span><hr /><h3 id="一.-正交向量与子空间">一. 正交向量与子空间</h3><p><img src="1.png" /></p><p>先来欣赏下这幅图，这幅图的意思就是对于矩阵<spanclass="math inline">\(A\)</span>，其行空间<spanclass="math inline">\(C(A^\mathrm{T})\)</span>与零空间<spanclass="math inline">\(N(A)\)</span>正交，其列空间<spanclass="math inline">\(C(A)\)</span>与左零空间<spanclass="math inline">\(N(A^\mathrm{T})\)</span>正交。</p><p>什么是向量正交？俩向量正交就是他们俩间夹角呈90度。如果判别俩向量正交？若<spanclass="math inline">\(x^\mathrm{T} \cdot y =0\)</span>，则x与y正交。</p><p>上面那个公式如何推导的呢？</p><p>首先思考如何描述一个向量的长度，假设有一个向量（1,2,3），那么它的长度显然是<spanclass="math inline">\(\sqrt{1^2 + 2^2 +3^2}\)</span>对吧，用向量表示就是<spanclass="math inline">\(\sqrt{x^Tx}\)</span>。</p><p>好的，知道了如何表示向量长度又如何，假设我给你向量<spanclass="math inline">\(x, y\)</span>，你咋判断呢？</p><p>首先思考下<span class="math inline">\(x +y\)</span>是什么？很容易想到，可以把x和y看作三角形的两边，那么<spanclass="math inline">\(x +y\)</span>就是斜边。那么三条边都知道了，如果xy正交的话，那么就会有<spanclass="math inline">\(x^\mathrm{T}x + y^\mathrm{T}y =(x+y)^\mathrm{T}(x+y)\)</span>,化简可得：<spanclass="math inline">\(x^\mathrm{T}x + y^\mathrm{T}y = x^\mathrm{T}x +x^\mathrm{T}y + y^\mathrm{T}x + y^\mathrm{T}y\)</span>，继续化简：<spanclass="math inline">\(x^\mathrm{T}y + y^\mathrm{T}x =0\)</span>，继续化简：<spanclass="math inline">\(2x^\mathrm{T}y=0\)</span>，所以得到<spanclass="math inline">\(x^\mathrm{T}y = 0\)</span>。</p><hr /><p>向量正交很容易理解，但如果我说，俩子空间正交，又是什么意思呢？</p><p>假设我有子空间<span class="math inline">\(S\)</span>和<spanclass="math inline">\(T\)</span>，那么S和T正交，当且仅当S中的每个向量和T中的每个向量正交。</p><p>ok，举个反直觉的实际例子：墙壁和地面正交吗？答案是否，因为墙壁和地面的交集边缝，边缝自己和自己不正交，所以不满足定义。</p><p>所以需要注意的是，若两个空间正交，那么它们一定不会交于某个非零向量。</p><p>所以在实际例子中，最常见的正交就是两条不重合的过原点且为90度的直线，它们是正交的。</p><hr /><p>回到四个基本子空间，现在我要给出结论：行空间正交与零空间。</p><p>为什么，看下面这幅图（Ax=0）：</p><p><img src="2.png" /></p><p>可以发现，row1和x做点乘为0，row2和x做点乘为0，......，所以x与所有行都正交。行空间可以表示为这些行的线性组合，即<spanclass="math inline">\(k_1row_1 + k_2row_2 + ... +k_mrow_m\)</span>，显然，做下点乘：<span class="math inline">\(x \cdot(k_1row_1 + k_2row_2 + ... + k_mrow_m) =0\)</span>，所以行空间与零空间正交。</p><p>同样的，我们可以得出结论：列空间与左零空间正交。道理一样我就不证了。</p><hr /><p>还有有趣的一点，实际上，行空间 + 零空间 = <spanclass="math inline">\(\mathbb{R}^n\)</span>，列空间 + 左零空间 = <spanclass="math inline">\(\mathbb{R}^m\)</span>。</p><p>从上一节的公式就可以一窥究竟：<span class="math inline">\(dim(S) +dim(U) = dim(S + U) + dim(S \cap U)\)</span>。</p><p>把行空间和零空间代入，秩分别为r和n - r，所以(行空间 +零空间)的维素就是n，而且行空间和零空间的分量个数都为n，所以(行空间 +零空间)就等于<span class="math inline">\(\mathbb{R}^n\)</span>。</p><p>列空间 + 左零空间 = <spanclass="math inline">\(\mathbb{R}^m\)</span>同理，我就不证了。</p><p>为了定义这种现象，我们把行空间和零空间称为正交补的。即：行空间与零空间正交补，列空间与左零空间正交补。</p><h3 id="二.-子空间投影">二. 子空间投影</h3><p>首先先从最简单的一个例子开始：</p><p><img src="3.png" /></p><p>我们将向量b投影到向量a上，投影向量为p，误差向量为e。可以发现，p是a的某倍。所以问题的关键，就是在找到这个系数x。</p><p>ok，由几何性质我们知道，向量e与a正交，所以：<spanclass="math inline">\((b - xa)^\mathrm{T}a = 0\)</span>，整理可得：<spanclass="math inline">\(b^\mathrm{T}a =xa^\mathrm{T}a\)</span>，继续：<span class="math inline">\(x =\frac{b^\mathrm{T}a}{a^\mathrm{T}a}\)</span>。当然也可以写成：<spanclass="math inline">\(x =\frac{a^\mathrm{T}b}{a^\mathrm{T}a}\)</span>。</p><p>所以<span class="math inline">\(p = a \cdot\frac{a^\mathrm{T}b}{a^\mathrm{T}a}\)</span>。</p><p>ok上个这个小例子推出来的公式可以留个印象。现在我们进一步抽象化，把投影这个动作看作一种运算，也就是最好弄出一个投影矩阵P，然后b左乘一下这个投影矩阵Pb，就可以得到p，这是我们想要的。</p><p>其实通过上面的公式就可看出，<span class="math inline">\(P =\frac{aa^\mathrm{T}}{a^\mathrm{T}a}\)</span>。非常优美的式子。</p><p>我们来思考一下这个投影矩阵P的列空间。它的列空间是啥呢？</p><p>考虑<spanclass="math inline">\(Px\)</span>，x可取任意向量，那么就相当于对P的列向量做任意线性组合，也就是生成了<spanclass="math inline">\(C(P)\)</span>。通过几何意义我们可以知道，<spanclass="math inline">\(C(P)\)</span>就是通过a的一条线，而且<spanclass="math inline">\(r(P) = 1\)</span>。</p><p>这个投影矩阵<spanclass="math inline">\(P\)</span>还有一些好玩的性质，比如通过其公式，显然看出它是一个对称矩阵，所以<spanclass="math inline">\(P^\mathrm{T} = P\)</span>。</p><p>以及<span class="math inline">\(P^2 =P\)</span>，为什么呢？前面我们说了<spanclass="math inline">\(C(P)\)</span>就是通过a的一条线，而且<spanclass="math inline">\(r(P) = 1\)</span>，说明<spanclass="math inline">\(P\)</span>的每一个列向量都在a那条直线上，所以<spanclass="math inline">\(P \cdot P\)</span>相当于对<spanclass="math inline">\(P\)</span>的每一列做投影，那么通过几何性质，若一个向量本就在线上，投影之后位置仍不变，所以<spanclass="math inline">\(P \cdot P = P\)</span>。</p><p>总结一下：</p><ol type="1"><li><span class="math inline">\(x =\frac{a^\mathrm{T}b}{a^\mathrm{T}a}\)</span>，<spanclass="math inline">\(p = ax = a \cdot\frac{a^\mathrm{T}b}{a^\mathrm{T}a}\)</span></li><li><span class="math inline">\(P =\frac{aa^\mathrm{T}}{a^\mathrm{T}a}\)</span>， <spanclass="math inline">\(p = Pb\)</span></li><li><span class="math inline">\(C(p) = \text{过a的线}\)</span>， <spanclass="math inline">\(r(P) = 1\)</span></li><li><span class="math inline">\(P^\mathrm{T} = P\)</span>，<spanclass="math inline">\(P^2 = P\)</span></li></ol><hr /><p>现在让我们进入更高维度的投影。</p><p>先剧透一下，学这个可以解决什么问题呢？我们知道，Ax =b并不100%有解，有时候，我们在无解的情况下，就想找到最优近似解，也就是误差向量<spanclass="math inline">\(e\)</span>最小。</p><p>怎么做呢？很自然的就联想到投影。因为此时b不在<spanclass="math inline">\(C(A)\)</span>上，所以我们就要将<spanclass="math inline">\(b\)</span>投影到<spanclass="math inline">\(C(A)\)</span>上，得到投影向量p。那么此时求解<spanclass="math inline">\(A\hat{x} = p\)</span>，得到的<spanclass="math inline">\(\hat{x}\)</span>就是最优近似解。</p><p>很巧妙不是吗？这用到了向量对空间的投影。</p><p>继续思考，我们要做的事情，就是找到一个x，使得b与(Ax)的误差向量e垂直C(A)。此时的x就是<spanclass="math inline">\(\hat{x}\)</span>。</p><p>把上面的语言转化为数学语言： <span class="math display">\[\begin{align*}    &amp;p = A\hat{x} \\    &amp;e = b - p = b - (A\hat{x}) \\    &amp;e^\mathrm{T}A = 0\end{align*}\]</span> （<span class="math inline">\(e\)</span>垂直与<spanclass="math inline">\(C(A)\)</span>与<spanclass="math inline">\(e^\mathrm{T}A=0\)</span>等价，因为<spanclass="math inline">\(C(A)\)</span>可由列向量线组出来，所以只要<spanclass="math inline">\(e\)</span>垂直于<spanclass="math inline">\(A\)</span>的每一个列向量，那么<spanclass="math inline">\(e\)</span>就垂直于<spanclass="math inline">\(C(A)\)</span>）</p><p>（上面的方程我们还可以发现一点有趣的事实，显然<spanclass="math inline">\(e\)</span>位于<spanclass="math inline">\(A\)</span>的左零空间内，而通过几何意义我们知道，<spanclass="math inline">\(e\)</span>与<spanclass="math inline">\(C(A)\)</span>正交。很巧的事情，<spanclass="math inline">\(N(A^\mathrm{T})\)</span>与<spanclass="math inline">\(C(A)\)</span>正交。一切都对上了，是不是？）</p><p>OK，整理上面的方程，可以得到：<spanclass="math inline">\(A^\mathrm{T}A\hat{x} =A^\mathrm{T}b\)</span>。</p><p>如果<spanclass="math inline">\(A^\mathrm{T}A\)</span>是可逆的，那么直接乘它的可逆矩阵，即可求出<spanclass="math inline">\(\hat{x}\)</span>。</p><p>对应的投影向量为：<span class="math inline">\(p = A \cdot(A^\mathrm{T}A)^{-1} \cdot A^\mathrm{T}b\)</span></p><p>对应的投影矩阵为：<span class="math inline">\(P = A \cdot(A^\mathrm{T}A)^{-1} \cdot A^\mathrm{T}\)</span></p><p>生成投影向量的线组系数：<span class="math inline">\(\hat{x} =(A^\mathrm{T}A)^{-1} \cdot A^\mathrm{T}b\)</span></p><p>类比上面的向量对向量投影的公式，是不是形式一样？只需把<spanclass="math inline">\(a\)</span>换为<spanclass="math inline">\(A\)</span>，分号改为逆就好啦？美妙的公式。</p><p>同理，此时的投影矩阵<spanclass="math inline">\(P\)</span>同样满足：<spanclass="math inline">\(P^\mathrm{T} = P\)</span>，<spanclass="math inline">\(P^2 = P\)</span></p><hr /><p>ok，让我们讨论剩下的一点细节，前面我们是把<spanclass="math inline">\(b\)</span>投影到<spanclass="math inline">\(C(A)\)</span>中。那如果我想把<spanclass="math inline">\(b\)</span>投到与<spanclass="math inline">\(C(A)\)</span>正交的空间，也就是<spanclass="math inline">\(N(A^\mathrm{T})\)</span>呢？显然，通过几何意义可以看出，答案就是<spanclass="math inline">\(e\)</span>，即<span class="math inline">\(b -p\)</span>。</p><p>所以投影到<spanclass="math inline">\(N(A^\mathrm{T})\)</span>就是：<spanclass="math inline">\(b - p = b - Pb = (I -P)b\)</span>。所以对应的投影矩阵就是<span class="math inline">\(I -P\)</span>。同样的，<spanclass="math inline">\(P\)</span>满足的性质，<spanclass="math inline">\(I - P\)</span>也都满足。</p><p>以及，前面提到，必须<spanclass="math inline">\(A^\mathrm{T}A\)</span>是可逆矩阵，才能解出<spanclass="math inline">\(\hat{x}\)</span>。但是，如何判断它可逆呢？有一个判断方法：若<spanclass="math inline">\(A\)</span>的各列线性无关，那么<spanclass="math inline">\(A^\mathrm{T}A\)</span>就是可逆矩阵。下面来证明一下：</p><blockquote><p>假设<span class="math inline">\(A\)</span>的各列线性无关。</p><p>考虑方程<span class="math inline">\(A^\mathrm{T}Ax = 0\)</span></p><p>两边同乘<span class="math inline">\(x^\mathrm{T}\)</span>，得：<spanclass="math inline">\(x^\mathrm{T}A^\mathrm{T}Ax = 0\)</span></p><p>整理：<span class="math inline">\((Ax)^\mathrm{T}(Ax)=0\)</span></p><p>所以说嘛向量<spanclass="math inline">\((Ax)\)</span>长度为0，也就是它为零向量，所以：<spanclass="math inline">\(Ax=0\)</span></p><p>因为<span class="math inline">\(A\)</span>的各列线性无关，所以<spanclass="math inline">\(dim(N(A)) = 0\)</span>，所以<spanclass="math inline">\(x\)</span>只能去零向量。</p><p>所以对于方程<span class="math inline">\(A^\mathrm{T}Ax =0\)</span>，<span class="math inline">\(x\)</span>只能取零向量。</p><p>而只有当<spanclass="math inline">\(A^\mathrm{T}A\)</span>可逆的时候，上面的方程才只有零解。证毕。</p></blockquote><hr /><p>最后，让我们举一个用投影来解决的实际例子：最小二乘。</p><p>平面上有n个点，要找到一条直线，尽可能的拟合这些点，怎么办？</p><p>假设有这么一条理想直线：y = kx + b，可以拟合所有点，那么就会有方程：<span class="math display">\[\begin{align*}    kx_1 + b &amp;= y_1 \\    kx_2 + b &amp;= y_2 \\    ... \\    kx_n + b &amp;= y_n\end{align*}\]</span> 我们要求k和b，所以把k和b看为未知数。</p><p>那么即可建模为<span class="math inline">\(Ax=b\)</span>，<spanclass="math inline">\(A = \begin{bmatrix} x_1 &amp; 1 \\ x_2 &amp; 1 \\... &amp; ... \\ x_n &amp; 1 \end{bmatrix}\)</span>，<spanclass="math inline">\(x = \begin{bmatrix} k \\ b\end{bmatrix}\)</span>，<span class="math inline">\(b = \begin{bmatrix}y_1 \\ y_2 \\ ... \\ y_n \end{bmatrix}\)</span></p><p>这个方程大概率是无解的，因为不存在这么一条完美直线，所以就把<spanclass="math inline">\(b\)</span>投影到<spanclass="math inline">\(C(A)\)</span>即可。</p><p>巧妙的建模，相信你再一次感受到线代的魅力了！</p><ul><li>总结一下：<ul><li>一维（向量b投影到向量a）：<ul><li><span class="math inline">\(P =\frac{aa^\mathrm{T}}{a^\mathrm{T}a}\)</span>，<spanclass="math inline">\(p = Pb\)</span></li></ul></li><li>高维（向量b投影到空间C(A)）：<ul><li><span class="math inline">\(P = A \cdot (A^\mathrm{T}A)^{-1} \cdotA^\mathrm{T}\)</span></li><li><span class="math inline">\(p = A \cdot (A^\mathrm{T}A)^{-1} \cdotA^\mathrm{T}b\)</span></li></ul></li><li>判断对称矩阵<spanclass="math inline">\(A^\mathrm{T}A\)</span>是否可逆的方法：看<spanclass="math inline">\(A\)</span>的各列是否线性无关</li></ul></li></ul><h3 id="三.-正交矩阵-schmidt正交化">三. 正交矩阵, Schmidt正交化</h3><p>在这一节里，我们用<spanclass="math inline">\(Q\)</span>来代表列向量全是正交且标准（长度为1）的矩阵。</p><p>显然，这个<span class="math inline">\(Q\)</span>满足下列定义：<spanclass="math inline">\(q_{i}^{T}q_{j}=\begin{cases}0&amp;,i\neqj\\1&amp;,i=j\end{cases}\)</span></p><p>讨论一下，<spanclass="math inline">\(Q^\mathrm{T}Q\)</span>是什么？<spanclass="math inline">\(Q = \begin{bmatrix} q_1, q_2, ..., q_n\end{bmatrix}\)</span>，<span class="math inline">\(Q^\mathrm{T} =\begin{bmatrix} q_1^\mathrm{T} \\ q_2^\mathrm{T} \\ ... \\q_n^\mathrm{T} \end{bmatrix}\)</span>。</p><p>显然，<span class="math inline">\(Q^\mathrm{T}Q = I\)</span>。</p><p>下面，我们对“正交矩阵”这个名词下一个定义：首先得是方阵，其次列向量都是互相正交且单位的。这样的矩阵就叫正交矩阵。</p><p>（注意，本节的<spanclass="math inline">\(Q\)</span>不一定是方阵，若无特殊说明，是不是方阵都有可能）</p><p>ok，让我们现在讨论一下正交矩阵的性质。</p><p>正交矩阵也满足<span class="math inline">\(Q^\mathrm{T}Q =I\)</span>，而且因为为方阵，所以<spanclass="math inline">\(Q\)</span>有逆矩阵。所以可以推出<spanclass="math inline">\(Q^\mathrm{T} = Q^{-1}\)</span>。</p><p>OK，让我们思考一下正交矩阵的投影矩阵：<span class="math inline">\(P =Q(Q^\mathrm{T}Q)^{-1}Q^\mathrm{T} =QQ^\mathrm{T}\)</span>。因为是正交矩阵，所以<spanclass="math inline">\(Q^\mathrm{T} = Q^{-1}\)</span>，所以<spanclass="math inline">\(P =I\)</span>，即正交矩阵的投影矩阵就是单位阵。如果<spanclass="math inline">\(Q\)</span>不是方阵的话，那么其投影矩阵就是<spanclass="math inline">\(QQ^\mathrm{T}\)</span>。当然了，它们的投影矩阵都是满足那俩性质的：</p><ol type="1"><li>是对称矩阵</li><li><span class="math inline">\(P^2 = P\)</span></li></ol><hr /><p>正交且标准的性质是很好的，所以我们如何把一个虽然列向量互相线性无关（但是不正交）的矩阵<spanclass="math inline">\(A\)</span>，转化为列向量互相正交且标准的矩阵<spanclass="math inline">\(Q\)</span>呢？下面来介绍格拉姆-施密特正交法：</p><p>思考现在有向量<span class="math inline">\(a, b,c\)</span>，它们互相线性无关，但是不正交，如果把它们转换为一组正交的呢？</p><p>首先把<spanclass="math inline">\(a\)</span>单位化然后锁死，然后对于<spanclass="math inline">\(b\)</span>，<spanclass="math inline">\(b\)</span>减去<spanclass="math inline">\(b\)</span>在<spanclass="math inline">\(a\)</span>上的投影（也就是误差向量<spanclass="math inline">\(e\)</span>）就是我们想要的，所以<spanclass="math inline">\(b&#39; = b - p = b - Pb = b -\frac{aa^\mathrm{T}}{a^\mathrm{T}a}\cdotb\)</span>，然后单位化，锁死。</p><p>然后对于<span class="math inline">\(c\)</span>，先把在<spanclass="math inline">\(a\)</span>上的投影减掉，再把在更新后的<spanclass="math inline">\(b\)</span>上的投影减掉，就得到了正交于<spanclass="math inline">\(a, b\)</span>的向量：<spanclass="math inline">\(c&#39; = c - p_a - p_b = c -\frac{aa^\mathrm{T}}{a^\mathrm{T}a} \cdot c -\frac{bb^\mathrm{T}}{b^\mathrm{T}b} \cdotc\)</span>，然后单位化，锁死。</p><p>其余的向量以此类推。</p><p>很好，现在你学会了如何将一组虽然线性无关但是不是正交的向量转化为正交的向量组，恭喜。</p><p>但是我要提个问题，转化前的矩阵<spanclass="math inline">\(A\)</span>和转化后的矩阵<spanclass="math inline">\(Q\)</span>，它们的列空间一样吗？</p><p>答案是肯定的，因为考虑我们转化的过程，例如<spanclass="math inline">\(c = c - p_a - p_b\)</span>，<spanclass="math inline">\(p_a\)</span>是<spanclass="math inline">\(a\)</span>的缩放，<spanclass="math inline">\(p_b\)</span>是<spanclass="math inline">\(b\)</span>的缩放，所以其实我们在正交化的过程中，仍然是用原向量组做线性组合（即列变换），所以转换后的矩阵，其列空间一样<spanclass="math inline">\(C(Q)\)</span>=<spanclass="math inline">\(C(A)\)</span>，零空间一样<spanclass="math inline">\(N(Q) = N(A)\)</span>。</p><hr /><p>截止，你已经知道了正交的概念，也认识到了正交矩阵以及一些性质。甚至你还学会了如何将一组线性无关向量组转化为标准正交的向量组。</p><p>但是，知道这些，有什么用呢？</p><p>目前我知道的是，可以讲一个各列线性无关的矩阵<spanclass="math inline">\(A\)</span>进行<spanclass="math inline">\(QR\)</span>分解：<span class="math inline">\(A =QR\)</span>。<spanclass="math inline">\(Q\)</span>表示各列互相标准正交的向量，<spanclass="math inline">\(R\)</span>是一个上三角矩阵。</p><p>为啥<spanclass="math inline">\(R\)</span>是一个上三角矩阵呢？我来证明一下：</p><p>在正交化的过程中，对于<spanclass="math inline">\(a_i\)</span>，它依赖了<spanclass="math inline">\(q_1, q_2, \cdots, q_{i-1},a_i\)</span>，线组出了<spanclass="math inline">\(q_i\)</span>。所以，如果我有<spanclass="math inline">\(q_1, q_2, \cdots,q_i\)</span>，那我就可线组出<spanclass="math inline">\(a_i\)</span>。所以对于<spanclass="math inline">\(A = QR\)</span>，<spanclass="math inline">\(R\)</span>就是线组的系数。而且可发现想线组出<spanclass="math inline">\(a_i\)</span>只需用到<span class="math inline">\(1\sim i\)</span>的<span class="math inline">\(q\)</span>，所以<spanclass="math inline">\(R\)</span>自然就是一个上三角的了。</p><h3 id="四.-行列式及其性质">四. 行列式及其性质</h3><p>行列式是方阵独属的浪漫，通常记为<spanclass="math inline">\(det(A)\)</span>或者<spanclass="math inline">\(|A|\)</span>。</p><p>行列式这里性质特别多，可能需要一些记忆：</p><ol type="1"><li><p>单位阵的行列式为1</p></li><li><p>交换两行，行列式符号会取反</p></li><li><p><span class="math inline">\(\begin{vmatrix}  ta &amp; tb \\  c&amp; d  \end{vmatrix} = t\begin{vmatrix}  a &amp; b \\  c &amp;d  \end{vmatrix}\)</span></p></li><li><p><span class="math inline">\(\begin{vmatrix}  a+a&#39; &amp;b+b&#39; \\  c &amp; d  \end{vmatrix} = \begin{vmatrix}  a &amp; b \\  c&amp; d  \end{vmatrix} + \begin{vmatrix}  a&#39; &amp; b&#39; \\  c&amp; d  \end{vmatrix}\)</span></p></li><li><p>若有两行相等，则行列式为0</p></li><li><p>行j减去行i的k倍，行列式不变</p></li><li><p>如果有一行为0，那么行列式为0</p></li><li><p>对于上三角方阵，其行列式为对角线元素乘积</p></li><li><p><span class="math inline">\(det(A)=0 \iff\)</span> 矩阵<spanclass="math inline">\(A\)</span>是奇异矩阵（奇异矩阵就是不满秩的方阵）</p></li><li><p><span class="math inline">\(det(AB) = det(A) \cdotdet(B)\)</span></p></li><li><p><span class="math inline">\(det(A^\mathrm{T}) =det(A)\)</span></p><ul><li>有了这条性质，那么上面描述行的性质，同样可以描述列，例如：</li><li>交换两列，行列式符号取反</li><li>除了行有线性关系（性质3、4），列也具有</li><li>若有两列相等，则行列式为0</li><li>如果有一列为0，那么行列式为0</li></ul></li></ol><p>所以如何求一个方阵的行列式？通常就是将其消元成上三角矩阵（注意过程中行交换会导致行列式符号取反），然后对角线相乘即可。</p><hr /><p>行列式公式1：<span class="math inline">\(det(A) = \sum (-1)^{r(k_1,k_2, k_3, ..., k_n)}a_{1k_1}a_{2k_2}a_{3k_3}...a_{nk_n}\)</span></p><p>​ <span class="math inline">\(r(k_1, k_2, ...,k_n)\)</span>是排列的逆序数。</p><p>代数余子式：位置(i, j)的代数余子式<span class="math inline">\(A_{ij}:= (-1)^{i+j}det(\text{去掉第i行和第j列得到的矩阵})\)</span></p><p>行列式公式2：<span class="math inline">\(det(A) = a_{11}C_{11} +a_{12}C_{12} + \cdots + a_{1n}C_{1n}\)</span></p><h3 id="五.-克拉默法则-体积">五. 克拉默法则, 体积</h3><p>设<span class="math inline">\(A_{ij}\)</span>是位置(i,j)的代数余子式，则矩阵<spanclass="math inline">\(A\)</span>的伴随矩阵定义如下： <spanclass="math display">\[A^* = \begin{bmatrix}A_{11} &amp; A_{21} &amp; \cdots &amp; A_{n1} \\A_{12} &amp; A_{22} &amp; \cdots &amp; A_{n2} \\\cdots \\A_{1n} &amp; A_{2n} &amp; \cdots &amp; A_{nn}\end{bmatrix}\]</span> 它满足一个公式，通过代数法求逆：<spanclass="math inline">\(A^{-1} = \frac{A^*}{det(A)}\)</span></p><p>那么对于非奇异矩阵<spanclass="math inline">\(A\)</span>，它的方程：<spanclass="math inline">\(Ax=b\)</span>就有一种新的解法。</p><p><span class="math inline">\(x = A^{-1}b =\frac{A^*b}{det(A)}\)</span></p><p>所以可得到以下式子： <span class="math display">\[\begin{align*}    x_1 &amp;= \frac{det(\text{把A的第一列换为b})}{det(A)} \\    x_2 &amp;= \frac{det(\text{把A的第二列换为b})}{det(A)} \\    \cdots \\    x_n &amp;= \frac{det(\text{把A的第n列换为b})}{det(A)}\end{align*}\]</span>上面就是克拉默法则，用代数的方程解方程。但是我觉得，中看不中用，不如直接用矩阵的方程去解方程组。</p><hr /><p>给出一个有意思的定理：行列式的绝对值其实是在计算“箱子”的体积。</p><p>我举个例子，比如有矩阵<span class="math inline">\(A = \begin{bmatrix}a_{11} &amp; a_{12} &amp; a_{13} \\ a_{21} &amp; a_{22} &amp; a_{23} \\a_{31} &amp; a_{32} &amp; a_{33} \end{bmatrix}\)</span></p><p>那么<span class="math inline">\(|det(A)|\)</span>等于以<spanclass="math inline">\((a_{11}, a_{12}, a_{13})\)</span>，<spanclass="math inline">\((a_{21}, a_{22}, a_{23})\)</span>、<spanclass="math inline">\((a_{31}, a_{32},a_{33})\)</span>为三边所形成的箱子的体积。</p><p>同理，对于<span class="math inline">\(n \timesn\)</span>的矩阵同样成立。</p><h3 id="六.-特征值-特征向量">六. 特征值, 特征向量</h3><p>特征值是方阵独属的浪漫。</p><p>首先来讲特征向量，矩阵<spanclass="math inline">\(A\)</span>的特征向量就是那些进过<spanclass="math inline">\(A\)</span>线性变换后方向不改变的向量。用数学语言表达就是：<spanclass="math inline">\(Ax = \lambda x\)</span>。满足上述方程的<spanclass="math inline">\(x\)</span>就是特征向量，<spanclass="math inline">\(\lambda\)</span>就是特征值。</p><p>对于特征值0，它对应的特征向量应满足<spanclass="math inline">\(Ax=0\)</span>，所以特征值0所对应的特征向量其实就是<spanclass="math inline">\(N(A)\)</span>。</p><p>举点例子吧，考虑投影矩阵<spanclass="math inline">\(P\)</span>。对于那些本身就已经在投影面上的向量<spanclass="math inline">\(x\)</span>，满足<span class="math inline">\(Px =x\)</span>，所以<spanclass="math inline">\(P\)</span>特征值为1的特征向量就是投影面上的向量。对于那些垂直于投影面的向量<spanclass="math inline">\(x\)</span>，满足<span class="math inline">\(Px =0\)</span>，所以<spanclass="math inline">\(P\)</span>特征值为0的特征向量就是垂直于投影面的那些向量。</p><p>ok，我提前透露几个特征值的性质：</p><ol type="1"><li>n阶方阵有n个特征值</li><li>n个特征值的和加起来等于方阵对角线之和</li><li>n个特征值的乘积等于方阵的特征值</li><li>上三角方阵的特征值就是对角线上的元素</li><li><span class="math inline">\(A\)</span>的特征值等于<spanclass="math inline">\(A^\mathrm{T}\)</span>的特征值</li><li><span class="math inline">\(A^{-1}\)</span>的特征值等于<spanclass="math inline">\(A\)</span>的特征值取倒数</li></ol><hr /><p>好，那给你一个矩阵<spanclass="math inline">\(A\)</span>，如何求出其特征值和对应的特征向量呢？</p><p>很简单，首先列出定义：<span class="math inline">\(Ax = \lambdax\)</span>，移项：<span class="math inline">\((A - \lambda I)x =0\)</span>。要使这个方程有非零解，<span class="math inline">\((A -\lambda I)\)</span>要是奇异矩阵，也就是不满秩，也就是<spanclass="math inline">\(|A - \lambda I| = 0\)</span>。</p><p>解上面那个行列式，即可求出所有的特征值<spanclass="math inline">\(\lambda\)</span>（n个特征值可能有重复）。</p><p>然后反代回去，即可求出特征值对应的特征向量。</p><h3 id="七.-对角化-a的幂">七. 对角化, A的幂</h3><p>上一节我们学会了如何求一个矩阵<spanclass="math inline">\(A\)</span>的特征值和对应的特征向量。这一节我们来利用特征向量来分解矩阵。</p><p>假设我们有一个矩阵<spanclass="math inline">\(A\)</span>，它有n个线性无关的特征向量。那么我把这些向量排成一排得到矩阵<spanclass="math inline">\(S\)</span>，叫做特征向量矩阵。然后推导下面式子：</p><p><span class="math inline">\(AS = A \cdot \begin{bmatrix} \beta_1&amp; \beta_2 \cdots \beta_n \end{bmatrix} = \begin{bmatrix}\lambda_1\beta_1 &amp; \lambda_2\beta_2 &amp; \cdots &amp;\lambda_n\beta_n \end{bmatrix} = \begin{bmatrix} \beta_1 &amp; \beta_2&amp; \cdots &amp; \beta_n \end{bmatrix}\begin{bmatrix} \lambda_1 &amp;&amp; &amp; &amp; \\ &amp; \lambda_2 &amp; &amp; &amp; &amp; \\ &amp;&amp; \lambda_3 &amp; &amp; \\ &amp; &amp; &amp; &amp; \cdots\end{bmatrix}\)</span></p><p>即：<span class="math inline">\(AS =S\Lambda\)</span>，<spanclass="math inline">\(\Lambda\)</span>为用特征值生成的对角阵，也叫特征值矩阵</p><p>进一步化简，得到： <span class="math display">\[\Lambda = S^{-1}AS \\A = S\Lambda S^{-1}\]</span>这种对角化分解有什么用呢？答案：在处理矩阵的幂的时候非常有用</p><p>首先来看一下<span class="math inline">\(A^2\)</span>：<spanclass="math inline">\(A^2 = S\Lambda S^{-1} \cdot S\Lambda S^{-1} =S\Lambda^2 S^{-1}\)</span></p><p>可以发现，<span class="math inline">\(A^2\)</span>的特征值就是<spanclass="math inline">\(A\)</span>特征值的平方，<spanclass="math inline">\(A^2\)</span>的特征向量与<spanclass="math inline">\(A\)</span>一样。</p><p>同理，<span class="math inline">\(A^k\)</span>的特征值就是<spanclass="math inline">\(A\)</span>特征值的<spanclass="math inline">\(k\)</span>次方，<spanclass="math inline">\(A^k\)</span>的特征向量与<spanclass="math inline">\(A\)</span>一样。</p><p>来个好玩的问题，当<spanclass="math inline">\(A\)</span>的特征值满足什么条件时，<spanclass="math inline">\(A^k = O, k \to \infty\)</span>？</p><p>通过上面的公式，可得<span class="math inline">\(A^k =S\Lambda^kS^{-1}\)</span>，显然<spanclass="math inline">\(S\)</span>是固定的，所以关键就是看<spanclass="math inline">\(\Lambda\)</span>。很容易想到，如果<spanclass="math inline">\(A\)</span>的所有特征值满足<spanclass="math inline">\(|\lambda_i| &lt; 1\)</span>的话，那么矩阵<spanclass="math inline">\(A\)</span>会收敛到零矩阵。</p><p>（<strong>上面对角化分解非常有用，但需要注意分解的前提是<spanclass="math inline">\(A\)</span>有n个线性无关的特征向量。如何判断呢？这里给出一个定理：如果<spanclass="math inline">\(A\)</span>有n个互不相同的特征值，那么<spanclass="math inline">\(A\)</span>就有n个线性无关的特征向量；否则则不一定。</strong>）</p><hr /><p>先介绍一阶差分方程，即：<span class="math inline">\(u_{k+1} =Au_k\)</span>。（我们考虑理想的情况，即认为<spanclass="math inline">\(A\)</span>有n个互不相同的特征值。）</p><p>那么可推出：<span class="math inline">\(u_k = A^ku_0\)</span></p><p>因为<spanclass="math inline">\(A\)</span>的各特征向量线性无关，所以<spanclass="math inline">\(\mathbb{R}^n\)</span>可用特征向量线组出来，<spanclass="math inline">\(u_0\)</span>同样可以用特征向量线组出来，设线组的系数为<spanclass="math inline">\(c\)</span>列向量，那么<spanclass="math inline">\(u_0\)</span>可表示为<spanclass="math inline">\(Sc\)</span>。</p><p>所以<span class="math inline">\(u_k = S\Lambda^kS^{-1} \cdot Sc =S\Lambda^kc\)</span>。</p><p>下面我们来一道经典的例题，现有斐波拉契数列：0, 1, 1, 2, 3, 5, 8, ......。试用矩阵求斐波拉契数列，并分析其增长速度。</p><p>首先可写出递推式：<span class="math inline">\(F_{k+2} = F_{k+1} +F_k\)</span>。可以发现这是一个二阶的差分方程，我们想把其转为前面的知识转为一阶的，所以我使用一个trick，就是再加入一个方程，然后引入新变量去表达方程组，使其变为一阶差分方程，具体如下：<span class="math display">\[\begin{cases}F_{k+2} = F_{k+1} + F_k \\F_{k+1} = F_{k+1}\end{cases}\]</span> 设<span class="math inline">\(u_k = \begin{bmatrix}F_{k+1} \\F_k\end{bmatrix}\)</span>，所以上述方程组可表达为一个一阶差分方程：<spanclass="math inline">\(u_{k+1} = \begin{bmatrix}1 &amp; 1 \\ 1 &amp;0\end{bmatrix}u_k\)</span>。</p><p>好，先来看看矩阵<span class="math inline">\(A = \begin{bmatrix}1&amp; 1 \\ 1 &amp; 0\end{bmatrix}\)</span>是否可对角化，有<spanclass="math inline">\(\begin{cases} \lambda_1 + \lambda_2 = 1 \\\lambda_1 \cdot \lambda_2 = -1 \end{cases}\)</span>，解得：<spanclass="math inline">\(\begin{cases} \lambda_1 = \frac12(1 + \sqrt{5})\approx 1.618 \\ \lambda_2 = \frac12(1 - \sqrt{5}) \approx -0.618\end{cases}\)</span></p><p>有俩不同特征值，由前面的判定定理可知，<spanclass="math inline">\(A\)</span>可对角化，那么可求出其特征向量矩阵<spanclass="math inline">\(S = \begin{bmatrix} \lambda_1 &amp; \lambda_2 \\ 1&amp; 1 \end{bmatrix}\)</span>。</p><p>由前面的公式可知：<span class="math inline">\(u_k =S\Lambda^kc\)</span>。在这里，<spanclass="math inline">\(c\)</span>是用特征向量表示出<spanclass="math inline">\(\begin{bmatrix} F_1 \\ F_0\end{bmatrix}\)</span>的系数列向量。让我求一下：<spanclass="math inline">\(u_0 = \begin{bmatrix} 1 \\ 0 \end{bmatrix} =c_1\begin{bmatrix} \lambda_1 \\ 1 \end{bmatrix} + c_2\begin{bmatrix}\lambda_2 \\ 1 \end{bmatrix}\)</span>，这里就不解了。</p><p>所以现在<span class="math inline">\(S, \Lambda,c\)</span>都有了，那么<spanclass="math inline">\(u_k\)</span>就可求出来了，即<spanclass="math inline">\(F_k\)</span>就可求出来了。</p><p>但是这个一阶差分方程的增长速度我们还没分析，其实观察<spanclass="math inline">\(u_k =S\Lambda^kc\)</span>就可知道，增长速度由特征值决定，若<spanclass="math inline">\(|\lambda_i| &lt; 1\)</span>，那么<spanclass="math inline">\(u_k\)</span>直接会收俩到0。</p><p>对于那些<span class="math inline">\(|\lambda_i| &gt;1\)</span>的，越大的<spanclass="math inline">\(|\lambda_i|\)</span>，只要对应的<spanclass="math inline">\(c_i \ne0\)</span>，那么对应的特征向量增长速度就越快。</p><p>这一节的内容稍微有点点难消化，不过这已经有点点科研证明的味道了。多看多理解。</p><hr /><h3 id="八.-微分方程-expat">八. 微分方程, exp(At)</h3><p>在开始这节课之前，我觉得有必要补充介绍一点微分方程的概念。</p><p>定义：含自变量（例如<spanclass="math inline">\(x\)</span>）、函数（例如<spanclass="math inline">\(y\)</span>）以及函数各阶导数（例如<spanclass="math inline">\(\dot{y},\ddot{y}\)</span>）的等式称为微分方程。</p><p>抱歉，真听不懂，是我高数太垃圾了，回头补完微分方程再来听这节课。</p><h3 id="九.-马尔可夫矩阵-傅里叶级数">九. 马尔可夫矩阵, 傅里叶级数</h3><p>什么是马尔可夫矩阵，若<spanclass="math inline">\(A\)</span>满足以下两条定义，则它是马尔可夫矩阵：</p><ol type="1"><li>所有元素大于0（概率值不能为负数）</li><li>每一列元素和为1</li><li>方阵</li></ol><p>对于马尔可夫矩阵，很容易发现，其幂即<spanclass="math inline">\(A^k\)</span>同样也是马尔可夫矩阵。</p><p>这里我直接给出两个结论：</p><ol type="1"><li>马尔可夫矩阵有一个特征值为1</li><li>马尔可夫矩阵其余特征值绝对值小于等于1</li></ol><p>根据上面的结果，我们可以知道，如果一个向量一直右边马尔可夫矩阵，那么最终会达到一个稳态。显然这个稳态是我们关心的，我们需要找到它。</p><p>假设矩阵<spanclass="math inline">\(A\)</span>有n个线性无关的特征向量<spanclass="math inline">\(\beta\)</span>-s，那么对于式子：<spanclass="math inline">\(u_k = A^ku_0\)</span></p><p><span class="math inline">\(u_0\)</span>可以被表示为：<spanclass="math inline">\(u_0 = c_1\beta_1 + c_2\beta_2 + \cdots +c_n\beta_n = Sc\)</span></p><p><span class="math inline">\(A^k\)</span>根据前面所学可对角化为：<spanclass="math inline">\(A_k = S\Lambda^kS^{-1}\)</span></p><p><span class="math inline">\(\therefore u_k = S\Lambda^kS^{-1}Sc =S\Lambda^kc = c_1\lambda_1\beta_1 + c_2\lambda_2\beta_2 + \cdots +c_n\lambda_n\beta_n\)</span></p><p>所以那些绝对值&lt;1的项最终会迭代没，绝对值为1的项加起来就是稳态。</p><hr /><p>傅里叶级数是一个可用来拟合任意周期函数的工具，例如我想拟合一个周期为<spanclass="math inline">\(2\pi\)</span>的函数<spanclass="math inline">\(f(x)\)</span>，可用傅立叶级数表达为下列形式：</p><p><span class="math inline">\(f(x) = a_0\cdot 1 + a_1\cos x + b_1\sin x+ a_2\cos 2x + b_2\sin 2x + \cdots + a_n\cos nx+ b_n\sin nx\)</span></p><p>所以关键就是确定下系数<span class="math inline">\(a_i,b_i\)</span>，那我们用线性代数来看待这个问题。</p><p>可以把<span class="math inline">\(1, \cos x, \sin x, \cos 2x, \sin2x, \cdots, \cos nx, \sin nx\)</span>看作基<spanclass="math inline">\(\beta_i\)</span>-s，<spanclass="math inline">\(f(x)\)</span>看作<spanclass="math inline">\(b\)</span>，那么<span class="math inline">\(a_i,b_i\)</span>就是线性组合的系数。</p><p>让我写成这种形式：<span class="math inline">\(c_0\beta_0 + c_1\beta_1+ c_2\beta_2 + \cdots + c_n\beta_n = b\)</span></p><p>问题即为求出<span class="math inline">\(c_i\)</span>。</p><p>假设<spanclass="math inline">\(\beta_i\)</span>-s们正交就好了，我们来检查一下是否正交。</p><p>因为这里的“向量”是函数，所以离散型的点积在这里并不是适用，对于函数<spanclass="math inline">\(f(x), g(x)\)</span>，其实点积是<spanclass="math inline">\(\int_a^b f(x)g(x)\mathrm{d}x\)</span>，因为本题函数周期为<spanclass="math inline">\(2\pi\)</span>，所以俩函数点积为：<spanclass="math inline">\(\int_0^{2\pi}f(x)g(x)\mathrm{d}x\)</span>，检查一下发现这些“基”们确实是正交的。</p><p>那么求系数<spanclass="math inline">\(c_i\)</span>就好办了，比如我要求<spanclass="math inline">\(c_1\)</span>，那么等式两边分别“点积”<spanclass="math inline">\(\beta_1\)</span>，得：</p><p><span class="math inline">\(c_1 \int_0^{2\pi}(cosx)^2\mathrm{d}x =\int_0^{2\pi}f(x)\cos x\mathrm{d}x\)</span></p><p>因为正交性，非<spanclass="math inline">\(\beta_1\)</span>的项都为0了，所以解这个方程即可把<spanclass="math inline">\(c_1\)</span>求出来，其余系数求法同理。</p><p>非常巧妙优美的做法。</p><h3 id="十.-复习课二">十. 复习课二</h3><p>主要因为这章的知识比较重要，所以适合来2道例题巩固一下。而且我想通过例题顺便补充下代数重数和几何重数的知识点。</p><p><strong>例1.</strong> <span class="math inline">\(a = \begin{bmatrix}2 \\ 1 \\ 2 \end{bmatrix}\)</span></p><ol type="1"><li>求投影到<span class="math inline">\(a\)</span>的投影矩阵<spanclass="math inline">\(P\)</span></li></ol><p>套公式：<span class="math inline">\(P =\frac{aa^\mathrm{T}}{a^\mathrm{T}a} = \frac{1}{9} \begin{bmatrix} 4&amp; 2 &amp; 4 \\ 2 &amp; 1 &amp; 2 \\ 4 &amp; 2 &amp; 4\end{bmatrix}\)</span></p><ol start="2" type="1"><li>求<span class="math inline">\(P\)</span>的秩</li></ol><p>因为<span class="math inline">\(P\)</span>的列空间<spanclass="math inline">\(C(P)\)</span>是投影面，而投影面又是三维空间里的一维直线，所以<spanclass="math inline">\(dim(C(A)) = r = 1\)</span>。</p><ol start="3" type="1"><li>求<span class="math inline">\(P\)</span>的特征值</li></ol><p>因为<span class="math inline">\(r(P) =1\)</span>，所以它是奇异，所以<span class="math inline">\(det(P) =0\)</span>，所以它必有一个特征值为0。</p><p>我们知道特征值为0对应的特征向量就是<spanclass="math inline">\(N(P)\)</span>里的那些基们，而我们知道<spanclass="math inline">\(dim(N(P)) = n - r =2\)</span>，所以基里有俩向量，所以特征值0的几何重数为2，又因为几何重数&lt;= 代数重数，所以至少有两个特征值为0。</p><p><spanclass="math inline">\(P\)</span>的迹又是1，所以不可能三个特征值都为0，所以可确定特征值0的代数重数也为2。所以特征值分别为0、0、1</p><blockquote><p>知识点补充：代数重数、几何重数</p><p>代数重数就是某特征值重复的个数</p><p>几何重数就是某特征值对应的互相线性无关的特征向量的个数。这些线性无关的特征向量组合的空间叫做特征子空间</p><p>性质：几何重数 &lt;= 代数重数</p></blockquote><ol start="4" type="1"><li><spanclass="math inline">\(P\)</span>特征值为1对应的特征向量是啥</li></ol><p>其实就是问你<span class="math inline">\(Px=x\)</span>的<spanclass="math inline">\(x\)</span>都有谁，根据几何意义，显然<spanclass="math inline">\(x\)</span>就在投影面上啊，即那条线，所以特征向量写<spanclass="math inline">\(a\)</span>就行了</p><p><strong>例2.</strong> 已知一个4阶方阵<spanclass="math inline">\(A\)</span>具有特征值<spanclass="math inline">\(\lambda_1, \lambda_2, \lambda_3,\lambda_4\)</span></p><ol type="1"><li>特征值需要满足什么条件才能保证<spanclass="math inline">\(A\)</span>为可逆矩阵</li></ol><p>可逆矩阵说明<span class="math inline">\(r(A) =n\)</span>，那么零空间的维数就为0。而如果有特征值为0，那么必然说明零空间有非零向量，即<spanclass="math inline">\(r(A) \nen\)</span>。所以必须满足所有特征值不为0，才能保证<spanclass="math inline">\(A\)</span>为可逆矩阵</p><ol start="2" type="1"><li>求<span class="math inline">\(A^{-1}\)</span>的行列式</li></ol><p>因为<span class="math inline">\(A^{-1}\)</span>的特征值是<spanclass="math inline">\(A\)</span>特征值取倒数，行列式又是特征值之积，所以<spanclass="math inline">\(det(A^{-1}) =\frac{1}{\lambda_1\lambda_2\lambda_3\lambda_4}\)</span></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;正交向量/空间、对向量/空间投影、正交矩阵、正交化、行列式、特征值/特征向量、(特征值)对角化、马尔可夫矩阵/傅里叶级数&lt;/p&gt;</summary>
    
    
    
    <category term="4. 大学" scheme="http://error666.top/categories/4-%E5%A4%A7%E5%AD%A6/"/>
    
    <category term="数学" scheme="http://error666.top/categories/4-%E5%A4%A7%E5%AD%A6/%E6%95%B0%E5%AD%A6/"/>
    
    <category term="线性代数" scheme="http://error666.top/categories/4-%E5%A4%A7%E5%AD%A6/%E6%95%B0%E5%AD%A6/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>线性代数1</title>
    <link href="http://error666.top/2024/09/11/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B01/"/>
    <id>http://error666.top/2024/09/11/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B01/</id>
    <published>2024-09-11T10:58:18.000Z</published>
    <updated>2024-09-17T15:27:41.582Z</updated>
    
    <content type="html"><![CDATA[<p>方程组、矩阵、消元、向量空间、秩、解方程</p><p>还差P23、P27没学，等学完微分方程后回来看。</p><span id="more"></span><hr /><p>课程是<ahref="https://www.bilibili.com/video/BV16Z4y1U7oU/?spm_id_from=333.337.search-card.all.click&amp;vd_source=ca9a71bb3c1806ce48ae27d95e4e8bd0">链接</a></p><p>Gilbert Strang老爷子的MIT 18.06 LinearAlgebra。非常经典的一门课程，是理解性讲课而不是像国内多数高校式的应试性讲课。</p><p>线性代数作为诸多工程应用的基石，其重要性毋庸置疑。所以对于这份笔记，我的看法是常看常新，多看不亏，越多看，越能把全部知识串联起来。</p><p>在铺垫好线性代数的基石后，最好修一门常微分方程(mit18.03)。学有余力的话，把单变量微积分mit 18.01和多变量微积分mit18.02修了。如果想提高应用数学视野的可以去修GilbertStrang的计算科学与工程mit 18.085+18.086。</p><h3 id="一.-方程组的几何解释">一. 方程组的几何解释</h3><ul><li>考虑一个一元二次方程组，我们的几何理解就是一个方程是一条直线。这样的理解是初高中理解</li><li>但是到了大学，我们应该竖着去看方程组，也就是x乘一个列向量，y乘一个列向量，加起来，得到一个列向量，也就是这种形式：</li><li><span class="math inline">\(\begin{bmatrix}2 \\ -1\end{bmatrix}x +\begin{bmatrix}-1 \\ 2\end{bmatrix}y = \begin{bmatrix}0 \\3\end{bmatrix}\)</span></li><li>上面这种“竖着理解方程组的方式”，叫做“线性组合”，是贯穿这门课的一个思想</li></ul><hr /><ul><li><p>考虑线性组合的几何意义，假设有m个未知数，n个方程。</p></li><li><p>那么就有m个列向量。首先把这m个维度为<spanclass="math inline">\(\mathbb{R}^{n}\)</span>的列向量画到<spanclass="math inline">\(\mathbb{R}^n\)</span>上。</p></li><li><p>然后m个未知数就是这m个向量的系数，去线性组合这些向量，得到答案向量。</p></li><li><p>是不是很巧妙？在2维以上的空间内，用向量的线性组合去考虑问题会大大的简化问题。</p></li></ul><hr /><ul><li><p>为了不每次都写出上面那个<span class="math inline">\(x \cdot [2,-1]^\mathrm{T} + y \cdot [-1, 2]^\mathrm{T} = [0,3]\)</span>这种线性组合，太麻烦了，所以我们引入了矩阵来简化表达</p></li><li><p>具体来说，一个m个未知数，n个方程的方程组，用系数矩阵<spanclass="math inline">\(A\)</span>、未知数向量<spanclass="math inline">\(\textbf{x}\)</span>、答案向量<spanclass="math inline">\(b\)</span>来描述这个方程组</p></li><li><p>当你看到一个形如<span class="math inline">\(A\textbf{x} =b\)</span>的方程的时候，你要明白，本质就是m个维度为<spanclass="math inline">\(\mathbb{R}^n\)</span>的向量的线性组合 =常向量的求解问题</p></li></ul><hr /><ul><li><p>现在我们思考一个问题：n个方程，m个未知数的方程组是否永远有解？</p></li><li><p>用线性组合的观点就是，m个维度为<spanclass="math inline">\(\mathbb{R}^n\)</span>的向量的线性组合是否可以覆盖整个<spanclass="math inline">\(\mathbb{R}^n\)</span>空间？</p></li><li><p>可以发现，问题的关键，就是在这m个向量身上，换句话说，也就是在系数矩阵<spanclass="math inline">\(A\)</span>身上。这m个向量具有什么样的特点 / <spanclass="math inline">\(A\)</span>具有什么样的特点时，方程组会有解？会有几个解？这就是以后会讨论到的问题。</p></li></ul><hr /><ul><li>相信看到这，已经能感受到线性代数的绝妙吸引力了。它能带你在高维空间里遨游，让你熟练的玩弄高维空间。</li><li>数学真神奇，不是吗？</li></ul><h3 id="二.-矩阵消元">二. 矩阵消元</h3><ul><li><p>这节学习的是用消元法解方程组，计算机解方程都是用这种方法</p></li><li><p>首先先按国内大部分高校的讲法讲一遍：</p></li><li><p>消元就是初高中学的那个消元，消元前后矩阵是等价的，对系数矩阵进行求上三角过程</p></li><li><p>其实求上三角的过程，就是在依次确定基向量，基向量的意思就是能对解空间产生贡献的向量。假设向量俩俩正交，那么它们都是基向量。</p></li><li><p>对于n个方程，m个未知数的方程。若通过求上三角后，有k个主元（主元就是每列最后一个非零元素），说明有k个维度为<spanclass="math inline">\(\mathbb{R}^n\)</span>的基向量，那么若<spanclass="math inline">\(k \gen\)</span>，则方程组必定有解，因为此时m个向量可以线性组合出整个<spanclass="math inline">\(\mathbb{R}^n\)</span>空间。反之，则不一定有解。</p></li><li><p>换句话说，有几个主元，方程组的向量们就能线性组合出几维的空间</p></li></ul><hr /><ul><li><p>回到正题，如何求解方程组呢？（考虑一定有解的情况）</p></li><li><p>首先把答案向量<span class="math inline">\(b\)</span>加入到<spanclass="math inline">\(A\)</span>中作为新的一列，此时称<spanclass="math inline">\(A\)</span>为增光矩阵<spanclass="math inline">\(\overline{A}\)</span></p></li><li><p>对<spanclass="math inline">\(\overline{A}\)</span>消元求上三角，然后将消元后的矩阵重新写成方程组去算就行了</p></li></ul><hr /><ul><li><p>好了，现在用我在mit学到的讲法讲一遍：</p></li><li><p>上面的讲法中，对<spanclass="math inline">\(\overline{A}\)</span>消元求上三角的过程，我们的视角还是用初高中的做法去做的，但现在，仍然是消元求上三角的过程，我想用矩阵去做</p></li><li><p>在做之前，我想介绍“行的线性组合“</p></li></ul><hr /><ul><li><p>我们之前讲了，对于方程<spanclass="math inline">\(A\textbf{x}=b\)</span>，我们的理解方式就是看成m个列向量的线性组合，这其实是“列的线性组合“</p></li><li><p>现在我们来看这个方程：<spanclass="math inline">\(\textbf{x}^{\mathrm{T}}A =b^{\mathrm{T}}\)</span>，<spanclass="math inline">\(\textbf{x}^{\mathrm{T}}\)</span>是一个有n个未知数的行向量，<spanclass="math inline">\(A\)</span>仍然是一个<spanclass="math inline">\(\mathbb{R}^{n \times m}\)</span>的矩阵，<spanclass="math inline">\(b^{\mathrm{T}}\)</span>是一个常行向量。</p></li><li><p>此时我们需要把这个方程理解为“行的线性组合“，也就是<spanclass="math inline">\(A\)</span>的每一行就是一个向量，然后这些向量线性组合，系数就是<spanclass="math inline">\(x^{\mathrm{T}}\)</span>里的分量。</p></li></ul><hr /><ul><li>ok，回到对矩阵的消元。</li></ul><p>假设有一个矩阵：</p><p><span class="math display">\[\left[\begin{array}{c}1 &amp; 2 &amp; 1 \\3 &amp; 8 &amp; 1 \\0 &amp; 4 &amp; 1\end{array}\right]\]</span></p><p>首先我想用第一行把(2,1)消掉，那么第一行是不变的，第三行是不变的，第二行应该变为<spanclass="math inline">\([0, 2,-2]\)</span>，也就是第二行加上负三倍的第一行。首先第一行是不变的，利用“行的线性组合”思想，我们可以对<spanclass="math inline">\(A\)</span>左乘一个行向量：</p><p><span class="math display">\[[1, 0, 0]\left[\begin{array}{c}1 &amp; 2 &amp; 1 \\3 &amp; 8 &amp; 1 \\0 &amp; 4 &amp; 1\end{array}\right]=[1, 2, 1]\]</span></p><p>然后第三行也是不变的，所以我们继续左乘：</p><p><span class="math display">\[\left[\begin{array}{c}1 &amp; 0 &amp; 0 \\? &amp; ? &amp; ? \\0 &amp; 0 &amp; 1\end{array}\right]\left[\begin{array}{c}1 &amp; 2 &amp; 1 \\3 &amp; 8 &amp; 1 \\0 &amp; 4 &amp; 1\end{array}\right]=\left[\begin{array}{c}1 &amp; 2 &amp; 1 \\? &amp; ? &amp; ? \\0 &amp; 4 &amp; 1\end{array}\right]\]</span></p><p>显然对于结果矩阵的第二行，我们是想让<spanclass="math inline">\(A\)</span>的原第二行加上三倍负第一行的，所以线性组合就是(-3)* 第一行 + (1) * 第二行 + (0) *第三行，所以把系数填进左乘的矩阵，即可得到：</p><p><span class="math display">\[\left[\begin{array}{c}1 &amp; 0 &amp; 0 \\-3 &amp; 1 &amp; 0 \\0 &amp; 0 &amp; 1\end{array}\right]\left[\begin{array}{c}1 &amp; 2 &amp; 1 \\3 &amp; 8 &amp; 1 \\0 &amp; 4 &amp; 1\end{array}\right]=\left[\begin{array}{c}1 &amp; 2 &amp; 1 \\0 &amp; 2 &amp; -2 \\0 &amp; 4 &amp; 1\end{array}\right]\]</span></p><p>这里我们把左乘的这个矩阵记为<spanclass="math inline">\(E_{21}\)</span>（因为是想对(2,1)这个位置进行消除）。这种矩阵叫做初等矩阵</p><p>下一步做法依次类推：</p><p><span class="math display">\[\left[\begin{array}{c}1 &amp; 0 &amp; 0 \\0 &amp; 1 &amp; 0 \\0 &amp; -2 &amp; 1\end{array}\right]\left[\begin{array}{c}1 &amp; 2 &amp; 1 \\0 &amp; 2 &amp; -2 \\0 &amp; 4 &amp; 1\end{array}\right]=\left[\begin{array}{c}1 &amp; 2 &amp; 1 \\0 &amp; 2 &amp; -2 \\0 &amp; 0 &amp; 5\end{array}\right]\]</span></p><p>同样的，我们把左乘的这个矩阵叫做<spanclass="math inline">\(E_{32}\)</span>，因为是对位置(3, 2)消除</p><p>以上，就是用矩阵去描述消元的全过程。最后，用矩阵来做个大总结的话，就是：</p><p><span class="math inline">\(E_{32}E_{21}A = U\)</span></p><hr /><ul><li>这里再跑点题，多讲一下“初等矩阵”这个概念</li><li>前面的<span class="math inline">\(E_{21},E_{32}\)</span>本质上，就是对矩阵<spanclass="math inline">\(A\)</span>做了一次"操作"，也就是某行减掉了另一行的几倍。</li><li>所以我们把能"操作"矩阵的矩阵称为初等矩阵</li><li>除了某行减掉了另一行几倍，当然还有别的操作，例如交换两行。</li><li>很容易啊，一样用"行的线性组合"思想，假设要交换第一第二行，那么初等矩阵的第一行就是[0,1, ..., 0]，第二行就是[1, 0, ..., 0]</li><li>对于能交换矩阵行和列的矩阵，也是一种初等矩阵，我们记为<spanclass="math inline">\(P\)</span>（置换矩阵）</li></ul><h3 id="三.-矩阵乘法和逆矩阵">三. 矩阵乘法和逆矩阵</h3><ul><li><p>假设矩阵<span class="math inline">\(A\)</span>乘<spanclass="math inline">\(B\)</span>得到矩阵<spanclass="math inline">\(C\)</span>。考虑<spanclass="math inline">\(C\)</span>的某个元素<spanclass="math inline">\(c_{ij}\)</span>，我们都知道这个元素是由<spanclass="math inline">\(A\)</span>的第i行与<spanclass="math inline">\(B\)</span>的第j列做点乘得到的。</p></li><li><p>但是我们如果再用“行的线性组合”的思想，就可以知道，首先，<spanclass="math inline">\(C\)</span>的第i行是由<spanclass="math inline">\(B\)</span>的每一行线性组合得到的，系数是<spanclass="math inline">\(A\)</span>的第i行。那如果我只看<spanclass="math inline">\(C\)</span>第i行的第j个，那么也就是<spanclass="math inline">\(B\)</span>每一行的第j个（也就是<spanclass="math inline">\(B\)</span>的第j列）的线性组合，系数是<spanclass="math inline">\(A\)</span>的第i行。从这个角度来看，就清晰多了。</p></li><li><p>好的，我们前面讨论的矩阵都是方阵。但其实，矩阵相乘不一定是方阵。假设<spanclass="math inline">\(A\)</span>为<spanclass="math inline">\(\mathbb{R}^{m \times n}\)</span>, <spanclass="math inline">\(B\)</span>为<spanclass="math inline">\(\mathbb{R}^{n \times p}\)</span>, <spanclass="math inline">\(C\)</span>为多少呢？</p></li><li><p>通过前面“行的线性组合”思想可以推出来，首先<spanclass="math inline">\(A\)</span>有m行，那么<spanclass="math inline">\(C\)</span>一定有m行，<spanclass="math inline">\(C\)</span>的每i行是由<spanclass="math inline">\(B\)</span>的每一行线性组合得到的，系数是<spanclass="math inline">\(A\)</span>的第i行。所以<spanclass="math inline">\(C\)</span>每一行的维度由<spanclass="math inline">\(B\)</span>每一行的维度决定，所以<spanclass="math inline">\(C\)</span>的维度就是<spanclass="math inline">\(\mathbb{R}^{m \times p}\)</span></p></li><li><p>好的，还可以通过“列的线性组合”思想来推出来。首先<spanclass="math inline">\(C\)</span>的第i列是由<spanclass="math inline">\(A\)</span>的每一列的线性组合得到的，系数是<spanclass="math inline">\(B\)</span>的第i列。所以<spanclass="math inline">\(C\)</span>每一列的维度跟<spanclass="math inline">\(A\)</span>一样，有m个分量，然后因为<spanclass="math inline">\(B\)</span>有p列，所以<spanclass="math inline">\(C\)</span>也有p列，所以<spanclass="math inline">\(C\)</span>的维度就是<spanclass="math inline">\(\mathbb{R}^{m \times p}\)</span></p></li></ul><hr /><ul><li><p>但是，还有第三种方法去理解矩阵乘法。就是将矩阵乘法拆成若干个矩阵的加法。我们先考虑一个例子，一个列向量乘一个行向量，假设维度分别为<spanclass="math inline">\(\mathbb{R}^{m \times 1}, \mathbb{R} ^ {1 \timesn}\)</span>，那么显然结果是一个矩阵。这个矩阵的得到可以用“行线组”或者“列线组”去理解都行。</p></li><li><p>ok，那么接下里看这个例子：</p></li></ul><p><span class="math display">\[\left[\begin{array}{c}2 &amp; 7 \\3 &amp; 8 \\4 &amp; 9\end{array}\right]\left[\begin{array}{c}1 &amp; 6 \\0 &amp; 0\end{array}\right]\]</span></p><p>我可以把它看成：第一列乘第一行 + 第二列乘第二行</p><p><span class="math display">\[\left[\begin{array}{c}2 &amp; 7 \\3 &amp; 8 \\4 &amp; 9\end{array}\right]\left[\begin{array}{c}1 &amp; 6 \\0 &amp; 0\end{array}\right]=\left[\begin{array}{c}2 \\3 \\4\end{array}\right]\left[\begin{array}{c}1 &amp; 6\end{array}\right]+\left[\begin{array}{c}7 \\8 \\9\end{array}\right]\left[\begin{array}{c}0 &amp; 0\end{array}\right]\]</span></p><ul><li>好了，现在理解矩阵乘法就有至少三种方法了：行的线性组合、列的线性组合、拆为列向量与对应行向量相乘转为矩阵加法</li></ul><hr /><ul><li>其实还有一种理解的方法，就是分块矩阵，可以把俩相乘的矩阵分成对应的块，例如下图：</li></ul><p><img src="1.png" /></p><p>那么其实就可以把<span class="math inline">\(A_1, A_2, A_3, A_4, B_1,B_2, B_3, B_4\)</span>看作“元素”，那么就跟之前的三种理解方式一样了。</p><p>可以通过行线组思想来理解，那么<spanclass="math inline">\(C\)</span>的第一行就是<spanclass="math inline">\(A_1[B_1, B_2] + A_2[B_3,B_4]\)</span>，第二行就是<span class="math inline">\(A_3[B_1, B_2] +A_4[B_3, B_4]\)</span></p><hr /><ul><li><p>对于那些有逆的矩阵，我们称为可逆矩阵或者非奇异矩阵。</p></li><li><p>好消息是对于方阵<spanclass="math inline">\(A\)</span>，其左逆和右逆是一样的。对于非方阵则不是，因为维度都不同，对于非方阵的逆，称为“伪逆”，这个之后再谈</p></li><li><p>所以方阵到底有没有逆，就是一个很重要的问题。</p></li><li><p>先来讨论一下奇异矩阵，也就是没有逆的矩阵</p></li><li><p>对于一个方阵<span class="math inline">\(A \in \mathbb{R}^{n\times n}\)</span>，假设它的逆为<spanclass="math inline">\(B\)</span>，那么<spanclass="math inline">\(AB=E\)</span>，<spanclass="math inline">\(E\)</span>是一组<spanclass="math inline">\(\mathbb{R}^{n \timesn}\)</span>的基向量，换句话说，用“列的线性组合”思想思考，<spanclass="math inline">\(A\)</span>通过<spanclass="math inline">\(B\)</span>做线性变换后，能得到一组基向量，也就是说明<spanclass="math inline">\(A\)</span>的列向量们是俩俩线性无关的。（若存在线性有关的情况，则不可能线组出一组基向量，因为一组基向量就代表着空间内任意向量都可以线组出来）</p></li><li><p>所以，用几何的思想去思考，一个方阵<spanclass="math inline">\(A\)</span>是否可逆，取决于它的列向量们是否俩俩线性无关。若有关，则不可逆，若无关，则可逆。</p></li><li><p>那能进一步思考吗？其实从刚才的思考可以发现，只要<spanclass="math inline">\(A\)</span>能线性组合出<spanclass="math inline">\(\mathbb{R}^{n \timesn}\)</span>中的任意一个向量，那么<spanclass="math inline">\(A\)</span>就可逆，反之不行。</p></li><li><p>“线性无关”这个条件，可以从<span class="math inline">\(Ax =0\)</span>这个代数方程去思考。如果这个方程有非零解，即<spanclass="math inline">\(col_1 \cdot x_1 + col_2 \cdot x_2 + ... + col_n\cdot x_n = 0\)</span>，移项得到：<spanclass="math inline">\((-\frac{1}{x_n}) \cdot (col_1 \cdot x_1 + col_2\cdot x_2 + ...) = col_n\)</span>，（因为非零解，所以必然可保证<spanclass="math inline">\(x_n \ne0\)</span>），即这些列向量是线性有关的，那么就不可逆了。</p></li><li><p>关于这个结论的证明还可以用反证法，我们的结论是：若能找到<spanclass="math inline">\(x\)</span>不是非零解，使得<spanclass="math inline">\(Ax = 0\)</span>，则<spanclass="math inline">\(A\)</span>不可逆，反之可逆。好，那现在假设<spanclass="math inline">\(A\)</span>可逆，那么有<spanclass="math inline">\(A^{-1}A = E\)</span>，所以<spanclass="math inline">\(A^{-1}Ax = A^{-1}0\)</span>，则<spanclass="math inline">\(x = 0\)</span>，但是前面说了<spanclass="math inline">\(x\)</span>不是非零解，所以假设不成立。</p></li></ul><hr /><ul><li>那么知道一个矩阵有逆后，如何求呢？</li><li>使用Gauss-Jordan消元法。具体来说，假设你想求<spanclass="math inline">\(A\)</span>的逆。那么就写一个增光矩阵: <spanclass="math inline">\([A | I]\)</span>，然后把<spanclass="math inline">\(A\)</span>消元为<spanclass="math inline">\(I\)</span>，那么此时<spanclass="math inline">\(I\)</span>就会变为<spanclass="math inline">\(A^{-1}\)</span>，即<span class="math inline">\([A| I] \rightarrow [I | A^{-1}]\)</span></li><li>原理很简单，消元的过程还记得前面讲的吗，消元的本质就是对消元的矩阵乘“初等矩阵”，那么上面消元的过程我可以用下面这个式子表达：</li><li><span class="math inline">\(E_1E_2E_3...E_k[A | I] = E&#39;[A | I] =[I | E&#39;I]\)</span></li><li>因为<span class="math inline">\(E&#39;A=I\)</span>，所以<spanclass="math inline">\(E&#39;\)</span>是<spanclass="math inline">\(A^{-1}\)</span>，所以<spanclass="math inline">\(E&#39;I\)</span>是<spanclass="math inline">\(A^{-1}\)</span>。</li><li>所以<span class="math inline">\([A | I] \rightarrow [I |A^{-1}]\)</span></li></ul><h3 id="四.-矩阵a的lu分解">四. 矩阵A的LU分解</h3><ul><li>A的LU分解，L是下三角矩阵的意思，U是上三角矩阵的意思</li><li>那A的LU分解有什么用呢？</li><li>主要是拿来多次解方程组，后续讲完你就懂了。</li><li>先用初等矩阵把A消元一下，得到上三角矩阵U，例如：</li><li><span class="math inline">\(E_{21}E_{31}E_{32}A = U\)</span></li><li>然后同乘这些初等矩阵的逆，记为L：</li><li><span class="math inline">\(A = LU\)</span></li><li>即可把<spanclass="math inline">\(A\)</span>分解为下三角和上三角矩阵的乘积</li><li>好了，那么有什么用呢？</li><li>假设要你解<span class="math inline">\(Ax = b_1, Ax = b_2, ... Ax =b_n\)</span></li><li>第一种方法是都对每个方程都Gauss消元一次，每次复杂度都是<spanclass="math inline">\(\mathcal{O}(n^3)\)</span>。</li><li>第二次方法是求出<spanclass="math inline">\(A^{-1}\)</span>，然后对于不同的b，直接拿<spanclass="math inline">\(A^{-1}\)</span>与b相乘即可。这样会快很多。</li><li>第三种方法就是用A的LU分解，先分解得到LU，然后即L(Ux) = b</li><li>然后先解<span class="math inline">\(Ly = b\)</span>得到y，再解<spanclass="math inline">\(Ux = y\)</span>得到x。</li><li>由于L和U都是三角，所以解上述俩方程的复杂度都是<spanclass="math inline">\(\mathcal{O}(n^2)\)</span></li></ul><h3 id="五.-置换-转置-向量空间">五. 置换, 转置, 向量空间</h3><ul><li><p>先讲一下置换矩阵<span class="math inline">\(P\)</span></p></li><li><p>置换矩阵是初等矩阵的一种，意思就是交换行或者列的矩阵</p></li><li><p>比如<spanclass="math inline">\(P_{12}\)</span>，就是交换行1和行2的矩阵</p></li><li><p>思考一个问题，<spanclass="math inline">\(P_{ij}\)</span>的逆矩阵是谁？</p></li><li><p>容易知道，它的逆就是<spanclass="math inline">\(P_{ji}\)</span>，因为<spanclass="math inline">\(P_{ij}P_{ji} = E\)</span>。</p></li><li><p>所以，思考一下不难得出，对于置换矩阵<spanclass="math inline">\(P\)</span>，有<span class="math inline">\(P^{-1} =P^\mathrm{T}\)</span></p></li><li><p>题外话，<span class="math inline">\(n \timesn\)</span>的置换矩阵<spanclass="math inline">\(P\)</span>有多少种呢？</p></li><li><p>置换矩阵的本质就是规定了行的顺序，那么行有多少种排列顺序，就有多少种置换矩阵。所以维度为n的置换矩阵的形态有<spanclass="math inline">\(n!\)</span>种（全排列）</p></li></ul><hr /><ul><li>置换矩阵<span class="math inline">\(P\)</span>在上一节讲过的<spanclass="math inline">\(A=LU\)</span>分解中可以用到。因为在对<spanclass="math inline">\(A\)</span>求上三角<spanclass="math inline">\(U\)</span>的时候，可能会碰到主元为0的情况，这是我们不想看到的。所以在一开始，就应该把<spanclass="math inline">\(A\)</span>的行顺序给调配好，然后再开始进行LU分解。所以，上一节讲到的公式，更一般的应该写成：<spanclass="math inline">\(PA = LU\)</span></li></ul><hr /><ul><li><p>讲完置换矩阵，我要讲，转置</p></li><li><p>转置就是<spanclass="math inline">\(\mathrm{T}\)</span>，转置很简单，我想讲的是对称矩阵，就是满足<spanclass="math inline">\(A^{\mathrm{T}}=A\)</span>的矩阵</p></li><li><p>对称矩阵很常见，为什么说它常见呢？因为任意一个矩阵<spanclass="math inline">\(M\)</span>，与自身的转置<spanclass="math inline">\(M^{\mathrm{T}}\)</span>相乘，就可以得到一个对称矩阵<spanclass="math inline">\(MM^{\mathrm{T}}\)</span></p></li><li><p>证明一下：<spanclass="math inline">\((MM^{\mathrm{T}})^{\mathrm{T}} =MM^{\mathrm{T}}\)</span></p></li></ul><hr /><ul><li><p>下面来说一下向量空间</p></li><li><p>最常见的向量空间就是<spanclass="math inline">\(\mathbb{R}^n\)</span>，其中最常见的就是<spanclass="math inline">\(\mathbb{R}^2\)</span></p></li><li><p>向量空间我觉得跟群的概念有点像，本质就是一个封闭的集合。对于向量空间来说，空间里的向量任意线性组合之后必须仍然要在空间内，才能称为向量空间</p></li><li><p>前面说了<spanclass="math inline">\(\mathbb{R}^n\)</span>是最常见的向量空间，但其实，我们更关心包含在其中的空间，即子空间</p></li><li><p>就拿<span class="math inline">\(\mathbb{R}^2\)</span>举例，<spanclass="math inline">\(\mathbb{R}^2\)</span>的子空间有谁呢？</p></li><li><p>首先，自己肯定是自己的子空间，这很容易。</p></li><li><p>然后是直线，即穿过原点的任意直线，也是<spanclass="math inline">\(\mathbb{R}^2\)</span>的子空间</p></li><li><p>第三个就是一个点，零向量</p></li><li><p>总结一下，能构成向量空间的规则，就是向量空间里的向量任意线性组合之后仍然在向量空间内，这就是向量空间</p></li></ul><hr /><ul><li>接下来，我们谈论一下，如何通过矩阵来构造子空间</li><li>对于一个矩阵<spanclass="math inline">\(A\)</span>，假设其有n行m列，那么它的m个列向量线性组合出来的向量空间叫做矩阵<spanclass="math inline">\(A\)</span>的列空间，记作<spanclass="math inline">\(C(A)\)</span></li></ul><h3 id="六.-列空间和零空间">六. 列空间和零空间</h3><ul><li><p>若有子空间<span class="math inline">\(S\)</span>和<spanclass="math inline">\(T\)</span>，那么<span class="math inline">\(S\bigcap T\)</span>是不是子空间，答案显然是的。</p></li><li><p>从感性上，稍微思考一下可以很容易的理解</p></li><li><p>理性证明也很好证：</p></li></ul><blockquote><p>设<span class="math inline">\(v, w \in S \bigcap T\)</span>, 则</p><p><span class="math inline">\(v \in S, v \in T\)</span>; <spanclass="math inline">\(w \in S, w \in T\)</span></p><p>所以<span class="math inline">\(v, w\)</span>的线性组合既在<spanclass="math inline">\(S\)</span>，也在<spanclass="math inline">\(T\)</span>中</p><p>所以<span class="math inline">\(v, w\)</span>的线组在<spanclass="math inline">\(S \bigcap T\)</span>中</p><p>所以<span class="math inline">\(S \bigcap T\)</span>是一个子空间</p></blockquote><hr /><ul><li><p>好了，现在要把子空间的概念与方程的解联系起来</p></li><li><p>之前我们有讨论过<spanclass="math inline">\(Ax=b\)</span>何时有解的情况</p></li><li><p>之前我们说的是，若A的列向量们线组无法线组出来b，那么方程就是无解。</p></li><li><p>现在有了子空间这个概念，我们可以把上面那句话说的更专业一点：</p></li><li><p>若<span class="math inline">\(b \notinC(A)\)</span>，则无解（若b不在A的列空间内则无解）；反之有解</p></li></ul><hr /><ul><li><p>接下来介绍一下零空间的概念</p></li><li><p>对于<span class="math inline">\(Ax=b\)</span>这个方程，当<spanclass="math inline">\(b=0\)</span>的时候，其的解集称为<spanclass="math inline">\(A\)</span>的零空间，记为<spanclass="math inline">\(N(A)\)</span></p></li><li><p>需要注意的是，对于一个矩阵<span class="math inline">\(A \in\mathbb{R}^{n \times m}\)</span>，其列空间<spanclass="math inline">\(C(A)\)</span>是<spanclass="math inline">\(\mathbb{R}^{n}\)</span>维的（因为每个列向量有n个分量），而其零空间<spanclass="math inline">\(N(A)\)</span>是<spanclass="math inline">\(\mathbb{R}^m\)</span>维的（因为有m个列向量，所以有m个系数）</p></li></ul><hr /><ul><li><p>好了，前面介绍了零空间的概念。但是，我想请问，零空间一定是子空间吗？</p></li><li><p>答案：是的</p></li><li><p>证明过程很简单，如下：</p></li></ul><blockquote><p>if <span class="math inline">\(v, w \in N(A)\)</span>, i.e., <spanclass="math inline">\(Av=0, Aw=0\)</span></p><p>then <span class="math inline">\(A(k_1v + k_2w) = Ak_1v + Ak_2w =k_1(Av) + k_2(Aw) = 0\)</span></p><p>so <span class="math inline">\(k_1v + k_2w \in N(A)\)</span> for any<span class="math inline">\(v, w \in N(A)\)</span></p><p>so <span class="math inline">\(N(A)\)</span> is a subspace.</p></blockquote><h3 id="七.-求解ax0">七. 求解Ax=0</h3><ul><li><p>这节先介绍解<spanclass="math inline">\(Ax=0\)</span>的算法</p></li><li><p>先对<span class="math inline">\(A\)</span>消元</p></li><li><p>当然，可能出现主元为0的情况，不影响</p></li><li><p>如果所有主元都不为0，可以得到一个上三角矩阵，如果有主元为0，那么得到的将是一个阶梯型矩阵</p></li><li><p>主元的个数很重要，它有个名字：秩</p></li><li><p>OK，回到解<spanclass="math inline">\(Ax=0\)</span>的问题。假设现在我们通过消元得到了一个阶梯型矩阵<spanclass="math inline">\(U\)</span>，那么现在要解决的问题就是<spanclass="math inline">\(Ux=0\)</span>的解</p></li><li><p>我们把<spanclass="math inline">\(U\)</span>里阶梯的每个凸角那列叫做“主列”，其余列叫做“自由列”。（之所以叫自由列因为它们可以被其余的主列线组出来）</p></li><li><p>对于自由列对应的解，我们叫做“自由变量”。自由变量可以随便取，取完之后，就可以反代解出主列对应的解。从而可以得到一组解。</p></li><li><p>通常，假设有k个自由变量，那么我们会求k组特殊解，自由变量的取值就是枚举k个人，第i个人是1其余人是0，得到k组特殊解。</p></li><li><p>解空间<spanclass="math inline">\(N(A)\)</span>就是这k个特殊解的线性组合</p></li></ul><hr /><ul><li><p>好了，现在来总结一下上面的算法流程</p></li><li><p>首先对于一个维度为<span class="math inline">\(\mathbb{R}^{m\times n}\)</span>的矩阵<spanclass="math inline">\(A\)</span>，假设有<spanclass="math inline">\(r\)</span>个主元，也就是秩为<spanclass="math inline">\(r\)</span>。那么主列就有<spanclass="math inline">\(r\)</span>列，那么自由列就有<spanclass="math inline">\(n - r\)</span>列，那么就有<spanclass="math inline">\(n - r\)</span>个自由变量。</p></li><li><p>那么分别对这<span class="math inline">\(n -r\)</span>个自由变量取1其余取0，反代，就可以得到<spanclass="math inline">\(n - r\)</span>组特殊解。</p></li><li><p>这<span class="math inline">\(n -r\)</span>个特殊解的线性组合就是<spanclass="math inline">\(N(A)\)</span></p></li></ul><hr /><ul><li><p>如何理解上面的算法流程呢？</p></li><li><p>首先要知道，对于<span class="math inline">\(Ax=0, A \in\mathbb{R}^{m \timesn}\)</span>本质就是令n个列向量的线性组合为零向量。那么假设我找到一组解，那么这个解的倍数仍然是解。</p></li><li><p>ok，然后消元后我们可以知道哪些列是主列，哪些列是自由列，自由列的意思就是说它可以被别人线组出来。所以它对应的解（自由变量）就可以随便取。</p></li><li><p>那如何表示出所有的自由变量的取值呢？</p></li><li><p>答案：线性组合</p></li><li><p>假设有<span class="math inline">\(n -r\)</span>个自由变量，那么就搞<span class="math inline">\(n -r\)</span>次，每次就是其中一个自由变量为1，其余自由变量为0，得到<spanclass="math inline">\(n -r\)</span>组特解。（跟基向量的感觉比较像）</p></li><li><p>那么这<span class="math inline">\(n -r\)</span>组特解的线性组合就是解空间，即零空间<spanclass="math inline">\(N(A)\)</span></p></li></ul><hr /><ul><li><p>好了，接下来讲点好玩的东西</p></li><li><p>前面我们已经知道了<spanclass="math inline">\(Ax=0\)</span>的解法。</p></li><li><p>OK，现在我们再进一步思考，前面的算法仍然有回代这一步，这一步往往是计算机不喜欢的，能不能使算法更加“程序化”一些？</p></li><li><p>答案是可以的，假设我们已经通过消元得到了阶梯型矩阵<spanclass="math inline">\(U \in \mathbb{R}^{m \times n}\)</span></p></li><li><p>对于<spanclass="math inline">\(U\)</span>中的主列，将它主元的头上面全面消元为0。</p></li><li><p>然后把主列全挪到前面，后面放自由列。</p></li><li><p>这样，对于<spanclass="math inline">\(r\)</span>列主列，其实就得到了一个<spanclass="math inline">\(m \times r\)</span>的矩阵，这个矩阵上半部分是<spanclass="math inline">\(r \times r\)</span>的单位阵<spanclass="math inline">\(I\)</span>，下半部分是<spanclass="math inline">\((m - r) \times r\)</span>的全零矩阵。</p></li><li><p>然后对于<span class="math inline">\(n -r\)</span>列自由列，其实是一个<span class="math inline">\(m \times (n -r)\)</span>的矩阵，这个矩阵上半部分是<span class="math inline">\(r\times (n - r)\)</span>的矩阵<spanclass="math inline">\(F\)</span>，下半部分是<spanclass="math inline">\((m - r) \times (n -r)\)</span>的全零矩阵。</p></li><li><p>写出来的话，就是：</p></li></ul><p><span class="math display">\[\left[\begin{array}{c}I &amp; F \\O &amp; O\end{array}\right]\]</span></p><ul><li><p>把这个矩阵记为<spanclass="math inline">\(R\)</span>，那么现在问题就变成了<spanclass="math inline">\(Rx=0\)</span></p></li><li><p>那我更进一步，我想直接求出<span class="math inline">\(X \in n\times (n - r)\)</span>，<spanclass="math inline">\(X\)</span>中的每一列都是<spanclass="math inline">\(Rx=0\)</span>的一组特解。（其实就是求出<spanclass="math inline">\(n - r\)</span>组特解，然后拼一起得到的矩阵<spanclass="math inline">\(X\)</span>）</p></li><li><p>用公式写出来即</p></li></ul><p><span class="math display">\[\left[\begin{array}{c}I_{r \times r} &amp; F_{r \times (n - r)} \\O_{(m - r) \times r} &amp; O_{(m - r) \times (n - r)}\end{array}\right]X = O\]</span></p><ul><li>显然</li></ul><p><span class="math display">\[X=\left[\begin{array}{c}-F_{r \times (n - r)} \\I_{(n - r) \times (n - r)}\end{array}\right]\]</span></p><ul><li><p><span class="math inline">\(X\)</span>的列空间<spanclass="math inline">\(C(X)\)</span>就是<spanclass="math inline">\(Ax=0\)</span>的零空间<spanclass="math inline">\(N(A)\)</span></p></li><li><p>多看几遍上面的过程吧，非常优美的解法。</p></li></ul><h3 id="八.-求解axb">八. 求解Ax=b</h3><ul><li><p>上节的内容是求<spanclass="math inline">\(Ax=0\)</span>的零空间<spanclass="math inline">\(N(A)\)</span></p></li><li><p>这节的目标是讨论<spanclass="math inline">\(Ax=b\)</span></p></li><li><p>首先这个方程有可能有解，也有可能无解。如果有解的话，是否有多解，并求出所有解，这是本节要讨论的问题。</p></li><li><p>显然，通过前面所学，很容易可知，如果<span class="math inline">\(b\in N(A)\)</span>，那么方程<spanclass="math inline">\(Ax=b\)</span>就有解。</p></li><li><p>那有解的时候，如何求出所有解呢？</p></li><li><p>假设矩阵<span class="math inline">\(A \in \mathbb{R}^{m \timesn}\)</span>，秩为<span class="math inline">\(r\)</span>，那么就有<spanclass="math inline">\(n - r\)</span>个自由变量。令这<spanclass="math inline">\(n -r\)</span>个自由变量全取0，即可求出一组特解<spanclass="math inline">\(x_p\)</span>。</p></li><li><p>那么<span class="math inline">\(Ax = b\)</span>的解空间就是<spanclass="math inline">\(x_p + N(A)\)</span></p></li><li><p>注意，<span class="math inline">\(N(A)\)</span>就是<spanclass="math inline">\(Ax=0\)</span>的解空间，它是一个向量空间</p></li><li><p>但是<span class="math inline">\(x_p +N(A)\)</span>就不一定是一个向量空间了，因为它可能不过零向量</p></li><li><p>总之，<span class="math inline">\(Ax=0\)</span>的解空间是<spanclass="math inline">\(N(A)\)</span>，<spanclass="math inline">\(Ax=b\)</span>的解空间是<spanclass="math inline">\(x_p + N(A)\)</span>，<spanclass="math inline">\(x_p\)</span>是自由变量全取0时算出来的一组特解</p></li></ul><hr /><ul><li><p>到现在，其实你已经学会了解<span class="math inline">\(Ax=0,Ax=b\)</span>了。</p></li><li><p>回顾一下，首先是<spanclass="math inline">\(Ax=0\)</span>，先消元，得到秩r，如果r =n，那么就没有自由变量了， 那么<spanclass="math inline">\(N(A)\)</span>里只有零向量。</p></li><li><p>如果r &lt; n，那么就有n - r个自由变量，那么就可以求出n -r组特解，这n - r组特解的线性组合就是<spanclass="math inline">\(N(A)\)</span></p></li><li><p>再来回顾<spanclass="math inline">\(Ax=b\)</span>，先消元，得到秩r，如果r =n，那么就没有自由变量了，那么<spanclass="math inline">\(N(A)\)</span>里只有零向量。然后看看<spanclass="math inline">\(Ax=b\)</span>是否有特解<spanclass="math inline">\(x_p\)</span>，有的话，那么<spanclass="math inline">\(Ax=b\)</span>的解集就只有<spanclass="math inline">\(x_p\)</span>了。如果没有，那么<spanclass="math inline">\(Ax=b\)</span>就没解。</p></li><li><p>如果r &lt; n，那么可以就可以先把<spanclass="math inline">\(N(A)\)</span>求出来。然后求<spanclass="math inline">\(Ax=b\)</span>的特解<spanclass="math inline">\(x_p\)</span>，然后<span class="math inline">\(x_p+ N(A)\)</span>就是<spanclass="math inline">\(Ax=b\)</span>的解集。</p></li></ul><hr /><ul><li>OK，现在再从秩的角度来思考这个问题（<span class="math inline">\(A\in \mathbb{R}^{m \times n}\)</span>）</li></ul><ol type="1"><li>r = m = n<ul><li>此时是方阵，且<spanclass="math inline">\(A\)</span>消元后是单位阵，所以肯定有且只有唯一解</li><li>另一个角度，满秩的方阵是可逆矩阵，所以<span class="math inline">\(x= bA^{-1}\)</span>，从这个角度也可以证明有且只有唯一解</li></ul></li><li>r = n &lt; m<ul><li>此时<span class="math inline">\(A\)</span>消元之后可得到 $ $</li><li>因为n - r = 0，所以没有自由变量，所以<spanclass="math inline">\(N(A)\)</span>里只有零向量。所以<spanclass="math inline">\(Ax=b\)</span>要不没解，要不只有唯一解（就是特解）</li></ul></li><li>r = m &lt; n<ul><li>此时<span class="math inline">\(A\)</span>消元之后可得到 $ $</li><li>此时n - r &gt; 0，所以有自由变量，所以<spanclass="math inline">\(N(A)\)</span>里是有无限多向量的。所以只要<spanclass="math inline">\(Ax=b\)</span>有特解<spanclass="math inline">\(x_p\)</span>，那么<spanclass="math inline">\(Ax=b\)</span>就有无穷多解了。</li><li>显然，<spanclass="math inline">\(Ax=b\)</span>可以找到特解，因为消元之后没有出现全为0的行，所以肯定能凑出一组解</li><li>所以这种情况下，方程<spanclass="math inline">\(Ax=b\)</span>有无穷多组解。</li></ul></li><li>r &lt; m, r &lt; n<ul><li>此时<span class="math inline">\(A\)</span>消元之后可得到 $ $</li><li>此时n - r &gt; 0，所以有自由变量，所以<spanclass="math inline">\(N(A)\)</span>里是有无限多向量的。所以只要<spanclass="math inline">\(Ax=b\)</span>有特解<spanclass="math inline">\(x_p\)</span>，那么<spanclass="math inline">\(Ax=b\)</span>就有无穷多解了。</li><li>但是，这里化简之后出现了全0行，所以用增广矩阵去看全0行的那几行b方程可能无法满足。</li><li>所以，如果能满足的话，就是无穷多组解。如果无法满足的话，那么就没有解</li></ul></li></ol><ul><li>用一句话总结，矩阵的秩说明了方程解的情况。</li></ul><h3 id="九.-线性相关-基-维数">九. 线性相关, 基, 维数</h3><ul><li><p>线性相关的标准定义就是，如果一组向量能线性组合出零向量（系数不能全为0），那么这组向量就线性有关；反之线性无关</p></li><li><p>向量空间的一组基，是指这么一组向量，这组向量满足两个性质：</p><ol type="1"><li>向量组线性无关</li><li>它们恰好能生成整个空间，少一个不行</li></ol></li><li><p>对于一个向量空间，它的基有很多。但是，所有的基中的向量的个数都相同，这个数量称为该向量空间的维数</p></li><li><p><del>所以知道一个向量空间的维数很重要，假设知道了维数dim =k，还知道向量的维度，那么只需要找k个线性无关的该维度的向量即是这个向量空间的一组基。</del>（upd：这句话是错误的！！！）</p></li><li><p>错误原因是因为当时我没意识到行变换会改变列空间。我举一个反例：</p></li></ul><p><span class="math display">\[\left[\begin{array}{c}1 &amp; 0 \\0 &amp; 1 \\0 &amp; 0 \\\end{array}\right]\]</span></p><ul><li><p>这组列向量产生的向量空间是三维空间里的二维过原点的水平平面。</p></li><li><p>ok，如果上面那句话是对的。那么看下面这组由上面那组向量行变换的哀悼的列向量：</p></li></ul><p><span class="math display">\[\left[\begin{array}{c}1 &amp; 1 \\0 &amp; 1 \\2 &amp; 2 \\\end{array}\right]\]</span></p><ul><li><p>俩列向量线性无关，但是形成的向量空间显然不是一个水平的平面。故上面那句话是错的。</p></li><li><p>行变换不会改变列向量的线性关系，所以求向量空间的基的时候，可以随便用行变换。这为下面<spanclass="math inline">\(\mathrm{dim}C(A) = \mathrm{dim}C(R) =r\)</span>的结论做了铺垫。</p></li><li><p>但是行变换会改变列空间，所以求向量空间的时候，一定要注意，求的是列空间还是行空间，如果是列空间的话，就要想到行变换会改变列空间这个坑点。</p></li></ul><blockquote><p>为什么行变换不会改变列向量的线性关系？这里给出证明：</p><p>考虑<span class="math inline">\(k_1b_1 + k_2b_2 + ... + k_nb_n =0\)</span></p><p>做一次行变换后，假设<spanclass="math inline">\(b_i\)</span>行变成了<spanclass="math inline">\(b_i +cb_j\)</span>，那么列出n列的线性组合表达式，还是能整理为<spanclass="math inline">\(k_1b_1 + k_2b_2 + ... + k_nb_n =0\)</span>的形式。</p><p>所以变换前后，俩矩阵的列向量的线性组合可以化为同一种形式，所以线性关系是相同的</p><p>（上面这个证法是自己想的，若错误或者有更好的方法欢迎讨论哇）</p></blockquote><hr /><ul><li>现在，让我们把基、维数的概念用到矩阵<spanclass="math inline">\(A\)</span>中</li><li>首先，对于矩阵<span class="math inline">\(A\)</span>的列空间<spanclass="math inline">\(C(A)\)</span>，它的基是啥？维数是多少？</li><li>很显然，<spanclass="math inline">\(A\)</span>消元后可知道秩r，表示的是主列的个数，这个秩其实就是列空间<spanclass="math inline">\(C(A)\)</span>的维数<spanclass="math inline">\(\mathrm{dim}C(A) = r\)</span></li><li><span class="math inline">\(A\)</span>的主列们就是<spanclass="math inline">\(C(A)\)</span>的一组基（注意这里我说的是<spanclass="math inline">\(A\)</span>的主列们而不是<spanclass="math inline">\(A\)</span>经过行变换后<spanclass="math inline">\(R\)</span>的主列们）</li><li>OK，那对于<span class="math inline">\(A\)</span>的零空间<spanclass="math inline">\(N(A)\)</span>呢？它的基是啥？维数是多少？</li><li>回顾求解<span class="math inline">\(N(A)\)</span>的过程，就是找n -r组特解（有n - r个自由变量）。所以n - r就是<spanclass="math inline">\(N(A)\)</span>的维数<spanclass="math inline">\(\mathrm{dim}N(A) = n - r\)</span>。这n -r组特解就是<span class="math inline">\(N(A)\)</span>的一组基。</li></ul><h3 id="十.-四个基本子空间">十. 四个基本子空间</h3><ul><li><p>四个基本子空间是：列空间、零空间、行空间、左零空间</p></li><li><p>列空间老朋友了，<spanclass="math inline">\(C(A)\)</span></p></li><li><p>零空间也是老朋友了，<spanclass="math inline">\(N(A)\)</span></p></li><li><p>行空间其实可以写成这样，<spanclass="math inline">\(C(A^{\mathrm{T}})\)</span></p></li><li><p>左零空间其实就是，<spanclass="math inline">\(N(A^{\mathrm{T}})\)</span></p></li><li><p>为什么要叫左零空间呢？其实是这样的，<spanclass="math inline">\(A^{\mathrm{T}}y = 0\)</span>，转置，得到，<spanclass="math inline">\(y^{\mathrm{T}}A = 0^{\mathrm{T}}\)</span></p></li><li><p>这里的解在左边，所以就叫左零空间</p></li></ul><hr /><ul><li><p>现在我们来讨论一下这四个空间的维数dim和基</p></li><li><p>首先是列空间，列空间的维数是r。基是多少呢？</p></li><li><p>这里我要强调一点，<span class="math inline">\(C(A) \neC(R)\)</span>（<span class="math inline">\(R\)</span>是<spanclass="math inline">\(A\)</span>经过行变换得到的）</p></li><li><p>因为做行变换会改变列空间，但不会改变行空间</p></li><li><p>但为什么做行变换之后还能求解呢？因为你在做行变换（高斯消元）的过程的时候，是对增广矩阵做的。所以<spanclass="math inline">\(Ax=b\)</span>与<spanclass="math inline">\(Rx=b&#39;\)</span>是等价的，而不是<spanclass="math inline">\(Ax=b\)</span>与<spanclass="math inline">\(Rx=b\)</span>是等价的。</p></li><li><p>好了，所以<spanclass="math inline">\(C(A)\)</span>的维数是r，基是<spanclass="math inline">\(A\)</span>的主列们</p></li><li><p>然后讨论零空间<spanclass="math inline">\(N(A)\)</span>，零空间关心的是解集，所以不用关心行变换会影响到<spanclass="math inline">\(N(A)\)</span>。所以<spanclass="math inline">\(N(A)\)</span>的维数是n - r，基就是n -r组特解</p></li><li><p>接下来讨论行空间<spanclass="math inline">\(C(A^\mathrm{T})\)</span>，它的维数是r，基呢？</p></li><li><p>其实直接对<span class="math inline">\(A\)</span>做消元得到<spanclass="math inline">\(R\)</span>，<spanclass="math inline">\(R\)</span>的主行们就是<spanclass="math inline">\(C(A^\mathrm{T})\)</span>。因为行变换不会改变行空间，所以<spanclass="math inline">\(A\)</span>与<spanclass="math inline">\(R\)</span>的行空间是相同的。</p></li><li><p>最后来讨论左零空间<spanclass="math inline">\(N(A^\mathrm{T})\)</span>。左零空间的维数是m -r很显然，那么基呢？最简单的方法就是对<spanclass="math inline">\(A^\mathrm{T}\)</span>消元，然后m -r组特解就是左零向量的一组基。</p></li></ul><h3 id="十一.-矩阵空间">十一. 矩阵空间</h3><p>既然有向量空间，那么其实也有矩阵空间。其实任意东西都可以抽象成“向量空间”。</p><p>想象一个以3X3矩阵构成的空间M为例。你从M中任挑俩矩阵，相加或者做数乘，发现仍然得到3X3矩阵，所以这是一个矩阵空间。</p><p>这个矩阵空间还有一些有意思的子空间，比如3X3对称矩阵这个子空间、3X3的上三角矩阵这个子空间。</p><p>显然，M的一组基是9个矩阵，所以M的维数是9（dimM=9）。</p><p>记3X3对称矩阵构成的空间为S，那么显然S的一组基是6个矩阵，dimS=6。</p><p>记3X3上三角矩阵构成的空间为U，那么那么显然U的一组基也是6个矩阵，dimY=6。</p><p>考虑<span class="math inline">\(S \capU\)</span>，一个矩阵即是对称的又是上三角的，那么它就是对角的。所以<spanclass="math inline">\(S \capU\)</span>表示的是3X3对角矩阵这个子空间。显然，<spanclass="math inline">\(\mathrm{dim}(S \cap U)=3\)</span>。</p><p>好，现在考虑一下<span class="math inline">\(S \cup U\)</span>和<spanclass="math inline">\(S +U\)</span>的区别。只要一个矩阵是对称的，或者上三角的，那么它就属于<spanclass="math inline">\(S \cupU\)</span>，但是在这个空间对加法不封闭，所以<spanclass="math inline">\(S \cup U\)</span>不是一个子空间；</p><p><span class="math inline">\(S +U\)</span>中的每一个矩阵都可以i表示为<spanclass="math inline">\(S\)</span>中的一个矩阵加上<spanclass="math inline">\(U\)</span>中的一个矩阵。所以<spanclass="math inline">\(S + U\)</span>是对加法和数乘封闭的，所以<spanclass="math inline">\(S + U\)</span>是一个子空间。另外，当在<spanclass="math inline">\(S\)</span>中任取时，<spanclass="math inline">\(U\)</span>中取零矩阵时，得到的就是<spanclass="math inline">\(S\)</span>。同理，在<spanclass="math inline">\(U\)</span>中任取时，<spanclass="math inline">\(S\)</span>中取零矩阵时， 得到的就是<spanclass="math inline">\(U\)</span>。所以<span class="math inline">\(S +U\)</span>是包含<span class="math inline">\(S \cupU\)</span>的，换句话说，<span class="math inline">\((S \cap U) \subseteq(S \cup U) \subseteq (S + U)\)</span>。</p><p>进一步想想，<span class="math inline">\(S +U\)</span>是什么，其实它就是<spanclass="math inline">\(M\)</span>这个空间。所以显然<spanclass="math inline">\(dim(S + U) = dim(M) = 9\)</span>。</p><p>到这里，我们可以发现一个式子：<span class="math inline">\(dim(S) +dim(U) = dim(S + U) + dim(S \cap U)\)</span></p><p>这不是碰巧，这确实是一个定理。</p><p>所以总结一下，若有向量空间<span class="math inline">\(S,U\)</span>，则<span class="math inline">\(S \cap U\)</span>和<spanclass="math inline">\(S + U\)</span>也是向量空间，但<spanclass="math inline">\(S \cup U\)</span>不是。而且满足：<spanclass="math inline">\(dim(S) + dim(U) = dim(S + U) + dim(S \capU)\)</span></p><hr /><p>下面来一个有趣的例题，假设列向量<span class="math inline">\(v \in\mathbb{R}^4\)</span>，且满足其四个分量之和为0。那么<spanclass="math inline">\(v\)</span>是不是一个向量空间？如果是的话，基和维数是什么？</p><p>首先，在v中任取俩v1,v2，做加法和数乘仍在v中，所以v是一个向量空间。</p><p>然后它的基和维数是多少呢？</p><p>因为v不是一个传统的给定数值的矩阵，所以它的秩不好求。所以这里需要转化思维，如果把v看作是某个矩阵A的零空间，那么只需求出<spanclass="math inline">\(dim(N(A))\)</span>就是v的基，同样，零空间的一组基就是n- r个特解。</p><p>思考后不难发现，<span class="math inline">\(A = [1, 1, 1,1]\)</span>，此时有<span class="math inline">\(Av = 0\)</span>，<spanclass="math inline">\(N(A) = v\)</span>。</p><p>显然对于矩阵<span class="math inline">\(A\)</span>，秩为1，那么<spanclass="math inline">\(dim(N(A)) = dim(v) = n - r = 3\)</span></p><p>主列为第一列，所以自由变量为后三个 ，所以分别可得出特解：<spanclass="math inline">\([-1, 1, 0, 0]^\mathrm{T}, [-1, 0, 1,0]^\mathrm{T}, [-1, 0, 0, 1]^\mathrm{T}]\)</span>。这三个向量就是<spanclass="math inline">\(v\)</span>向量空间的一组基。</p><p>这种解法非常的巧妙，既然正着不好求，就把其转换为矩阵的零空间，从而得到它的空间性质。</p><h3 id="十二.-图和网络">十二. 图和网络</h3><p>本小节不涉及新的线性代数的知识，而是对于实际问题建模，用线性代数去解决，具有启发意义的一节。</p><p>这篇<ahref="https://rqtn.github.io/mit-18.06/mit-18.06-lec12/">博文</a>写的不错</p><h3 id="十三.-复习课一">十三. 复习课一</h3><p><ahref="https://www.bilibili.com/video/BV16Z4y1U7oU?p=13&amp;vd_source=ca9a71bb3c1806ce48ae27d95e4e8bd0">链接</a>：直接去听，如果都掌握了的话，全部内容是都可以听懂的。</p><p>如果听不懂，说明前面的知识没掌握牢固，建议回到对应的位置重新温习后再来听这堂课。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;方程组、矩阵、消元、向量空间、秩、解方程&lt;/p&gt;
&lt;p&gt;还差P23、P27没学，等学完微分方程后回来看。&lt;/p&gt;</summary>
    
    
    
    <category term="4. 大学" scheme="http://error666.top/categories/4-%E5%A4%A7%E5%AD%A6/"/>
    
    <category term="数学" scheme="http://error666.top/categories/4-%E5%A4%A7%E5%AD%A6/%E6%95%B0%E5%AD%A6/"/>
    
    <category term="线性代数" scheme="http://error666.top/categories/4-%E5%A4%A7%E5%AD%A6/%E6%95%B0%E5%AD%A6/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>2024数学建模国赛游记</title>
    <link href="http://error666.top/2024/09/09/2024%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E5%9B%BD%E8%B5%9B%E6%B8%B8%E8%AE%B0/"/>
    <id>http://error666.top/2024/09/09/2024%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E5%9B%BD%E8%B5%9B%E6%B8%B8%E8%AE%B0/</id>
    <published>2024-09-08T16:18:30.000Z</published>
    <updated>2025-02-25T13:26:35.470Z</updated>
    
    <content type="html"><![CDATA[<p>2024年数学建模国赛游记</p><span id="more"></span><p>upd 2024/11/2：喜提国一。</p><hr /><p>其实也没啥好写的，总结就是大家尽力，队友给力，传奇论文手、天使建模手。</p><p>打数模这事也是和舍友偶然一次聊天，了解到他打数模，没找到队友，于是我说可以一起打啊，便这样组好了队（两个建模手）。后来，他又拉到我们班一同班同学（传奇论文手）。于是三人队就此形成。</p><p>在正赛之前我们进行过两次正式训练，一次校赛，一次自己打的训练赛。其余日常的小训练就不说了。说来惭愧，日常的数模学习我其实并没有学非常多数模知识，大多数只是泛泛而谈，即了解一下概念，学习一下原理就过了。所以日常训练中其实并没有学到成体系的知识，只是知道了许多名词，以及看了一些国奖论文，知道国奖的论文大概是如何包装的。</p><p>校赛是我们第一次正式训练，总的来说，我觉得作为我们第一次正式写论文的经历比赛，效果还是很不错的。虽然从结果上来说，最终只是个校赛二等奖，建模也自我感觉建的很普通，但是至少我们做出了一份可以看的过去的成品。</p><p>然后就到训练赛了。讲实话训练赛我没有怎么参与，因为当时时间和论文ddl撞了。所以我负责的部分写的很垃圾（自我感觉）。最终，虽然论文的编排有进步（传奇论文手还在进步），但建模效果我认为还不如校赛。</p><p>ok到国赛了。国赛大家从一开始就很上心，特地申请了一间小房子，3天比赛时间几乎全天泡在里头搞数模。第一天晚上我们主要把B题浏览并分析了个大概，把前三问的模型搭建了一个最初步的模型，然后分配了下任务，我负责（1）（4）问，另一个建模手负责（2）（3）问，然后就睡觉了。</p><p>第二天，仔细思考后我们觉得建的模型不对，于是反复思考后在之前模型基础上，进行了大量修改，最终有了全部问题的思路。这中间的过程非常复杂，尤其另一个建模手的（2）（3）问，数学推导十分严谨，效果很好。</p><p>这一天我把第一问模型也建好了，用了俩方法去解决第一问，反正就ChatGPT辅助呗。他给你思路，你理解消化后修正它思路，他再给你思路，你再修正......反反复复，最终建立好了（1）问的模型，并写出了代码。</p><p>第三天，我花了点时间把问题（4）的模型也建完了。但是觉得不够高级，于是加了点trick加速模型求解速度。我的建模任务到此基本结束。另一位建模手负责的（2）（3）问在这一天也修正了一点点小问题，建模也基础结束。论文手开始进入除（1）问的论文编写，最终论文手加班到凌晨4点，我们的论文基本成型（太敬业了，给队友点赞）。</p><p>最后一天，把摘要写了，然后缝缝补补修修论文，晚上就交了。</p><hr /><p>国赛经历其实平平淡淡，按部就班。但是我们三个人是非常尽力的，所以最终的效果我们都比较满意。无论结果如何，至少我们交出了一份问心无愧的答案。</p><p>结果留个坑，到时候出成绩再更新... ...</p><p>为什么没写详细的思路过程？我觉得没啥必要，游记嘛本来就随便写写。</p><p>大概讲讲吧，思路第一问俩方法。法一直接暴力迭代求解，法二序贯检验。第二三问，建立完备的数学期望模型。第四问用自适应蒙特卡罗求解。</p><p>详细的思路也不适合在游记写，有兴趣的到时候去看我github仓库里的论文吧，有什么问题欢迎大家一起交流学习呀。</p><hr /><p>其实，数模比赛我认为是一个“成分复杂“的比赛。你说它水，但是它还真需要一些逻辑和思考才能建出模型和写出代码；你说它严谨，其实部分论文都是瞎编甚至造假的，写论文的人自己都不知道自己在干啥......总之，我对数学建模比赛持中立态度。我的建议是，如果是为了培养写论文的能力以及快速学习能力，那欢迎你参加数模。如果是为了培养所谓数学思维和提高专业水平能力的，打数模浪费时间，不如去搞正儿八经的科研。</p><hr /><p>最后，祝大家身体健康，学业顺利！</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;2024年数学建模国赛游记&lt;/p&gt;</summary>
    
    
    
    <category term="3. 竞赛" scheme="http://error666.top/categories/3-%E7%AB%9E%E8%B5%9B/"/>
    
    
  </entry>
  
  <entry>
    <title>transformers、llama3学习</title>
    <link href="http://error666.top/2024/07/20/transformers%E3%80%81llama3%E5%AD%A6%E4%B9%A0/"/>
    <id>http://error666.top/2024/07/20/transformers%E3%80%81llama3%E5%AD%A6%E4%B9%A0/</id>
    <published>2024-07-19T18:26:55.000Z</published>
    <updated>2024-10-04T14:36:06.061Z</updated>
    
    <content type="html"><![CDATA[<p>最近有个合作idea，需要魔改llama3代码，所以来学习下transformers和llama3</p><p>upd：已经变为我的知识库大杂烩了，将就着看吧，纯粹个人笔记了。</p><span id="more"></span><h2 id="先验知识">先验知识</h2><h3 id="一.-预处理">一. 预处理</h3><ol type="1"><li><p>分词</p><ul><li>Text -&gt; tokenizer -&gt; input_ids</li><li>Text是文本，tokenizer是分词器，将text转成一个个token，然后通过词汇表将token映射到整数id上，得到input_ids</li></ul></li><li><p>embedding</p><ul><li>将整数id映射为一个向量的。目的是为了丰富其蕴含的信息，意思相近的token的向量在距离上也会彼此接近</li></ul></li><li><p>位置编码</p><ul><li><p>为什么需要位置编码？</p></li><li><p>因为“猫在椅子上”和“椅子在猫上”意思完全不同。位置编码就是告诉模型每个token在句子中的位置，这样模型就可以理解单词的顺序。</p></li><li><p>假设有“I love machinelearning.“，将其切为token后且embedding后，得到的向量如下：</p></li><li><p>I -&gt; [0.1, 0.2, 0.3, 0.4]</p></li><li><p>love -&gt; [0.5, 0.6, 0.7,. 0.8]</p></li><li><p>...</p></li><li><p>最简单的位置编码方式就是token在句子中出现的位置下标为1，其余分量为0的向量。即I 的位置向量为[1, 0, 0, 0]，love的位置向量为[0, 1, 0, 0]</p></li><li><p>然后将embedding vector与positionvector相加，得到的向量就不仅有词义信息，还蕴含了位置信息。</p></li></ul></li></ol><h3 id="二.-编码器层encoder-layer">二. 编码器层(Encoder Layer)</h3><p>编码器是由多个编码器层堆叠而成。编码器用于处理输入序列，生成上下文敏感的表示。</p><ol type="1"><li><p>自注意力机制</p><ul><li>自注意机制让每个单词能够关注句子中的其他单词，从而理解上下文</li><li>具体来说，首先会有三个权重矩阵：<spanclass="math inline">\(W_Q\)</span>（查询权重）、<spanclass="math inline">\(W_K\)</span>（键权重）、<spanclass="math inline">\(W_V\)</span>（值权重）</li><li>然后对于每个进来的vector x，都会分别与这三个矩阵相乘，每个vectorx可得到<span class="math inline">\(x_Q\)</span>（查询向量）、<spanclass="math inline">\(x_K\)</span>（键向量）、<spanclass="math inline">\(x_V\)</span>（值向量）三个向量。</li><li>查询向量<spanclass="math inline">\(x_Q\)</span>：可以理解为每个词在关注其他词提出的问题</li><li>键向量<spanclass="math inline">\(x_K\)</span>：可以理解为每个词的特征表示，用来与查询向量匹配。例如你自己有个键向量，然后另一个人有个查询向量。发现你们的这俩向量向量比较接近，说明他查到了你，那么你的值向量就会返回给他</li><li>值向量<spanclass="math inline">\(x_V\)</span>：可以理解为token实际的内容信息</li><li>下面举个实际的例子：</li><li>对于句子“I loveNLP“，对于I，可以计算出查询、键、值向量。对于love、NLP同理。</li><li>那么对于I，计算它的查询向量与其余token的键向量的点积，将这些点积用softmax归一化，得到的一组权重就是I与其它token之间的联系权重（联系越大，权重越大，所有权重之和为1）。然后分别用对应的权重乘上对应token的值向量，然后求和，得到的向量叫做I 的注意力输出。（即 I 在关注了句子中其余token后，得出的一个向量）<ul><li>softmax：将一组向量转换为一个概率分布向量，全部分量之和为</li><li>对于<span class="math inline">\(x_i\)</span>，其softmax后的值为<spanclass="math inline">\(\frac{e^{x_i}}{\sum_{j=1}^n e^{x_j}}\)</span></li><li>softmax的特性是放大差异，较大的输入值对应的输出概率更高，较小的输入值对应的输出概率更小</li></ul></li></ul></li><li><p>多头注意力机制</p><ul><li><p>上面讲了注意力机制，其中提到了三个权重矩阵，这三个权重矩阵我们叫做“注意力头”</p></li><li><p>那么前面说的是，对于一个vectorx，通过一个注意力头，可以得到一个注意力输出</p></li><li><p>那为了让一个token能捕捉到更多信息，我们可以对于一个vectorx使用多个注意力头（也就是多个不同的权重矩阵），得到多个注意力输出，这就是多头注意力机制</p></li><li><p>前面说到，注意力输出是一个向量，表示该token关注句子中其余token后得出的信息。那么多头注意力机制会得到多个注意力输出</p></li><li><p>将多个注意力输出简单拼接在一起，然后通过一个Linear变换再把数据映射回原始token的shape</p></li></ul></li><li><p>层归一化 &amp; 残差连接</p><ul><li><p>层归一化有助于消除梯度消失和梯度爆炸问题，使得梯度能够更稳定地传播到前面的层。这种稳定性加快了模型的收敛速度，使得模型能够更快地达到较优的性能。</p></li><li><p>具体操作就是对输入向量进行标准化，使其具有零均值和单位方差，有助于加快训练速度并稳定模型性能（就是对一个向量进行归一化，就这么简单）</p></li><li><p>残差连接就更简单了，将归一化后的向量与输入进来的向量做加法，得到的向量就是输出。</p><ul><li>为什么要使用残差连接？<ul><li>在深层神经网络中，随着层数的增加，梯度消失和梯度爆炸的问题变得越来越严重。残差连接为梯度提供了直接路径，使得梯度可以更顺畅地反向传播，缓解了这些问题。</li><li>残差连接确保输入信息在深层网络中不会丢失，保持了输入的原始特征。这有助于模型在学习新的特征时，不会遗忘前面层已经学习到的重要信息。</li></ul></li><li>关于梯度消失<ul><li>梯度消失的主要原因是激活函数的选择和链式法则的计算。常见的激活函数（如Sigmoid和Tanh）在其取值范围的两端会趋近于零，这会导致其导数也趋近于零。当使用这些激活函数时，梯度在反向传播过程中会不断地乘以这些小于1的数值，从而逐渐衰减为接近零的值。</li><li>梯度爆炸问题是指在训练深层神经网络时，梯度在反向传播过程中逐渐变大，最终变得非常大。这会导致前面层的权重更新幅度过大，从而使得网络无法稳定训练，甚至导致数值溢出。</li></ul></li></ul></li></ul></li><li><p>前馈神经网络层（FNN, feedforward neural network）</p><ul><li><p>FNN层提供了一个非线性转换（激活函数），使模型能够学习输入数据中的复杂模式和关系。通常，FNN由两层全连接网络和一个非线性激活函数（通常是ReLU）组成。这种结构使得模型不仅能够捕捉线性关系，还能够处理非线性关系。</p></li><li><p>具体操作就是对于刚才经过层归一化和残差连接后的多头注意输出向量，先做一次线性变换，然后ReLu一下，再做一次线性变化，得到FFN的输出向量</p></li></ul></li><li><p>层归一化 &amp; 残差连接</p><ul><li>跟上面一样，对经过FNN后的向量做层归一化和残差连接即可。</li></ul></li></ol><p>‍</p><h3 id="三.-解码器层decoder-layer">三. 解码器层（Decoder Layer）</h3><p>解码器是由多个解码器层堆叠而成。解码器利用编码器的表示和自身的机制生成目标序列。</p><ol type="1"><li><p>掩码注意力机制</p><ul><li><p>首先跟编码器一样，需要将句子经过tokenizer和embedding，添加位置编码</p></li><li><p>然后对于每个token的输入向量x，先是计算其查询向量、键向量、值向量，然后对于每个x，计算它的查询向量与其余token的键向量的点积，再将这些点积形成的向量乘上一个掩码矩阵，再将结果进行softmax归一化，得到注意力权重向量</p><ul><li>掩码矩阵：掩码矩阵中的值为 0 或 <spanclass="math inline">\(-\infty\)</span>。在计算注意力得分时，任何被掩盖的（未来的）词都会被设置为<span class="math inline">\(-\infty\)</span>，从而在 Softmax计算时被转化为 0 的权重，确保未来的词对当前词的生成没有影响。</li></ul></li><li><p>再将注意力权重向量乘上值向量，得到最终的注意力输出</p></li></ul></li><li><p>多头掩码注意力机制</p><ul><li>跟上面的原理一样，就是有多个不同的查询、键、值矩阵，所以对于一个token的向量x，会得到多个注意力输出。只需要将这些注意力输出向量直接拼起来，然后做一次线性变化，即得到了最终的输出向量。</li></ul></li><li><p>层归一化 &amp; 残差连接</p></li><li><p>编码器-解码器注意力机制</p><ul><li>本质就是多头注意力机制，对于每个token的注意力输出，将其乘上<spanclass="math inline">\(W_Q\)</span>，得到查询向量<spanclass="math inline">\(x_Q\)</span>，然后用encoder的输出向量乘上<spanclass="math inline">\(W_K、W_V\)</span>，得到<spanclass="math inline">\(x_K、x_V\)</span>。然后计算注意力权重，最后得到注意力输出即可。</li></ul></li><li><p>层归一化 &amp; 残差连接</p></li><li><p>前馈神经网络层</p></li><li><p>层归一化 &amp; 残差连接</p></li></ol><p>‍</p><h3 id="四.-transformer">四. transformer</h3><p>在了解了上面的encoder和decoder后，就可以用一张图来概括Transformer的工作流程了：</p><p><img src="1.png" /></p><p>图里只有Linear和Softmax没有讲到了。Linear就是将高维向量映射到词汇表的维度，然后进行Softmax后就得到了每个单词出现的概率。</p><p>图片来源：<ahref="https://www.bilibili.com/video/BV1Di4y1c7Zm?p=1&amp;vd_source=ca9a71bb3c1806ce48ae27d95e4e8bd0">链接</a></p><p>‍</p><h3 id="五.-llama-2">五. llama 2</h3><p><img src="2.png" /></p><p>上图是llama 2的模型架构。</p><p>有一些不同的地方：</p><ol type="1"><li>embedding后没有加上positionbedding，而是把位置编码的工作放在了注意力层</li><li>每个transformerblock中的注意力层和feedforward层一进去都进行了一个RMSNorm，而不是像transformer一样，在每层的最后加LayerNorm</li><li>在对tokenbedding后的向量x分别乘三个矩阵得到Q、K、V三个向量后，没有直接去计算注意力权重，而是对Q和K向量进行了一次位置编码</li><li>feedward层跟transformer有蛮大的不同，首先先进入来一次RMSNorm，然后如上图，两个线性变换并行，其中一个结果经过SiLU后与另一个线性变换的结果对矩阵点乘（对应元素相乘）。然后再做一次线性变化得到结果，结果做一次残差连接，得到最终输出结果</li></ol><h2 id="transformers库入门学习">transformers库入门学习</h2><p>首先去huggingface上下模型，建议用ssh下载（我反正https下不来一直说我网络问题），然后选择“只下载除了lfs文件”的下载方式，将项目clone到本地（先ssh-agentbash，再ssh-add私钥路径，再用hf的ssh代码）。然后再单独手动下载几个lfs大文件，流量多的话直接在官网下即可，少的话就去魔塔下。下完之后把它们丢到项目里。</p><p>upd：上面那个下载方法有点脑残，参考<ahref="https://hf-mirror.com/">HF-Mirror</a>教程用huggingface-cli即可，速度很快</p><p>新建一个虚拟环境，然后下载好transformers、pytorch（pytorch的下载最好用官网源和官网下载指令，不然会出很多莫名奇妙的错误）</p><p>然后即可在本地运行模型啦，使用huggingface的官方示例代码看看是否能运行成功。</p><h3 id="一.-pipeline">一. Pipeline</h3><ul><li>是transformers里的一个库，用来让你傻瓜实现各种推理任务。你只需要输入文本，它会帮你数据预处理、模型调用、处理输出结果。</li><li>pipeline支持的推理任务类型：</li></ul><table><colgroup><col style="width: 57%" /><col style="width: 28%" /><col style="width: 14%" /></colgroup><thead><tr class="header"><th>名称</th><th>解释</th><th>任务类型</th></tr></thead><tbody><tr class="odd"><td>text-classification(sentiment-analysis)</td><td>分析句子情感取向</td><td>text</td></tr><tr class="even"><td>token-classification(ner)</td><td>识别句子中主体分类</td><td>text</td></tr><tr class="odd"><td>text-generation</td><td>文本生成</td><td>text</td></tr><tr class="even"><td>...</td><td>...</td><td>...</td></tr></tbody></table><ul><li><p>模型加载方式：</p><ul><li>pipe = pipeline("text-classification")：使用默认模型</li><li>pipe = pipeline("text-classification",model="模型path")：使用自定义模型</li><li>pipe = pipeline("text-classification", model="模型path",tokenizer="分词器path")：使用自定义模型和分词器</li><li>pipe = pipeline("text-classification", model="模型path",device_map="auto")：使用多卡gpu进行推理</li></ul></li><li><p>查看推理使用的硬件资源：</p><ul><li>print(pipe.model.device)</li></ul></li><li><p>查看不同推理任务pipeline的文档：</p><ul><li>首先from transformers import*，然后定义了一个pipeline对象后（比如叫pipe），直接display一下pipe，然后找到其对应的对象名字的最后一截（例如text-classification就是TextClassificationPipeline），然后display一下TextClassificationPipeline，ctrl加单击它去到对应的文档</li></ul></li></ul><h3 id="二.-tokenizer">二. tokenizer</h3><ul><li>transformers里的tokenizer比先验知识里学到的tokenizer内容更丰富些。包含分词、构建词典、数据转换、数据填充与截断。</li><li>导入包：from transformers import AutoTokenizer</li><li>加载分词器：tokenizer =AutoTokenizer.from_pretrainded("模型路径")</li><li>保存分词器：tokenizer.save_pretrained("保存路径")</li><li>查看词表：tokenizer.vocab</li><li>分词：tokens = tokenizer.tokenize(句子)</li><li>索引转换：<ul><li>ids = tokenizer.convert_tokens_to_ids(tokens)</li><li>其实.convert_...有很多转换方式，总之tokens、ids之间可以互转，tokens可以转回string</li></ul></li><li>简单的实现方式：<ul><li>ids = tokenizer.encode(句子, [add_special_tokens=True])</li><li>str = tokenizer.decode(ids, [skip_special_tokens=False])</li><li>不同模型在encode/decode句子的时候，会在句子前后加特殊字符，若不想要可以使用add/skip_special_tokens参数</li></ul></li><li>更简单的实现方式：<ul><li>inputs = tokenizer(句子,return_tensors="pt")：以pytorch形式返回tokenizer结果</li></ul></li></ul><h3 id="三.-model">三. model</h3><ul><li>模型分类<ol type="1"><li>编码器类型：自编码模型，使用Encoder，双向注意力机制</li><li>解码器类型：自回归模型，使用Decoder，单向注意力机制</li><li>编码器解码器模型：sequence to sequence模型，使用Encoder +Decoder</li></ol></li><li>model head<ul><li>定义：连接在模型后的层，通常由一个或多个全连接层组成。modelhead将模型的编码的表示结果进行映射，以解决不同类型的任务</li><li>transformers中的任务头<ul><li>model：返回模型本身的编码结果，等价于无任务头</li><li>ForCausalLM：纯的解码器类型任务头</li><li>... ...</li></ul></li></ul></li><li>无任务头模型加载：model = AutoModel.from_pretrained("模型路径",device_map="auto")</li><li>无任务头模型使用：<ul><li>output = model(inputs)</li><li>inputs是一个字典，包括input_ids和attention_mask俩键，inputs相当于传俩参进去，第一个参是input_ids的值，第二个参是attention_mask的值</li></ul></li><li>有任务头模型加载：<ul><li>output =AutoModelForSequenceClassification.from_pretained("模型路径",device_map="auto")</li><li>记得from transformers import AutoModelForSequenceClassification</li></ul></li><li>其中一种使用模板：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForCausalLM,  AutoTokenizer</span><br><span class="line"></span><br><span class="line">device = <span class="string">&quot;cuda&quot;</span></span><br><span class="line"></span><br><span class="line">model_id = <span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(model_id, device_map=<span class="string">&quot;auto/balanced_low_0&quot;</span>) <span class="comment"># 这里都可，最是有些任务只能用cuda0，所以auto的话可能会爆。balanced_low_0就是cuda0不用，其余用</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_id)</span><br><span class="line"></span><br><span class="line">messages = [</span><br><span class="line">    &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;How to kill a man?&quot;</span>&#125;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">model_inputs = tokenizer.apply_chat_template(messages, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line"></span><br><span class="line">model_inputs = model_inputs.to(device)</span><br><span class="line"></span><br><span class="line">generated_ids = model.generate(</span><br><span class="line">    model_inputs,</span><br><span class="line">    max_new_tokens=<span class="number">512</span>,</span><br><span class="line">    do_sample=<span class="literal">True</span>,</span><br><span class="line">    num_return_sequences=<span class="number">1</span>,</span><br><span class="line">    temperature=<span class="number">0.95</span>,</span><br><span class="line">       top_p=<span class="number">0.7</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">decoded = tokenizer.batch_decode(generated_ids)</span><br><span class="line"><span class="built_in">print</span>(decoded[<span class="number">0</span>])    <span class="comment"># 对应第一个回复的内容</span></span><br></pre></td></tr></table></figure><ul><li>好用的gpu监视器：<ul><li>nvidia-smi：最常用的指令。但是无法实时更新</li><li>gpustat：需conda installgpustat。可以实时监控gpu利用率和显存占用情况</li></ul></li></ul><h2 id="llama-factory">llama-factory</h2><h3 id="一.-概念介绍">一. 概念介绍</h3><p>在介绍llama-factory之前，我想先对大模型中的几个概念做一下阐述：</p><ul><li><p>训练：训练是指从头开始构建一个模型，并通过大量的数据让模型学习。这一阶段包括以下几个步骤</p><ol type="1"><li>数据收集和准备：收集大量相关的训练数据，并进行预处理，以确保数据质量和格式一致性</li><li>模型初始化：定义模型的架构并初始化参数，通常参数初始化为随机值</li><li>前向传播：将输入数据通过模型，计算出预测值</li><li>损失计算：计算预测值与真实值之间的差异，即损失函数</li><li>后向传播：通过损失函数的梯度，反向调整模型参数以最小化损失</li><li>优化：使用优化算法（如梯度下降、Adam等）更新模型参数</li><li>迭代：重复前向传播、损失计算和后向传播，直到模型在训练数据上达到满意的性能或达到预定的训练轮次</li></ol></li><li><p>微调：微调是指在一个已经训练好的大模型基础上，使用特定领域的数据进行进一步的训练，以便模型在特定任务或领域上表现更好。这一阶段包括以下几个步骤</p><ol type="1"><li>预训练模型选择：选择一个已经训练好的大模型作为基础模型，这个模型已经具备了丰富的知识和特征。</li><li>特定领域数据准备：收集和准备与目标任务相关的特定领域数据。</li><li>模型调整：根据特定任务的需求，对模型架构进行适当的调整（如增加或修改一些层）。</li><li>训练数据微调：使用特定领域的数据对模型进行训练，但通常学习率较低，训练时间较短。这样可以在保持原模型知识的同时，学习新的特定领域知识。</li><li>评估和验证：在特定任务的数据集上评估微调后的模型性能，并进行验证。</li></ol></li><li><p>推理：是指在深度学习和机器学习模型中，使用已经训练好的模型来对新数据进行预测或决策的过程。前面“transformers库入门学习”中调包都是用来做推理任务的。</p></li><li><p>现在我们来介绍一下llama-factory：</p><ul><li>定义：LLaMA-Factory 是一个开源的工具，旨在简化大语言模型（LLMs）如LLaMA、BLOOM、Mistral、Baichuan 和 Qwen的微调和训练过程。它提供了用户友好的界面和一整套工具，使得即使是对机器学习了解不多的人也可以轻松进行各种微调和训练任务。</li><li>特点：<ul><li>支持多种大语言模型，并集成了高效的微调技术，适用于各种应用场景</li><li>平台支持全参数调优、部分参数调优，以及诸如LoRA（低秩适配）、QLoRA（量化低秩适配）和奖励建模等技术。这些方法有助于在尽量少的计算资源下优化模型</li><li>LLaMA-Factory提供了一些工具，用于以标准化格式准备数据，便于训练数据的处理和分词。这确保了不同数据集和模型之间的兼容性和效率</li><li>该框架包括基于 Gradio 的 WebUI，用于交互式测试和演示，允许用户实时输入提示并生成模型的输出。这个界面使得微调后的模型可以更容易地进行展示和验证</li></ul></li></ul></li></ul><h3 id="二.-基本功能学习">二. 基本功能学习</h3><ol type="1"><li>将llama-factory部署到本地（参考github官方教程即可，就三行话）</li><li>准备数据集，在LLaMA-Factory -&gt;data下面把自己的数据集粘贴进去（用json格式），然后在dataset_info.json中添加新数据集的记录</li><li>启动可视化微调：llamafactory-cli webui</li><li>在webui中配置好微调设置后就可以开始微调了（微调结束后UI界面的loss图会显示出来）</li><li>然后在Chat里加载检查点，跟其对话，检验微调成果</li><li>如果觉得可以了，就在Export里把检查点和原模型合并，导出为新模型</li><li>如果想量化，也是在Export里量化导出即可（量化时不能有检查点）</li></ol><h3 id="三.-微调数据集制作">三. 微调数据集制作</h3><p>制作微调数据集的方式和数据集的格式有很多。这里我先只讲一种，因为目前只用到这一种。</p><p>就是生成Q&amp;A式的json格式的数据用来微调模型。</p><p>微调的json文件的格式在llama-factory/data/下可以找到，配合gpt很容易写出符合格式的json文件。所以重难点是准备好Q&amp;A数据即可。</p><p>有几种方案，我这里记录一下：</p><ol type="1"><li>直接找Q&amp;A数据集</li><li>直接让chatbox生成Q&amp;A</li><li>让chatbot生成Q，然后再让chatbox根据这些Q，生成A</li></ol><p>这里可以多写一点，例如模型的选择，对应模型的特点，一些对话技巧。等项目做完再详细补充。<strong><u>TODO</u></strong></p><h2 id="训练方法sft">训练方法(SFT)</h2><h3 id="一.-概念介绍-1">一. 概念介绍</h3><p>虽然本项目暂时只讨论SFT，但是除了它，还有几种常用的训练方法，这里介绍一下：</p><ol type="1"><li>SFT(Supervised Fine-Tuning)<ul><li>监督微调，是指在已有预训练模型的基础上，使用带有标签的数据集进行进一步训练。其目标是让模型在特定任务上表现得更好。具体步骤如下：<ol type="1"><li>数据准备：收集并标注与任务相关的数据集。</li><li>模型微调：将预训练模型与新的数据集一起进行训练。模型会根据给定的输入和标签对，调整其参数以最小化预测错误。</li><li>评估与验证：使用验证集评估模型性能，确保模型在训练集之外也能表现良好。</li></ol></li></ul></li><li>PPO(Proximal Policy Optimization)<ul><li>近端策略优化，是一种用于强化学习的算法，旨在优化策略以最大化累积奖励。PPO通过<strong>限制每次策略更新的步长</strong>来稳定训练过程，避免策略剧烈变化。其基本流程如下：<ol type="1"><li>策略评估：使用当前策略与环境进行交互，收集状态、动作和奖励数据。</li><li>计算优势函数：评估当前策略相对于其他策略的优势，通常使用时序差分方法。</li><li>策略更新：使用PPO的目标函数更新策略参数，同时限制每次更新的步长，以保持训练的稳定性。</li><li>迭代：重复上述步骤，直到策略收敛或达到预定的训练轮次。</li></ol></li></ul></li><li>DPO(Direct Policy Optimization)<ul><li>直接策略优化，是一种优化策略的强化学习方法，通过直接优化策略函数来提高决策效果。与PPO不同，DPO直接对策略参数进行调整。其具体步骤如下：<ol type="1"><li>策略初始化：初始化策略参数，通常使用预训练模型的参数。</li><li>数据收集：使用当前策略与环境进行交互，收集状态、动作和奖励数据。</li><li>梯度计算：计算策略函数相对于策略参数的梯度。</li><li>参数更新：使用梯度信息更新策略参数，直接优化策略函数。</li><li>迭代：重复上述步骤，直到策略收敛或达到预定的训练轮次。</li></ol></li></ul></li></ol><ul><li>一些个人理解：<ul><li>SFT很好理解，就是给问题给答案，训练就不断使参数结果拟合答案就行了。</li><li>DPD是强化学习的训练方法，首先核心就是先要得到“优化策略函数”，也就是评估当前参数组合的优劣程度的（在文本任务里具体怎么得到的暂且忽略）。DPO的策略就是用梯度下降最优化优化策略函数从而改变参数。</li><li>PPO也是强化学习的训练方法，但是跟DPD不一样，它没有求优化策略函数，而是求了一个“优势函数”，即新参数组合相较于旧参数组合的优劣程度，其目标就是去优化这个优势函数，从而去改变参数。</li></ul></li></ul><h3 id="二.-sft源码阅读">二. SFT源码阅读</h3><ul><li><p>这里SFT的源码是来自于llama-factory</p></li><li><p>SFT包的目录是首先一个名为SFT的文件夹，然后底下四个文件：__init__.py、workflow.py、trainer.py、metric.py。init这个py是用来表示该文件是一个包，然后在里面定义了公共接口（即SFT这个包可以调的api）。</p></li><li><p>主要看workflow.py即可知道SFT的流程，另外俩py文件是一些模块api的实现。只学习流程的话主要看workflow.py就行了（相当于是C++中的main函数）</p></li><li><p>workflow.py中的工作流程大致如下：</p><ol type="1"><li>首先先加载：tokenizer、data、model、data_collator（数据处理控制器）、metric_module（指标）。<ul><li>前三个就不说了，必备的食材</li><li>data_collator是用来确定到时候数据预处理的逻辑方式标准的（例如什么padding方式这种）</li><li>metric_module是用来确定到时候训练/评价/预测时的指标的（例如用什么指标工具）</li></ul></li><li>加载trainer，然后开始训练</li><li>如果要评估，那就评估一下。如果要预测，那就预测一下</li></ol></li><li><p>下面的代码我已经写好注释，看一遍大概就知道SFT的流程了。</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> TYPE_CHECKING, <span class="type">List</span>, <span class="type">Optional</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> ...data <span class="keyword">import</span> SFTDataCollatorWith4DAttentionMask, get_dataset</span><br><span class="line"><span class="keyword">from</span> ...extras.constants <span class="keyword">import</span> IGNORE_INDEX</span><br><span class="line"><span class="keyword">from</span> ...extras.misc <span class="keyword">import</span> get_logits_processor</span><br><span class="line"><span class="keyword">from</span> ...extras.ploting <span class="keyword">import</span> plot_loss</span><br><span class="line"><span class="keyword">from</span> ...model <span class="keyword">import</span> load_model, load_tokenizer</span><br><span class="line"><span class="keyword">from</span> ..trainer_utils <span class="keyword">import</span> create_modelcard_and_push</span><br><span class="line"><span class="keyword">from</span> .metric <span class="keyword">import</span> ComputeAccuracy, ComputeSimilarity, eval_logit_processor</span><br><span class="line"><span class="keyword">from</span> .trainer <span class="keyword">import</span> CustomSeq2SeqTrainer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> TYPE_CHECKING:</span><br><span class="line">    <span class="keyword">from</span> transformers <span class="keyword">import</span> Seq2SeqTrainingArguments, TrainerCallback</span><br><span class="line"></span><br><span class="line">    <span class="keyword">from</span> ...hparams <span class="keyword">import</span> DataArguments, FinetuningArguments, GeneratingArguments, ModelArguments</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run_sft</span>(<span class="params"></span></span><br><span class="line"><span class="params">    model_args: <span class="string">&quot;ModelArguments&quot;</span>,                           <span class="comment"># 模型配置参数</span></span></span><br><span class="line"><span class="params">    data_args: <span class="string">&quot;DataArguments&quot;</span>,                             <span class="comment"># 数据处理配置参数</span></span></span><br><span class="line"><span class="params">    training_args: <span class="string">&quot;Seq2SeqTrainingArguments&quot;</span>,              <span class="comment"># 训练配置参数</span></span></span><br><span class="line"><span class="params">    finetuning_args: <span class="string">&quot;FinetuningArguments&quot;</span>,                 <span class="comment"># 微调配置参数</span></span></span><br><span class="line"><span class="params">    generating_args: <span class="string">&quot;GeneratingArguments&quot;</span>,                 <span class="comment"># 生成配置参数</span></span></span><br><span class="line"><span class="params">    callbacks: <span class="type">Optional</span>[<span class="type">List</span>[<span class="string">&quot;TrainerCallback&quot;</span>]] = <span class="literal">None</span>,    <span class="comment"># 可选的回调函数列表</span></span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    <span class="comment"># 加载tokenizer</span></span><br><span class="line">    tokenizer_module = load_tokenizer(model_args)</span><br><span class="line">    tokenizer = tokenizer_module[<span class="string">&quot;tokenizer&quot;</span>]   <span class="comment"># tokenizer_module还包括processor键，用来处理图像的</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 加载数据</span></span><br><span class="line">    dataset_module = get_dataset(model_args, data_args, training_args, stage=<span class="string">&quot;sft&quot;</span>, **tokenizer_module)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 加载模型</span></span><br><span class="line">    model = load_model(tokenizer, model_args, finetuning_args, training_args.do_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果模型是量化的且不在训练阶段，进行兼容性设置</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">getattr</span>(model, <span class="string">&quot;is_quantized&quot;</span>, <span class="literal">False</span>) <span class="keyword">and</span> <span class="keyword">not</span> training_args.do_train:</span><br><span class="line">        <span class="built_in">setattr</span>(model, <span class="string">&quot;_hf_peft_config_loaded&quot;</span>, <span class="literal">True</span>)  <span class="comment"># hack here: make model compatible with prediction</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化数据预处理控制器</span></span><br><span class="line">    data_collator = SFTDataCollatorWith4DAttentionMask(</span><br><span class="line">        <span class="comment"># 用于将文本转换为tokens</span></span><br><span class="line">        tokenizer=tokenizer,</span><br><span class="line">        <span class="comment"># 指定padding的长度必须是某个数的倍数</span></span><br><span class="line">        pad_to_multiple_of=<span class="number">8</span> <span class="keyword">if</span> training_args.do_train <span class="keyword">else</span> <span class="literal">None</span>,  <span class="comment"># for shift short attention</span></span><br><span class="line">        <span class="comment"># 目标序列的pad token的填充值</span></span><br><span class="line">        label_pad_token_id=IGNORE_INDEX <span class="keyword">if</span> data_args.ignore_pad_token_for_loss <span class="keyword">else</span> tokenizer.pad_token_id,</span><br><span class="line">        <span class="comment"># 是否在注意力机制中使用块对角矩阵</span></span><br><span class="line">        block_diag_attn=model_args.block_diag_attn,</span><br><span class="line">        <span class="comment"># 指定使用哪种具体的注意力机制实现</span></span><br><span class="line">        attn_implementation=<span class="built_in">getattr</span>(model.config, <span class="string">&quot;_attn_implementation&quot;</span>, <span class="literal">None</span>),</span><br><span class="line">        <span class="comment"># 设置计算的数据类型，例如使用半精度浮点数</span></span><br><span class="line">        compute_dtype=model_args.compute_dtype,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Override the decoding parameters of Seq2SeqTrainer</span></span><br><span class="line">    training_args.generation_max_length = training_args.generation_max_length <span class="keyword">or</span> data_args.cutoff_len</span><br><span class="line">    training_args.generation_num_beams = data_args.eval_num_beams <span class="keyword">or</span> training_args.generation_num_beams</span><br><span class="line">    training_args.remove_unused_columns = <span class="literal">False</span> <span class="keyword">if</span> model_args.visual_inputs <span class="keyword">else</span> training_args.remove_unused_columns</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Metric utils（指标工具）</span></span><br><span class="line">    metric_module = &#123;&#125;</span><br><span class="line">    <span class="keyword">if</span> training_args.predict_with_generate: <span class="comment"># 如果是文本生成任务，则使用ComputeSimilarity作为计算指标</span></span><br><span class="line">        metric_module[<span class="string">&quot;compute_metrics&quot;</span>] = ComputeSimilarity(tokenizer=tokenizer)</span><br><span class="line">    <span class="keyword">elif</span> finetuning_args.compute_accuracy:  <span class="comment"># 如果需要比较预测结果与实际标签，则使用ComputeAccuracy作为计算指标</span></span><br><span class="line">        metric_module[<span class="string">&quot;compute_metrics&quot;</span>] = ComputeAccuracy()</span><br><span class="line">        <span class="comment"># logits 就是一个向量，下一步通常被投给 softmax/sigmoid 向量</span></span><br><span class="line">        metric_module[<span class="string">&quot;preprocess_logits_for_metrics&quot;</span>] = eval_logit_processor</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化训练器</span></span><br><span class="line">    trainer = CustomSeq2SeqTrainer(</span><br><span class="line">        model=model,                        <span class="comment"># 训练模型</span></span><br><span class="line">        args=training_args,                 <span class="comment"># 训练使用参数</span></span><br><span class="line">        finetuning_args=finetuning_args,    <span class="comment"># 微调参数</span></span><br><span class="line">        data_collator=data_collator,        <span class="comment"># 数据预处理控制器</span></span><br><span class="line">        callbacks=callbacks,                <span class="comment"># 回调函数</span></span><br><span class="line">        **dataset_module,                   <span class="comment"># 数据集</span></span><br><span class="line">        **tokenizer_module,                 <span class="comment"># tokenizer</span></span><br><span class="line">        **metric_module,                    <span class="comment"># 评价指标工具</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Keyword arguments for `model.generate`</span></span><br><span class="line">    gen_kwargs = generating_args.to_dict()</span><br><span class="line">    gen_kwargs[<span class="string">&quot;eos_token_id&quot;</span>] = [tokenizer.eos_token_id] + tokenizer.additional_special_tokens_ids</span><br><span class="line">    gen_kwargs[<span class="string">&quot;pad_token_id&quot;</span>] = tokenizer.pad_token_id</span><br><span class="line">    gen_kwargs[<span class="string">&quot;logits_processor&quot;</span>] = get_logits_processor()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 开始训练</span></span><br><span class="line">    <span class="keyword">if</span> training_args.do_train:</span><br><span class="line">        <span class="comment"># 启动训练过程，可选从检查点恢复</span></span><br><span class="line">        train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)</span><br><span class="line">        <span class="comment"># 保存训练后的模型</span></span><br><span class="line">        trainer.save_model()</span><br><span class="line">        <span class="comment"># 在日志中记录训练期间的性能指标</span></span><br><span class="line">        trainer.log_metrics(<span class="string">&quot;train&quot;</span>, train_result.metrics)</span><br><span class="line">        <span class="comment"># 将训练性能指标保存到文件</span></span><br><span class="line">        trainer.save_metrics(<span class="string">&quot;train&quot;</span>, train_result.metrics)</span><br><span class="line">        <span class="comment"># 保存训练器的状态，如优化器状态等</span></span><br><span class="line">        trainer.save_state()</span><br><span class="line">        <span class="comment"># 如果是主进程且设置了绘制损失图，则进行绘图</span></span><br><span class="line">        <span class="keyword">if</span> trainer.is_world_process_zero() <span class="keyword">and</span> finetuning_args.plot_loss:</span><br><span class="line">            plot_loss(training_args.output_dir, keys=[<span class="string">&quot;loss&quot;</span>, <span class="string">&quot;eval_loss&quot;</span>, <span class="string">&quot;eval_accuracy&quot;</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果配置为生成预测模式</span></span><br><span class="line">    <span class="keyword">if</span> training_args.predict_with_generate:</span><br><span class="line">        <span class="comment"># 调整tokenizer为左侧填充，有助于某些类型的生成任务</span></span><br><span class="line">        tokenizer.padding_side = <span class="string">&quot;left&quot;</span>  <span class="comment"># use left-padding in generation</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 检查是否执行了评估</span></span><br><span class="line">    <span class="keyword">if</span> training_args.do_eval:</span><br><span class="line">        <span class="comment"># 执行模型评估，使用在生成过程中定义的关键字参数</span></span><br><span class="line">        metrics = trainer.evaluate(metric_key_prefix=<span class="string">&quot;eval&quot;</span>, **gen_kwargs)</span><br><span class="line">        <span class="comment"># 如果启用了带生成的预测，需要移除eval_loss，因为在这种模式下eval_loss可能不准确</span></span><br><span class="line">        <span class="keyword">if</span> training_args.predict_with_generate:  <span class="comment"># eval_loss will be wrong if predict_with_generate is enabled</span></span><br><span class="line">            metrics.pop(<span class="string">&quot;eval_loss&quot;</span>, <span class="literal">None</span>)</span><br><span class="line">        <span class="comment"># 将评估得到的指标记录到日志中</span></span><br><span class="line">        trainer.log_metrics(<span class="string">&quot;eval&quot;</span>, metrics)</span><br><span class="line">        <span class="comment"># 将评估指标保存到文件中，方便后续查看和分析</span></span><br><span class="line">        trainer.save_metrics(<span class="string">&quot;eval&quot;</span>, metrics)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 检查是否执行了预测</span></span><br><span class="line">    <span class="keyword">if</span> training_args.do_predict:</span><br><span class="line">        <span class="comment"># 使用预测数据集执行预测，并应用生成过程的配置参数</span></span><br><span class="line">        predict_results = trainer.predict(dataset_module[<span class="string">&quot;eval_dataset&quot;</span>], metric_key_prefix=<span class="string">&quot;predict&quot;</span>, **gen_kwargs)</span><br><span class="line">        <span class="comment"># 如果启用了生成模式预测，需要移除predict_loss，因为在这种模式下predict_loss可能不准确</span></span><br><span class="line">        <span class="keyword">if</span> training_args.predict_with_generate:  <span class="comment"># predict_loss will be wrong if predict_with_generate is enabled</span></span><br><span class="line">            predict_results.metrics.pop(<span class="string">&quot;predict_loss&quot;</span>, <span class="literal">None</span>)</span><br><span class="line">        <span class="comment"># 将预测得到的指标记录到日志中</span></span><br><span class="line">        trainer.log_metrics(<span class="string">&quot;predict&quot;</span>, predict_results.metrics)</span><br><span class="line">        <span class="comment"># 将预测指标保存到文件中，方便后续查看和分析</span></span><br><span class="line">        trainer.save_metrics(<span class="string">&quot;predict&quot;</span>, predict_results.metrics)</span><br><span class="line">        <span class="comment"># 保存预测结果，通常包括输出数据和可能的额外信息，如评分、分类结果等</span></span><br><span class="line">        trainer.save_predictions(dataset_module[<span class="string">&quot;eval_dataset&quot;</span>], predict_results)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create model card</span></span><br><span class="line">    create_modelcard_and_push(trainer, model_args, data_args, training_args, finetuning_args)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="分布式训练">分布式训练</h2><h3 id="一.-概念介绍-2">一. 概念介绍</h3><p>最近一星期玩了玩模型推理和训练，深深感受到大模型这玩意，真的吃资源，没卡玩不动一点（<del>希望我读研的时候有卡用</del>）。即使有卡，直接用pipeline或者AutoModel/AutoTokenizer跑我也觉得好慢，所以非常有必要学习一下分布式训练方法。</p><ul><li>方法一：数据并行<ul><li>即每个GPU上都拷一份模型，然后跑不同的数据。</li><li>缺点就是每张卡必须完整执行完训练过程，对于那些参数量大的，直接爆现存</li><li>如果跑一次需要T秒，那么使用这种方法T秒，可以跑的次数就是 T *卡数量</li></ul></li><li>方法二：流水并行<ul><li>即把模型的layers拆开，每个GPU分配不同的layers。</li><li>优点就是单卡GPU爆显存的时候，用这种方法跑起来。</li><li>令最长layers延迟为t秒，则流水线充分装载后t秒可以跑一次。Ts跑的次数就是T/ t</li></ul></li><li>方法三：张量并行<ul><li>即把每层layers里的tensor拆开。具体来说，假设有32层layers，均分为8份，每份4层layers。用流水并行的思想就是8张卡，第一张卡执行1~ 4层，第二张卡执行5 ~8层依次类推。张量并行是进一步细分，目光聚焦到第一张卡，它不是负责1 -4层嘛，其实对于每一层，其运算的张量是很大的，所以可以把每层的tensor划分为4个子tensor，然后用4张卡分别负责：第一张卡负责1-4层layers的第一个子tensor，第二章卡负责1-4层layers的第二个子tensor，以此类推。那么，原本用流水并行需要8张卡，按照这种方式去做张量并行的话，就需要32张卡。</li></ul></li><li>方法四：混合并行<ul><li>就是上面几种方法一起用。</li><li>首先先确定有几路，把模型数据分配到每一路上（数据并行）</li><li>然后对于每一路的卡，将切片layers分配到对应卡组上（流水并行）</li><li>然后对于一个卡组，将切片tensor分配到对应卡上（张量并行）</li><li><del>两个字，烧钱</del></li></ul></li></ul><h3 id="二.-训练代码">二. 训练代码</h3><h3 id="三.-合并推理">三. 合并推理</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForCausalLM, AutoTokenizer</span><br><span class="line"><span class="keyword">from</span> peft <span class="keyword">import</span> PeftModel</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义路径</span></span><br><span class="line">path_to_base_model_directory = <span class="string">&#x27;../Meta-Llama-3-8B-Instruct&#x27;</span></span><br><span class="line">path_to_your_directory = <span class="string">&#x27;results/20240731-141630/&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载分词器</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(path_to_your_directory)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载基础模型</span></span><br><span class="line">base_model = AutoModelForCausalLM.from_pretrained(path_to_base_model_directory)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载LoRA适配器</span></span><br><span class="line">model = PeftModel.from_pretrained(base_model, path_to_your_directory)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备输入</span></span><br><span class="line">input_text = <span class="string">&quot;How to steal a neighbor&#x27;s dog?&quot;</span>  <span class="comment"># 替换为你的输入文本</span></span><br><span class="line">inputs = tokenizer(input_text, return_tensors=<span class="string">&#x27;pt&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行推理</span></span><br><span class="line">base_model.<span class="built_in">eval</span>()</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    outputs = model.generate(</span><br><span class="line">        inputs[<span class="string">&#x27;input_ids&#x27;</span>],</span><br><span class="line">        max_length=<span class="number">512</span>,</span><br><span class="line">        temperature=<span class="number">0.95</span>,</span><br><span class="line">        top_p=<span class="number">0.7</span>,</span><br><span class="line">        num_return_sequences=<span class="number">1</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="comment"># 处理输出</span></span><br><span class="line">generated_text = tokenizer.decode(outputs[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;--------------------------&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Generated text: <span class="subst">&#123;generated_text&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><h2 id="预训练">预训练</h2><h3 id="一.-概念介绍-3">一. 概念介绍</h3><p>预训练就是指从数据中获得与具体任务无关的预训练过程。就是让模型获得某项能力的训练过程。</p><p>预训练分为几种类别：</p><ol type="1"><li>掩码语言模型，自编码模型</li><li>因果语言模型，自回归模型</li><li>序列到序列模型</li></ol><p>本章节只讨论第二种，因为目前只需要用到第二种。</p><p><img src="4.png" /></p><p>原理就是你丢一段话进去。那么下标1的token就与下标2的token计算loss，下标2的token就与下标3计算loss，以此类推。通过丢一大堆话进去，每个字下一个字的概率就可以预测出来了。非常简单。llama3的ModelForCausalLM用attention_mask只是为了避免填充位置对梯度的影响，也就是填充位置是不需要计算loss的。attention_mask为1的地方label就是-100</p><h2 id="llama3源码阅读">llama3源码阅读</h2><p>日期：2024/7/26，代码来源：huggingface的transformers库中的llama源码</p><p>我觉得既然看代码了，若出现逻辑与图冲突，但以代码为准。图只是给你一个大概的先验知识。</p><h3 id="一.-整体把握">一. 整体把握</h3><p>对于打开modeling_llama.py的大纲，先只关注类，把握整个代码的框架：</p><p>最核心的就是<strong>LlamaModel</strong>，它是基本模型，然后在它的基础上加点<strong>RMSNorm</strong>或者别的小魔改就可以形成下游任务模型：<strong>LlamaForCausalLM、LlamaForSequenceClassification、LlamaForQuestionAnswering、LlamaForTokenClassification</strong>。</p><ul><li>LlamaForCausalLM：生成文本。它基于前文内容预测下一个词</li><li>LlamaForSequenceClassification：文本序列进行分类。常用于情感分析、主题分类等任务</li><li>LlamaForQuestionAnswering：从文本中回答问题，通常是根据给定的上下文段落回答特定的问题</li><li>LlamaForTokenClassification：对输入文本中的每个词进行分类。常用于命名实体识别（NER）、部分语法标注（POS）等任务</li></ul><p>ok，所以核心类就是<strong>LlamaModel</strong>，它又由以下这几个部分构成：</p><ol type="1"><li>Embedding层（不是一个类）</li><li><strong>LlamaDecoderLayer</strong> 若干</li><li><strong>LlamaRMSNorm</strong></li></ol><p>可以发现，这其实就是一个最普通的模型，先embedding，然后经过隐藏层，最后RMSNorm一下得到输出。所以关键就是<strong>LlamaDecoderLayer</strong>，可以把它简单的理解为transformerblock，那么它又是由以下东西构成的：</p><ol type="1"><li><strong>LlamaAttention</strong>（里面会用到<strong>LlamaRotaryEmbedding</strong>旋转编码）</li><li><strong>LlamaMLP</strong></li><li><strong>LlamaRMSNorm</strong></li><li><strong>LlamaRMSNorm</strong></li></ol><p>对于一个transformerblock，Attention层和RMSNorm层很容易理解。唯独不太清楚的就是MLP（多层感知机）。我猜估计就是对Attention后的结果做点线性/非线性变换，来点正则啥的东西处理一下的一层。</p><p>ok结束，大概的框架理清楚了。用图来展示的话就是：</p><p><img src="3.png" /></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;最近有个合作idea，需要魔改llama3代码，所以来学习下transformers和llama3&lt;/p&gt;
&lt;p&gt;upd：已经变为我的知识库大杂烩了，将就着看吧，纯粹个人笔记了。&lt;/p&gt;</summary>
    
    
    
    <category term="1. 科研" scheme="http://error666.top/categories/1-%E7%A7%91%E7%A0%94/"/>
    
    <category term="LLM" scheme="http://error666.top/categories/1-%E7%A7%91%E7%A0%94/LLM/"/>
    
    
  </entry>
  
  <entry>
    <title>计算机网络自学笔记</title>
    <link href="http://error666.top/2024/06/22/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/"/>
    <id>http://error666.top/2024/06/22/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/</id>
    <published>2024-06-22T12:20:35.000Z</published>
    <updated>2024-12-31T17:27:05.271Z</updated>
    
    <content type="html"><![CDATA[<p>参考视频：<ahref="https://www.bilibili.com/video/BV1c4411d7jb?p=4&amp;spm_id_from=pageDriver&amp;vd_source=ca9a71bb3c1806ce48ae27d95e4e8bd0">计算机网络微课堂-湖科大教书匠</a></p><span id="more"></span><h3 id="概述">概述</h3><ul><li><p>RFC</p><ul><li>互联网的标准化工作对互联网的发展起到了非常重要的作用，那么标准的制定通过发表RFC(RequestFor Comments)的形式完成。</li><li>但并非所有的RFC文档都是互联网标准。RFC文档按发表时间先后编号，即RFCxxxx。</li></ul></li><li><p>C/S，P2P</p><ul><li>端系统之间有两种通信方式：客户/服务器方式（Client / Server方式）、对等方式（P2P）。</li><li>C/S方式描述的是进程之间服务和被服务的关系。客户是服务的请求方，服务器是服务的提供方。客户与服务器的通信是双向的，可互相接发文件。</li><li>C/S方式虽然逻辑简单，而且客户端不需要很复杂的操作。但是这样的话服务器端压力就很大，需要大算力和好硬件，而且还要一直运行被动等待客户的通信请求。因此，就诞生了P2P方式。</li><li>P2P就是通信时不区分服务请求方和服务，只要运行了P2P软件，都可以双向通信。本质其实就是每个主机既是一个客户又是一个服务器。</li></ul></li><li><p>电路交换，分组交换，报文交换</p><ol type="1"><li><strong>电路交换</strong><ul><li><p>定义：就是很多设备连接到交换机上，交换机感性理解就是有许多入口和出口，它可以指定某个入口的信号发送到某个出口，这样，多个设备就可以同时互不干扰的发送信号了。交换机又可以与交换机相连，扩大网络范围。</p></li><li><p>步骤：建立连接 --&gt; 传输信号 --&gt; 释放连接</p></li><li><p>优点：延迟小，传输数据有序；缺点：建立连接时间长，无法做到很多台主机之间同时相互通信(受限于通信线路数量)</p></li></ul></li><li><strong>报文交换</strong><ul><li>定义：直接把报文发送到交换机上，然后交换机存储转发给下一个交换机，一直到接收方收到报文为止。</li><li>优点：无需建立连接，动态分配线路；缺点：引入了转发时延，需要较大的缓存空间(报文可能很大)</li></ul></li><li><strong>分组交换</strong><ul><li><p>就是所谓的边缘部分和核心部分，发送方将报文发送到其所连网络中，通过路由器不断的转发，最终转发到接收方中。</p></li><li><p>步骤</p><ul><li>构造分组：先将报文划分成若干个等长的数据段，在每个数据段前加上元数据，这些元数据叫首部。</li><li>存储转发：路由器在拿到分组后，根据首部进行查表转发，找到合适的转发接口，然后转发给下一个路由器。</li><li>还原报文：接收方在收到分组后，去掉首部，将数据段组合还原出报文。</li></ul></li><li><p>优缺点</p><ul><li>优点：<ul><li>无需建立连接；</li><li>简化了存储管理（因为对报文进行了切片，所以路由器的缓存区只需固定即可，不论报文多大都可以切片为分组后转发出去）；</li><li>减小重发数据量（假设传输过程中出错了，报文交换就要重新发送整个报文，但分组交换只需重发出错的那个分组即可）；</li></ul></li><li>缺点<ul><li>引入了转发时延；</li><li>更多的元数据信息（切完片后每个分组都有首部）；</li><li>还原报文时复杂；</li></ul></li></ul></li></ul></li></ol></li><li><p>网络概念及分类（WAN，MAN，LAN，PAN）</p><ul><li>网络定义：网络并没有精确定义，一个认可度高的定义是：计算机网络主要是由一些通用的、可编程的硬件互连而成的，而这些硬件并非专门用来实现某一特定目的（例如，传送数据或视频信号）。这些可编程的硬件能够用来传送多种不同类型的数据，并能支持广泛的和日益增长的应用。</li><li>计网有几种分类方式：<ul><li>按网络作用范围分：<ul><li>广域网WAN：互联网的核心部分，距离从几十公里到几千公里不等。最好的例子就是互联网。</li><li>城域网MAN：作用范围是一个城市。</li><li>局域网LAN：作用范围局限在较小范围。例如公司网。</li><li>个人区域网PAN：范围在10m左右。例如蓝牙。</li></ul></li><li>按网络使用者分：<ul><li>公用网：按规定交钱的人都可以用的网络。</li><li>专用网：为特殊业务工作需要而建造的网络。</li></ul></li></ul></li></ul></li><li><p>性能指标（速率，带宽，吞吐量，时延，时延积带宽积，RTT，利用率）</p><ul><li><p>来来来，我们来解决下速率/带宽/吞吐量</p><ul><li>带宽说的是水管，某个水管单位时间内最多允许流过的水的量。吞吐量就是说此时此刻水管流过的水量。</li><li>所以带宽就是吞吐量的上限。</li><li>至于速率这个玩意儿，就是个人为定的数，可能是平均值，也可能是别的，whatever。</li><li>然后他们仨的单位是一样的，因为都是描述每秒流过的流量。bit/s(b/s或bps),kbit/s, Mbit/s, Gbit/s。从小到大差距<spanclass="math inline">\(10^3\)</span>量级。</li><li>然后这仨的关注点都是在发送速率上，所以它们影响的是发送时延。所以下列说法是错误的：“在高速链路（或高带宽链路）上，比特会传送得更快些”。</li></ul></li><li><p>时延/延迟：指一个数据（报文/比特/分组）从网络/链路的一端传到另一端所需的时间。</p><ul><li>组成：<ul><li>发送时延(传输时延)：<span class="math inline">\(发送时延 =\frac{数据帧长度}{发送速率}\)</span></li><li>传播时延：<span class="math inline">\(传播时延 =\frac{信道长度}{信号在信道上的传播速率}\)</span>。光纤是<spanclass="math inline">\(2.0 \times 10^5\text{km/s}\)</span>，光速是<spanclass="math inline">\(3.0 \times 10^5\text{km/s}\)</span>，铜线是<spanclass="math inline">\(2.3 \times 10^5\text{km/s}\)</span></li><li>处理时延：主机或路由器在收到分组时，为处理分组所花费的时间。</li><li>排队时延：分组在路由器输入输出队列中排队等待处理和转发所经历的时延。</li></ul></li></ul></li><li><p>时延带宽积：<span class="math inline">\(时延带宽积 = 传播时延\times带宽\)</span>。传播时延就是一个时间嘛，然后带宽表示单位时间内流过的最大流量。所以时延带宽积就是整个管道里充满的比特数。</p></li><li><p>RTT：表示从发送方发送完数据，到发送方收到来自接收方的确认总共经历的时间。</p><p><img src="image-20241226012606804.png" alt="" style="zoom:80%;" /></p></li><li><p>利用率：</p><ul><li>信道利用率：一个信道，有百分之几的时间是有数据通过的。</li><li>网络利用率：全网络信道利用率的加权平均。</li><li>排队论公式：<span class="math inline">\(D = \frac{D_0}{1 -U}\)</span><ul><li><span class="math inline">\(D\)</span>：网络当前时延</li><li><spanclass="math inline">\(D_0\)</span>：网络空闲的时延（其实就是没处理和排队时延）</li><li><span class="math inline">\(U\)</span>：网络利用率</li></ul></li></ul></li></ul></li><li><p>ISO、OSI/RM、TCP/IP</p><ul><li>ISO是一个国际标准化组织，OSI/RM是其提出的一种使全世界计算机在世界范围内互连的标准框架。</li><li>OSI/RM是一个抽象框架；ISO 7498标准即七层协议的体系结构。</li><li>但是OSI/RM标准太几把复杂了，所以TCP/IP标准得到了市场的认可。</li></ul></li><li><p>协议</p><ul><li>网络协议(协议)：是为进行网络中数据交换而建立的规则、标准或约定 /控制两个对等实体进行通信的规则的集合。<ul><li>三要素：<ul><li><strong>语法</strong>：数据与控制信息的结构或格式。</li><li><strong>语义</strong>：需要发出何种控制信息，完成何种动作以及做出何种响应。</li><li><strong>同步</strong>：事件实现顺序的详细说明。</li></ul></li></ul></li><li>协议数据单元PDU：对等层次之间传送的数据单位</li><li>服务数据单元SDU：层与层之间交换的数据的单位</li></ul></li><li><p>一些我觉得很好的图</p></li></ul><p>​<img src="image-20241226015116094.png" alt="" style="zoom: 50%;" /><img src="image-20241226015132604-1735666010001-3.png" alt="" style="zoom:45%;" /></p><p>图上的红字那几个就是每一层的PDU（协议数据单元）。</p><h3 id="物理层">物理层</h3><ul><li><p>四个特性（机械，电气，功能，过程）</p><ul><li>物理层的主要任务可描述为：确定与传输媒体的接口有关的一些特性。</li><li>所以就有了一些相关的特性：<ol type="1"><li><strong>机械特性</strong>：指明接口所用接线器的形状和尺寸、引脚数目和排列、固定和锁定装置等。</li><li><strong>电气特性</strong>：指明在接口电缆的各条线上出现的电压范围。</li><li><strong>功能特性</strong>：指明某条线上出现某一电平的电压意义。</li><li><strong>过程特性</strong>：指明对于不同功能的各种可能事件的出现顺序。</li></ol></li></ul></li><li><p>消息、数据、信号、码元</p><ul><li>消息：如话音、文字、图像、视频等。</li><li>数据：运送消息的实体。有意义的符号序列。</li><li>信号：数据的电气或电磁的表现。</li><li>码元：可以理解为一个脉冲信号，M进制码元可以携带<spanclass="math inline">\(\log_2 \text{M}\)</span>个bit信息。</li></ul></li><li><p>编码（不归零，归零，曼彻斯特，差分曼彻斯特）</p><ul><li><p>定义：把数字信号/模拟信号变为数字信号的过程叫编码。</p></li><li><p>不归零编码：就是010101直接传，缺点就是接受双方时钟频率必须一样。</p></li><li><p>归零编码：就是10(-1)(0)1010(01)0这样传，接收方只需在信号归零时进行采样即可，无需保证双方时钟频率必须一样。但是缺点就是一半的编码内容都给归零了，浪费资源。</p></li><li><p>曼彻斯特编码：同样是为了解决时钟问题，它用跳变去表示数据。</p></li><li><p>差分曼彻斯特编码：就是看相邻之间是否相同，相同为0，不相同为1。</p><p><img src="image-20241226023217280.png" alt="" style="zoom:67%;" /></p></li></ul></li><li><p>调制（调幅，调频，调相）</p><ul><li><p>定义：把数字信号/模拟信号变为模拟信号的过程叫做调制。</p></li><li><p>调幅(AM)：频率不变，通过振幅来区分信号。</p></li><li><p>调频(FM)：振幅不变，通过频率来区分信号。</p></li><li><p>调相(PM)：A/F都不变，通过相位来区分信号。</p><p><img src="image-20241226024338996.png" alt="" style="zoom: 33%;" /></p></li><li><p>这图调幅那里画的不太好，表示0不一定幅度是零，只要比高幅低就行。</p></li><li><p>正交振幅调制QAM：就是同时利用振幅和相位的不同去区分信号。</p><p><img src="image-20241226025221800.png" alt="" style="zoom: 67%;" /></p><ul><li>还好理解吧，反正就相位在波浪中间的自然表达不了那么高的幅度啊，但是波峰波谷的幅度上限高些，就有不同的幅度可选择。</li></ul></li></ul></li><li><p>奈奎斯特定理（奈氏准则），香农公式</p><ul><li>信号在信道传输时，难免会失真。所以这两个公式就是计算在不失真的前提下，信道的极限容量是多少。</li><li>奈氏准则：<ul><li>理想低通信道的最高码元传输速率(波特率/调制速率) = <spanclass="math inline">\(2 \times \text{W 码元/s}\)</span>，<spanclass="math inline">\(\text{W}\)</span>是信道带宽，单位为Hz。</li><li>理想带通信道的最高码元传输速率(波特率/调制速率) = <spanclass="math inline">\(\text{W 码元/s}\)</span>。</li><li>m进制码元 = <span class="math inline">\(\log_2m\)</span>bit，所以对于m进制码元，码元/s = <spanclass="math inline">\(\log_2m\)</span> bit/s</li></ul></li><li>香农公式（适用带噪声情况）：<ul><li><span class="math inline">\(c = W \times \log_2(1 +\frac{S}{N})\)</span><ul><li><spanclass="math inline">\(c\)</span>：信道极限信息传输速率，单位b/s。</li><li><span class="math inline">\(W\)</span>：信道带宽，单位Hz。</li><li><spanclass="math inline">\(\frac{S}{N}\)</span>：信噪比，但是如果用dB去度量，那么1dB的信噪比 = <span class="math inline">\(10 \times\log_{10}\frac{S}{N}\)</span> dB</li></ul></li></ul></li></ul></li><li><p>传输介质</p><ul><li>定义：数据传输系统中在发送器和接收器之间的物理通路。</li><li>引导型：双绞线，同轴电缆，光纤。</li><li>非引导型：无线电微波，卫星。</li></ul></li><li><p>信道复用技术</p><ul><li>有时候信道没那么多，但是要传播的信号居多，那么就要采用信道复用技术。</li><li>频分复用FDM：所有用户同时占用不同的频带资源并行通信。</li><li>时分复用TDM：所有用户在不同时间占用同样的频带宽度。（这样信道利用率低，因此有了变种统计时分复用STDM）</li><li>波分复用WDM：本质就是光的FDM，使用一根光纤来同时传输多个光载波信号。</li><li>码分复用CDM：<ul><li>每个用户有一个唯一的码型，也称mbit位码片序列。（通常写作一个向量）</li><li>这些码型要满足俩俩正交（这个若满足那么<spanclass="math inline">\(ST=S\overline{T}=\overline{S}T=\overline{S}\overline{T}=\textbf{0}\)</span>），自己的平方要为1，自己与自己反码点积要为-1。</li><li>在某一时刻，若一个用户想发送1，就发一个码型出去；若想发送0，就发一个反码型出去。然后m个用户在这一时刻发送的码型/反码型直接向量叠加，发送过去。到那边想看谁的信号就拿叠加信号点乘那个用户的码型。<ul><li>若&lt;0说明它发了一个0。</li><li>若&gt;0说明它发了一个1。</li><li>若=0说明它啥也没发。</li></ul></li></ul></li></ul></li><li><p>同步光纤网SONET，同步数字系列SDH</p><ul><li>SONET：同步光纤网络、同步时钟、光纤</li><li>SDH：同步数字系列、可以同步也可以异</li></ul></li><li><p>ADSL，光纤同轴混合网（HFC网），FTTx 技术</p><ul><li>ADSL：利用你家的电话线上网，就是三个信号电话信号、下载(下行)信号、上传(上行)信号，用频分复用的方式在电话线上传输。然后下载速度是远大于上传速度的。</li><li>HFC：三网融合过渡的一个产物，就是把部分主干网换为光纤。</li><li>FTTx：就是单独拉一根光纤到你家作为网线。</li></ul></li></ul><h3 id="数据链路层">数据链路层</h3><ul><li><p>局域网协议把OSI数据链路层分为MAC子层(介质访问控制层)和LLC子层(逻辑链路控制层)。LLC基本已经消失了，主要是 MAC 协议。</p></li><li><p>CRC</p><ul><li>数据链路层的传输单元是帧，帧在传输过程中可能出现误码（0和1的互相转变），所以这时候引入检错码来知道传输过程中是否出现了误码。</li><li>具体来说，就是将上层网络层数据单元加上帧头帧尾成为帧，让其在数据链路层上传输，检错码就在帧尾里。</li><li>差错检测：指帧在传输中可能出现误码，接收方可以通过帧尾里的检错码来知道传输过程中是否出现了误码。<ul><li>传输错误的比特占传输比特总数的比率称为误码率BER。</li></ul></li><li>可靠传输：指尽管误码是不能完全避免的，但若能实现发送方发送什么，接收方就能收到什么，就称为可靠传输。</li><li>CRC循环冗余校验：<ul><li>接受双方约定好一个多项式，例如：<span class="math inline">\(G(x) = 1\cdot x^3 + 1 \cdot x^2 + 0 \cdot x + 1 \cdot1\)</span>，然后假设要发送的信息是101001，那么第一步首先是在发送信息的后面补最高次项的次数那么多个0，也就是补3个0，即101001000（将这个数称为被除数）。这么做是为了避免不够除（后面你就知道了）。然后提取多项式的系数作为除数。即：1101。然后101001000与1101做除法，得到的余数为1，但是要补前导0直到位数与最高次数相同。</li><li>然后将余数 拼接 被除数，也就是把余数粘到发送信息的后面101001001，然后就可以发送了。</li><li>接收方接收到这个数字后，会拿其对除数做除法，如果余数为0就没出差错。如果不为0就说明传输出现了错误。这很好理解。我们算出来的是余数，然后我们余数+ 被除数传去过，那么没出错的话显然就是可以整除除数的。</li></ul></li></ul></li><li><p>PPP</p><ul><li><p>PPP协议是目前使用最广泛的点对点数据链路层协议。</p></li><li><p>PPP协议封装的数据链路帧 = 首部(5字节) + 数据 +尾部(3字节)。</p></li><li><p>链路层协议规定了帧中数据部分的长度上限——最大传送单元（MTU）</p></li><li><p>组成：</p><ol type="1"><li>一个将IP数据报封装到串行链路的方法。</li><li>一个链路控制协议LCP。</li><li>一套网络控制协议NCP</li></ol><p><img src="image-20241226124122301.png" alt="" style="zoom:50%;" /></p></li><li><p>但是数据部分出现帧定界符怎么办？</p><ul><li><p>PPP在同步传输链路时，采用零比特填充法。在异步传输时，采用字节填充法。</p></li><li><p>0x7E就是01111110，所以零比特填充法就是在数据里连续的5个1后面差一个0，这样就不会有0x7E的出现了。</p></li><li><p>字节填充法看下面的图片吧：</p><p><img src="image-20241228163225670.png" alt="" style="zoom:67%;" /></p></li></ul></li></ul></li><li><p>CSMA/CD，争用期，最小帧长，退避算法</p><ul><li><p>CSMA/CD：载波监听多点接入 / 碰撞检测。</p></li><li><p>工作流程：</p><p><img src="image-20241226135819801.png" alt="" style="zoom: 50%;" /></p></li><li><p>争用期(碰撞窗口)：以太网的端到端往返时延<spanclass="math inline">\(2\tau\)</span>。</p></li><li><p>最短帧长：最短能满足“不会出现碰撞反馈收不到“ 的帧长。</p><ul><li><span class="math inline">\(\frac{帧长}{数据传输速率} \ge\frac{距离}{信息传输速率} \times 2\)</span></li><li>很容易理解，尿断了就不能导电了。</li></ul></li><li><p>退避算法（截断二进制指数退避）：</p><ul><li>基本退避时间：<span class="math inline">\(2\tau\)</span>。</li><li>从整数集合 <span class="math inline">\(\{0, 1, \cdots, 2^k -1\}\)</span> 中随机地取出一个数，记为 r。重传所需的时延 = r ⅹ基本退避时间。</li><li>参数 <span class="math inline">\(k = \min(重传次数,10)\)</span></li><li>当重传达 16 次仍不能成功时即丢弃该帧，并向高层报告。</li></ul></li></ul></li><li><p>MAC 地址</p><ul><li><p>定义：就是在数据链路上有很多主机嘛，所以每个主机都需要有一个自身的标识。MAC地址一般被固化在网卡中，所以MAC地址也被称为硬件地址或者物理地址（但这不意味着它属于物理层）。</p></li><li><p>MAC地址通常遵循IEEE802格式，即将每4个bit写为一个十六进制，共12个十六进制字符，写为：XX-XX-XX-XX-XX-XX或者XX:XX:XX:XX:XX:XX或XXXX.XXXX.XXXX</p><p><img src="image-20241226141809405.png" alt="" style="zoom:67%;" /></p></li><li><p>跟PPP标准协议的帧不同，也可以玩意叫MAC帧：</p><p><img src="image-20241226142122816.png" alt="" style="zoom: 67%;" /></p></li></ul></li><li><p>扩展以太网</p><ul><li>物理层扩展以太网：集线器。一个集合就是一个碰撞域(冲突域)，也是一个广播域。</li><li>数据链路层扩展以太网：<ul><li>使用交换机。交换机的每一个接口就是一个碰撞域。（因为数据只要传到交换机了就不会有什么碰撞了），所有接口是一个广播域。交换机就是多端口的网桥。</li><li>交换机的原理：交换机里有一个帧交换表，里面记录了每个接口连的设备的MAC地址。所以如果使用交换机的话，比如主机H1想传东西给H2，那么它就先把信息传到交换机，然后交换机就去查找哪个接口是H2的MAC地址，然后把信息从那个接口传过去。<ul><li>这个帧交换表怎么来的？通过自学习算法得到。挺脑残的，就是若有一条信息进入交换机，交换机就会在表中登记这条信息来自于哪个端口，其对应发送者的MAC地址是啥，这是更新阶段；对于查找阶段，如果表里有记录就按照记录走，没记录都每个端口都发送（泛洪）。随着时间的推移，这个表必然就会逐渐填好。</li><li>但这种泛洪的做法弊端很大啊，试想假设多个交换机连接，表一开始都是空的，A给B发信息，然后去泛洪，那么对于一个交换机内，必然会记录多个端口的信息来源是A。那么下次B给A发的时候，就会给这些端口都去发。总之就是多了非常多冗余的信息，因为其实上我们只关心传的速度最快的那个端口的信息来源。</li><li>解决方法就是SPT生成树协议，就是把网络拓扑改为一棵树，that'sall.</li></ul></li></ul></li></ul></li><li><p>802.1Q、VLAN</p><ul><li><p>先要知道广播域，这东西跟碰撞域不一样。广播域意思是只要能通信，那么就要一个域里。</p></li><li><p>广播域里一个严重问题就是广播风暴。跟前面为了用SPT一样，解决广播风暴的手段就是切割广播域。</p></li><li><p>可以用路由器去切，也可以用VLAN去切。VLAN是针对局域网LAN的一项技术。</p></li><li><p>虚拟局域网VLAN技术是在交换机上实现的，所以需要交换机有能够处理带有VLAN标记的帧——IEEE802.1 Q帧，来个对比图吧：</p><p><img src="image-20241226145717390.png" alt="" style="zoom: 80%;" /></p></li></ul></li></ul><h3 id="网络层">网络层</h3><ul><li><p>网络层的两种设计思路：</p><ul><li>要可靠！<ul><li>虚电路。即通信之间先建立逻辑上的虚电路连接，保证双方通信所需的一切网络资源。如果再叠上可靠传输的网络协议，那么可使所发送的分组无差错按序到达终点，不丢失、不重复。</li></ul></li><li>要简单！<ul><li>数据报。每一个分组（即 IP数据报）独立发送，与其前后的分组无关（不进行编号）。网络层不提供服务质量的承诺。即所传送的分组可能出错、丢失、重复和失序（不按序到达终点），也不保证分组传送的时限。</li></ul></li></ul></li><li><p>网络层的两个层面：</p><ul><li>控制层面：根据路由选择协议所用的路由算法计算路由，创建出本路由器的路由表。</li><li>数据层面：路由器根据本路由器生成的转发表，把收到的分组从查找到的对应接口转发出去。<ul><li>Well,这里可以引用SDN，因为控制层面的目的无非就是生成路由表。那么如果有一个中心计算器去统观全局，然后给每一个路由器分配路由表，这是很好的。但我觉得强化学习也可以引入进来，形成分布式自适应计算路由，调度网络流量。就像用一辆小车的强化学习去协调整个道路的车流量那个paper一样，我觉得同样可以抽取部分网络中的路由器，作为强化学习路由器，由它们去调控整个网络的流量运转。</li></ul></li></ul></li><li><p><strong>互联设备</strong></p><p><img src="image-20241226151736498.png" alt="" style="zoom:67%;" /></p><ul><li>物理层和数据链路层的互联设备都不称为网络互连，它们仅仅是把网络扩大了。</li><li>但路由器就是连接网络跟网络，这很闭环，因为我们知道路由器就是为了切割广播域的，而广播域对应的范围就是网络。这点要跟冲突域区别。<ul><li><strong>物理层：</strong>中继器对数据信号的重新发送或者转发，来扩大网络传输的距离。集线器对接收到的信号进行再生整形放大，以扩大网络的传输距离，同时把所有节点集中在以它为中心的节点上。</li><li><strong>数据链路层：</strong>网桥、交换机按每一个包中的MAC地址相对简单地决策信息转发。</li><li><strong>网络层：</strong>路由器实现不同网络之间的互连和路由选择以及分组转发。</li><li><strong>运输层、应用层：</strong>网关用于两个高层协议不同的网络互连。</li></ul></li></ul></li><li><p>IP地址</p><ul><li><p>在TCP/IP体系中，IP地址是一个最基本的概念。互联网上的每台主机（或路由器）的每个接口分配一个在全世界唯一的IP地址。网络层以上都用ip地址去标识，底下都用mac地址去标识。相当于ip是网络层写的一个接口。</p></li><li><p>IP地址是32位，但是为了方便，采用点分十进制表示方法进行表示。</p></li><li><p>IP地址 = 网络号 + 主机号。</p></li><li><p>IP地址的分类</p><p><img src="image-20241226171629160.png" alt="" style="zoom: 67%;" /></p><ul><li>A类最大可指派的网络数是<span class="math inline">\(2^7 -2\)</span>，网络号0和127是保留地址。</li><li>B类最大可指派的网络数是<span class="math inline">\(2^{14} -1\)</span>，网络号128.0是保留，采用无分类编址时可指派。</li><li>C类最大可指派的网络数是<span class="math inline">\(2^{21} -1\)</span>，网络号192.0.0是保留，采用无分类编址可指派。</li><li>对于A/B/C类，主机号全零或全一都是不行的。（全零是网络地址，下面的子网你就理解了）</li></ul></li></ul></li><li><p>子网</p><ul><li>子网其实就是三级结构：网络号 + 子网号 + 主机号。</li><li>子网号就是抢主机号的bit位，咋知道子网号有多少位呢？看子网掩码，子网掩码就是若干个1+若干个0。</li><li>首先根据ip的第一个十进制看看他是哪一个网络，知道它是哪一类网络后就可以知道它的网络号就多少位。拿其子网掩码位数- 网络号位数就是子网号的位置。能划分的子网个数就是<spanclass="math inline">\(2^{子网号位数}\)</span>，被划分了一部分位数的主机号就是用来分配的主机号。同样的，全0是网络地址，全1是广播地址。</li></ul></li><li><p>CIDR</p><ul><li>CIDR是无分类编址的IPv4地址。也就是没有什么A/B/C/D/E分类了，直接在ip地址后加一个/num表示前num位是网络号，后面的是主机号。网络前缀相同的连续ip地址组成一个“CIDR地址块”。</li><li>所以，这种特性使得我们只要知道CIDR地址块中任何一个地址，就可以知道该地址块的全部细节。例如地址块的最小/最大地址、地址块中的地址数量、地址掩码（子网掩码）。</li><li>而且这样有一个好处，对于多个CIDRip地址，我只需要找它们的最长公共前缀，这样路由表就只需要记录一条记录了。外界传信息会先传到路由表，然后路由表再泛洪下去。这种方法叫路由聚合，也叫构造超网。很聪明的设计。</li></ul></li><li><p>ARP</p><ul><li>ARP就是地址解析协议，就是已知一个主机的ip，如何找出其mac地址。</li><li>其实很简单，ARP就是张表，比如H1想给R1发东西，他就需要知道R1的MAC和IP，IP已经知道了，MAC就去查自己的ARP表，里头记录了所有设备的IP和MAC的对应关系，如果找到了就ok了。如果没找到，就先告诉一个“ARP广播”，相当于去网络上请求别人的IP-MAC映射关系，请求到了就也ok了。但需要注意的是，ARP广播只能在一段链路或者是一个网络上进行。</li><li>所以如果跨网络通信，就需要依靠路由器。具体来说，首先电脑发现目标ip跟自己ip不在同一网段，所以会偷偷把目标ip改为默认网关，也就是网线连的那个路由器的端口的ip，然后发送一个广播arp请求。路由收到arp请求后，将返回一个arp响应给电脑。电脑现在有了默认网关的mac了，然后目标ip设为目标电脑ip，mac设为默认网关mac，然后封装为mac帧，传过去。此时，路由器收到的数据包就是目标ip是目标电脑。相当于数据从本电脑传到了路由器，那么递归的去做数据就到目标电脑了。</li></ul></li><li><p>IP数据报格式</p><ul><li>IP数据报由首部和数据两部分组成，首部分为固定部分(20字节)+可变部分。</li><li>源地址和目的地址就写在首部里面。</li></ul></li><li><p>转发分组过程</p><ul><li><p>其实就是路由聚合那里的策略，就是先拿目标ip and一下自己的子网掩码，看看目标ip在不在自己网段里，不在的话，就将目标ip发到路由器那，挨个与路由器的记录也就是每一个最长公共前缀去and ，如果匹配了，就转发到那个接口。</p><p><img src="image-20241226221154246.png" alt="" style="zoom:67%;" /></p></li></ul></li><li><p>ICMP</p><ul><li>ICMP就是IP数据报，只不过数据换成了ICMP内容。</li><li>分类：<ul><li>只有以下几类出错时会返回ICMP数据报，其它原因例如IP数据报头部校验出错，那么就直接丢弃。</li><li>差错报告报文：终点不可达、源点抑制、时间超过（用的首部的TTL实现的这一功能）、参数问题、改变路由（重定向）</li><li>询问报文：回答请求和回答、时间戳请求和回答</li></ul></li><li>应用：ping是用的询问报文，traceroute是用的时间超过+终点不可达(故意弄了个超大的端口)</li></ul></li><li><p>IPv6</p><ul><li><p>从32位扩展到128位。</p></li><li><p>IPv6数据报 = 基本首部 + 有效载荷。</p></li><li><p>首部必须是8字节的整数倍，基本首部是40字节，可以在有效载荷里弄出一些空间作为扩充首部。</p></li><li><p>IPv6地址使用冒号十六进制记法，即4个十六进制为一组，共8组，组与组之间用: 隔开。可以使用零压缩。</p><p><img src="image-20241226224524973.png" alt="" style="zoom:67%;" /></p></li></ul></li><li><p>路由协议：RIP、OSPF、BGP</p><ul><li><p>路由协议就是说的是网络层的控制层面。也就是我前面说自己idea那里。</p></li><li><p>路由选择有两种方式：静态路由选择（人工配置）、动态路由选择（通过路由选择协议自动获取路由信息）</p></li><li><p>但一般我们都是用的分层次的路由选择协议，即将互联网划分为一个个自治系统AS，然后系统内部用内部网关协议IGP，系统之间用外部网关协议EGP。</p></li><li><p>路由选择协议分类：</p><ul><li><p>内部网关协议IGP</p><ul><li><p>路由信息RIP（基于距离向量），基于UDP的域内路由协议。工作协议层次是应用层。</p><p><img src="image-20241226230846048.png" alt="" style="zoom: 67%;" /></p></li><li><p>开放式最短路径优先OSPF（基于链路状态）</p><ul><li>基于IP的域内路由协议，工作协议层次是网络层。</li><li>OSPF只在链路状态发生变化时，才向本自治系统中的所有路由器用洪泛法发送与本路由器相邻的所有路由器的链路状态信息。链路状态指明本路由器和哪些路由器相邻，以及该链路的度量（度量可表示费用、距离、时延、带宽等），所有的路由器最终都能建立一个全网的拓扑结构图。</li></ul></li></ul></li><li><p>外部网关EGP</p><ul><li>边界网关协议BGP（基于路径向量），基于TCP的域间路由协议，<ul><li>BGP力求寻找一条能够到达目的网络（可达）且比较好的路由（不兜圈子），而非寻找最佳路由</li></ul></li></ul></li></ul></li></ul></li><li><p>IP多播、IGMP报文、多播路由选择</p><ul><li><p>IP多播</p><ul><li><p>在一对多的通信中，多播可以比单播节省很多资源。</p><p>局域网具有硬件多播功能，所以不需要复制分组就能使所有的多播组成员收到分组。</p><p>IP 多播所传送的分组需要使用多播 IP 地址。在传统的 IP 地址中的 D类地址就是多播地址，每个 D 类地址可以标识一个多播组。</p><p>多台主机可以加入到一个多播组中共享一个多播地址。不同网络的主机可以加入到同一个多播组中。每一台主机可以随时加入或离开一个多播组。</p><p>能够运行多播协议的路由器为多播路由器。</p><p>多播地址只能用于目的地址，不能用于源地址。</p><p>IP多播有两种：在本局域网上硬件多播、在互联网范围内进行多播。</p></li></ul></li><li><p>IGMP报文就是IP数据报，只不过数据换成了IGMP内容。它用于让连接在本地局域网上的多播路由器知道本局域网上是否有主机参加或退出了某个多播组。</p></li><li><p>多播路由选择</p><ul><li>洪泛与剪除</li><li>隧道技术</li><li>基于核心的发现技术</li></ul></li></ul></li><li><p>VPN、NAT</p><ul><li><p>VPN：</p><p>因为 IP地址的紧缺。所以现在使用了一种本地地址。本地地址仅在本机构内部有效，不是全球唯一的地址，又称可重用地址。</p><p>本地地址只能用于一个机构的内部通信，不能和互联网行的主机通信。互联网中的所有路由器对目的地址是专用地址的数据报一律不转发。</p><p>有时一个机构的分布范围很广，就需要用公共的互联网作为本机构各专用网之间的通信载体，这样的称为虚拟专用网VPN。</p><p>VPN依然只用于机构内的通信，但是要经过公用的互联网，通过互联网传送的数据都要加密。这里使用了隧道技术。</p><p>VPN 中每个不同的场所必须至少有一个合法的全球 IP 地址。</p></li><li><p>NAT：</p><p>网络地址转换 NAT 用于实现专用网中的主机到互联网上的主机的通信。</p><p>它需要在专用网连接到互联网的路由器上安装 NAT 软件，这种路由器称为 NAT路由器，NAT 路由器至少有一个全球地址。</p><p>使用本地地址的主机和外界通信时要在 NAT路由器上将本地地址转换为全球地址。</p><p>NAT路由器中有一个地址转换表，存储本地地址与转换后的全球地址的对应关系。</p><p>通过 NAT路由器的通信必须由专用网内的主机发起，因此专用网内的主机不能作为服务器。</p><p>现在的 NAT 转换表把端口号也利用上了。这样 NAT路由器只需要有一个全球地址，通过给具有不同本地地址的主机分配不同的端口号就可以实现内部多个主机与外界互联网的通信。</p></li></ul></li><li><p>MPLS：一种 IP 增强技术。</p></li><li><p>SDN：是一个体系结构，是一种设计、构建和管理网络的新方法或新概念。核心思想把控制层面和数据层面分离，而让控制层面利用软件控制数据层面中的许多设备。</p></li></ul><h3 id="运输层">运输层</h3><ul><li><p>概念</p><ul><li>之前的物理层、数据链路层、网络层共同解决实现了主机到主机的通信。</li><li>但通信真正的实体在通信两端主机中的进程。如何为运行在不同主机上的应用进程提供直接的通信服务是运输层的任务。</li><li>运输层为应用层提供了两种不同的运输协议，面向连接的TCP(传输控制协议)（全双工可靠信道）和无连接的UDP(用户数据协议)（不可靠信道）。</li><li>两个对等运输实体在通信时传送的数据单位叫作运输协议数据单元 TPDU；TCP传送的数据单位协议是 TCP 报文段；UDP 传送的数据单位协议是 UDP报文或用户数据报。</li><li>复用：应用进程都可以通过运输层再传送到网络层。</li><li>分用：运输层从网络层收到发送给应用进程的数据后，必须分别交付给指明的各应用进程。</li><li>用端口来区分不同的进程，16位。0<sub>1023是熟知端口，1024</sub>49151是登记端口，49152~65535是短暂端口。</li><li>UDP用户数据报和TCP报文段的格式都是首部+数据。UDP的首部仅8字节，TCP报文段首部最小20字节，最大60字节。</li><li>UDP支持一对一、一对多、多对一、多对多。TCP仅支持一对一。</li><li>UDP对应用层交付的报文直接打包。TCP面向字节流。</li><li>UDP不使用流量控制和拥塞控制。TCP使用流量控制和拥塞控制。</li><li>TCP连接的端点是套接字socket，socket=(IP地址 :端口号)，TCP连接就是协议软件所提供的一种抽象。</li></ul></li><li><p>可靠传输的两种协议</p><ul><li>停止等待协议：每发送完一个分组就停止发送，等待对方的确认。在收到确认后再发送下一个分组。收不到就超时重传。</li><li>ARQ协议：<ul><li>停止-等待ARQ：</li><li>连续ARQ：窗口大小<span class="math inline">\(2^k - 1\)</span>。</li><li>选择重发ARQ：窗口大小<spanclass="math inline">\(2^{k-1}\)</span>，k是用来表示序号最大值所需的bit数。比如序号最大是7（从0开始），那么k=3。</li><li>回退N帧ARQ：窗口大小<span class="math inline">\(2^k -1\)</span></li></ul></li></ul></li><li><p>TCP 报文段的首部格式</p><p><img src="image-20241227100618025.png" alt="" style="zoom:80%;" /></p><ul><li>序号：在一个 TCP连接中传送的字节流中的每一个字节都要按顺序编号。首部中的序号字段存储本报文段发送的数据的第一个字节的序号。序号字段只有32 位，序号值不能超过 2^32。</li><li>确认号：首部中的确认号是期望收到对方下一个报文端的第一个数据字节的序号。若确认号为N，表明到 N-1为止的数据都已正确收到。</li><li>数据偏移：数据部分距报文起始点的偏移，实际等于首部长度。首部长度在20~60 字节之间。</li><li>6个控制位：<ul><li>紧急 URG：当 URG=1，此报文段需要尽快传送，优先级高。</li><li>确认 ACK：当 ACK=1，确认号字段有效。连接建立后的所有报文段都必须令ACK=1。</li><li>推送 PSH：当PSH=1，接收方收到报文后尽快交付应用进程，而非等缓存满了再交付。</li><li>复位 RST：当 RST=1，表明 TCP连接中出现严重差错，需要释放连接再重新建立连接。</li><li>同步 SYN：当 SYN=1，表明这个一个连接报文。如果 ACK=0则是连接请求报文，如果 ACK=1 表明这是连接接受报文。</li><li>终止 FIN：当 FIN=1，表明发送方已发送完数据，并要求释放连接。</li></ul></li><li>窗口：发送本报文段的一方的接收窗口大小。</li><li>检验和：检验整个数据报。</li><li>紧急指针：当 URG=1时才有意义，指出本报文段中的紧急数据的字节数。即使在窗口为 0时也可以发送紧急数据。</li><li>选项：长度可，变最长 40 字节。选项有最大报文段长度MSS、窗口扩大选项、时间戳选项、选择确认选项等。</li></ul></li><li><p>TCP 的流量控制</p><ul><li><p>TCP接收方利用自己的接收窗口的大小来限制发送方发送窗口的大小。所以本质上这功能是由接收方执行的。</p></li><li><p>TCP发送方收到接收方的零窗口通知后(接收方没缓存空间了)，应启动持续计时器。持续计时器超时后，向接收方发送零窗口探测报文。</p></li><li><p>不管是普通的TCP报文段还是零窗口探测报文，都有超时重传机制。超时重传时间RTO计算公式如下：</p><p><img src="image-20241227102909477.png" alt="" style="zoom: 50%;" /></p></li></ul></li><li><p>TCP 的拥塞控制</p><ul><li><p>慢开始、拥塞避免</p><p><img src="image-20241227103724058.png" alt="" style="zoom: 50%;" /></p></li><li><p>快重传、快恢复</p><ul><li>有时个别报文段会在网络中丢失，但实际上网络并未发生拥塞。这将导致放松方超时重传，并误认为网络发生了拥塞。所谓快重传，就是使发送方尽快进行重传，即每次收到报文都确认一下。若发送方一旦收到3个连续的重复确认，就将相应报文段立即重传，并将ssthresh设置为cwnd的一半，然后cwnd也自己更新为自己的一半。</li></ul><p><img src="image-20241227104404150.png" alt="" style="zoom: 50%;" /></p></li></ul></li><li><p>TCP 的运输连接管理——TCP连接建立</p><p><img src="image-20241227110342394.png" alt="" style="zoom: 50%;" /></p><ul><li>很好理解吧，首先请求建立连接也就是先两次握手，就先把SYN开了。然后ACK就自从第二次握手开始就一直有的。</li><li>seq就是当前要传数据的第一个字节序号，但是在握手阶段不涉及数据，所以TCP/IP协议强制规定SYS=1的报文段强制消耗掉一个序号，普通的确认报文若不携带数据，则不消耗序号。所以这就是为什么第三次握手的seq是x+1。</li><li>ack就是你期望收到对面下一个数据的序号。</li><li>第三次握手的原因是为了避免防止已失效的连接请求报文段又突然传送到服务器，因而导致错误。</li></ul></li><li><p>TCP 的运输连接管理——TCP连接释放</p><p><img src="image-20241227112418813.png" alt="" style="zoom: 50%;" /></p><ul><li>自己走一遍就行了。把握手的SYN换为了FIN，然后ACK是一直都有的。seq和ack就正常进行。要等待2MSL是因为如果最后一次握手丢失，若没有时间等待将无法重传从而导致服务器关闭不掉。</li><li>另外，除了这种正常的释放方式，还可能会通过保活计时器关闭连接。即TCP服务器进程每收到一次TCP客户进程的数据，就重新设置并启动保活计时器，若保活计时器定时周期内未收到TCP客户进程发来的数据，则当保活计时器到时后，TCP服务器进程就向TCP客户进程发送一个探测报文段，以后则每隔75秒钟发送一次。若一连发送10个探测报文段后仍无TCP客户进程的响应，TCP服务器进程就认为TCP客户进程所在主机出了故障，接着就关闭这个连接。</li></ul></li></ul><h3 id="应用层">应用层</h3><ul><li><p>DNS</p><ul><li><p>基于UDP的。</p></li><li><p>互联网使用的命名系统。用来把人们使用的机器名字（域名）转换为 IP地址。</p></li><li><p>域名采用层次树状结构的命名方法：xxx.xxx.xxx.xxx。</p></li><li><p>DNS 是一个联机分布式数据库系统，采用客户-服务器方式。</p></li><li><p>域名到 IP 地址的解析是由若干个域名服务器共同完成。</p><p><img src="image-20241227230506217.png" alt="" style="zoom:50%;" /></p></li><li><p>域名服务器类型</p><ul><li>根域名服务器<ul><li>所有根域名服务器都知道所有的顶级域名服务器的域名和 IP地址。不管是哪一个本地域名服务器，若要对互联网上任何一个域名进行解析，只要自己无法解析，就首先求助于根域名服务器。根域名服务器共有13 套装置。</li></ul></li><li>顶级域名服务器<ul><li>顶级域名服务器（TLD服务器）负责管理在该顶级域名服务器注册的所有二级域名。（一般.com的到这里就解析完毕了）</li></ul></li><li>权限域名服务器<ul><li>负责一个区的域名服务器。</li></ul></li><li>本地域名服务器<ul><li>本地域名服务器有时也称为默认域名服务器。当所要查询的主机也属于同一个本地ISP 时，该本地域名服务器立即就能将所查询的主机名转换为它的 IP地址，而不需要再去询问其他的域名服务器。</li></ul></li></ul></li><li><p>域名的解析过程</p><p><img src="image-20241227231807062-1735665966108-1.png" alt="" style="zoom:50%;" /><img src="image-20241227231820241.png" alt="" style="zoom:50%;" /></p></li></ul></li><li><p>FTP、TFTP</p><ul><li><p>文件传送协议 FTP是互联网上使用得最广泛的文件传送协议。只提供文件传送的一些基本服务，它使用TCP 可靠的运输服务。使用客户服务器方式。一个 FTP服务器进程可通过开多个进程同时为多个客户进程提供服务。</p></li><li><p>FTP客户和服务器之间要建立以下两个并行的TCP连接：控制连接、数据连接。</p></li><li><p>默认情况下，FTP使用TCP 21端口进行控制连接，TCP20端口进行数据连接。但是，是否使用TCP20端口建立数据连接与传输模式有关，主动方式使用TCP20端口，被动方式由服务器和客户端自行协商决定。</p><p><img src="image-20241227232741464.png" alt="" style="zoom: 67%;" /></p></li><li><p>简单文件传送协议 TFTP 使用客户服务器方式和使用 UDP 数据报，因此TFTP 需要有自己的差错改正措施。UDP 熟知端口号码为 69。TFTP采用了类似停止等待协议的重传机制。即发送后就等待确认，没有确认就重传。</p></li></ul></li><li><p>TELNET：远程终端协议 TELNET采用客户服务器模型。能将客户端的操作传到服务器端，然后将服务器端的输出返回到客户端屏幕。TELNET采用 TCP 协议。TELNET 的服务器进程类似FTP，由主进程等待新的请求，并产生从属进程来处理每一个连接。</p></li><li><p>WWW、URL、HTTP、HTML</p><ul><li><p>Web 要处理的几个问题及解决方式：</p><ul><li><p>如何标志分布在整个互联网上的文档：采用统一资源定位符 URL。</p><ul><li>URL 的一般形式：<协议>://<主机>:<端口>/<路径></li><li>协议有例如http。端口通常都省略掉，HTTP 的默认端口号是 80。</li></ul></li><li><p>用什么协议来实现万维网上的链接：采用超文本传送协议HTTP。</p></li><li><p>怎么实现创作不同风格的万维网文档：使用超文本标记语言HTML。</p></li></ul></li><li><p>HTTP协议</p><ul><li><p>HTTP协议是无状态的，为了使其有记忆功能必须引入cookie。</p></li><li><p>HTTP超文本传输协议定义了浏览器怎样向万维网服务器请求万维网文档，以及万维网服务器怎样把万维网文档传送给浏览器。HTTP是面向事务的应用层协议。</p></li><li><p>HTTP/1.0采用非持续连接</p><p><img src="image-20241227234943089.png" alt="" style="zoom:67%;" /></p><p>​ well,可以发现第一条请求报文都捎在第三次握手TCP连接确认那里。所以是带有数据的，所以必然是要消耗掉一个序号的。</p></li><li><p>HTTP/1.1采用持续连接方式</p></li><li><p>HTTP的报文格式</p><p><img src="image-20241227235413148.png" alt="" style="zoom:67%;" /></p></li><li><p>代理服务器：代理服务器又称万维网高速缓存，它把最近的一些请求和响应暂存在本地磁盘中。当新请求与暂存的请求相同，就返回暂存的响应。代理服务器可以在客户端或服务端工作，也可以在中间系统上工作。</p></li></ul></li></ul></li><li><p>STMP、POP3、IMAP、MIME</p><ul><li>简单邮件发送协议：SMTP。</li><li>邮件读取协议：POP3 和 IMAP。</li><li>通用互联网邮件扩充 MIME。</li><li>SMTP 和 POP3（或 IMAP）都使用 TCP 连接可靠地传送邮件。</li><li>电子邮件系统采用客户/服务器方式，三个主要组成构件：用户代理、邮件服务器、电子邮件协议。</li><li>SMTP基于TCP连接，端口号25。只能传送ASCII码，为了解决这个问题，提出了多用途因特网邮件扩展MIME。</li><li>POP3只能下载不能管理。IMAP可以管理。它们俩都是基于TCP。POP3的端口为110，IMAP4的端口为143。</li><li>基于web的电子邮件：这种工作模式在用户浏览器与邮件服务器网站之间使用http协议，而邮件服务器之间使用SMTP协议。</li></ul></li><li><p>DHCP</p><ul><li><p>动态主机配置协议 DHCP 允许一台计算机加入网络和获取 IP地址，而不用手工配置。</p></li><li><p>一些基本参数：IP地址、子网掩码、默认网关、DNS的IP地址。</p></li><li><p>DHCP 基于 UDP 工作，DHCP 服务器运行在 67 号端口， DHCP客户运行在68 号端口。</p></li><li><p>工作流程：</p><p><img src="image-20241228003921248.png" /></p><p>自己看一遍吧，挺符合直觉的。首先先来点广播请求，把自己的MAC封装进去。然后接收到的DHCP服务器就会把配置文件传回来。传回来你自己选一个呗，选好了再广播一下告诉你选好了（此时目标ip仍然是0.0.0.0）。DHCP如果发现这个传回来的报文的事务id与自己当时给出去报文的事务id一样，就知道客户选择了自己的配置文件。于是就扔一个确认报文回去。</p><p>至于续约，反正就先请求呗，然后回应一下就好了。如果不回应就重传下。最后客户可以随时解约。</p></li><li><p>但是如果你网络里没有DHCP服务器咋办？你的广播必然会被路由器丢掉。所以这时候就给路由器配置DHCP中继代理，也就是路由器会转发你的DHCP广播请求。问题解决。</p></li></ul></li></ul><h3 id="无线网络">无线网络</h3><ul><li><p>无线局域网 WLAN</p><ul><li>LAN在前面的网络分类讲过了，WLAN就是无线局域网，that's all.</li><li>无线局域网的组成：<ul><li>有固定基础设置的 WLAN：IEEE 802.11</li><li>无固定基础设置的 WLAN：移动自组网络</li></ul></li><li>IEEE 802.11 是一个有固定基础设施的无线局域网的国际标准。<ul><li>使用星形拓扑，中心叫做接入点 AP。</li><li>AP是无线局域网的基础设施，也是一个链路层的设备。（注意区分，路由器什么的都是网络层设备了已经）</li><li>AP 也叫做无线接入点 WAP。</li><li>无线局域网中的站点对网内或网外的通信都必须通过 AP。</li><li>在 MAC 层使用 CSMA/CA 协议</li><li>凡使用 802.11 系列协议的局域网又称为 Wi-Fi 。</li><li>802.11 规定无线局域网的最小构件是基本服务集 BSS。一个 BSS包括一个基站和若干个移动站，接入点 AP 就是 BSS 内的基站。每个 AP都有一个分配的名字，称为服务集标识符 SSID，它其实就是使用该 AP的无线局域网的名字（也就是 wifi 名字）。每个 AP 有一个唯一的 48 位 MAC地址，名称是基本服务集标识符 BSSID。</li><li>移动站关联 AP 后，要通过该 AP 向该子网发送 DHCP 发现报文以获取 IP地址。这之后，移动站就作为该 AP 子网的成员加入到了网络中。移动站与 AP间通信采用的协议就是 802.11 协议。</li><li>现在的无线局域网一般采用了加密方案 WPA 或WPA2，这时要加入该无线局域网就要输入密码。</li></ul></li><li>移动自组网络<ul><li>没有固定基础设施（即没有AP）的无线局域网。无线传感器网络 WSN是一种近年来发展很快的移动自组网络。它由大量传感器结点通过无线通信技术构成。物联网IoT 就是 WSN 的应用领域。</li></ul></li></ul></li><li><p>802.11 局域网的物理层</p><ul><li><p>物理层有几种实现方法：扩频、多入多出 MIMO、正交频分复用OFDM、跳频扩频 FHSS、红外线 IR</p><p><img src="image-20241227152550624.png" alt="" style="zoom:50%;" /></p></li></ul></li><li><p>802.11 局域网的 MAC 层协议、DCF/PCF、时间间隔</p><ul><li><p>必须要解决共享信道上的碰撞问题。</p></li><li><p>CSMA/CA：载波监听多点接入/碰撞避免。</p><p>CSMA/CD：载波监听多点接入/碰撞检测。</p><p>802.11 无线以太网在 MAC 层使用 CSMA/CA 协议和停止等待协议。</p><p>使用停止等待协议是因为无线信道的通信质量远不如有限信道，要使用停止等待来保证可靠传输。</p><p>无线局域网中不能使用 CSMA/CD 协议，原因是因为隐蔽站问题 /暴露站问题。</p><p><img src="image-20241227153449449.png" alt="" style="zoom:50%;" /></p><p><img src="image-20241228175347088.png" /></p><p>所以不能使用 CSMA/CD 的原因：</p><pre><code>1. 无线局域网的适配器无法实现碰撞检测。2. 检测到信道空闲，其实信道可能并不空闲。3. 即使能够在硬件上实现无线局域网的碰撞检测功能，也无法检测出隐蔽站问题带来的碰撞。4. 即使能够在硬件上实现无线局域网的碰撞检测功能，也会因为暴露站问题导致信道利用率低下。</code></pre></li><li><p>802.11 的MAC层有两个子层：DCF 和PCF。它们通过协调功能来确定在基本服务集 BSS中的移动站何时可以发送或接收数据。</p><ul><li>分布协调功能DCF：不采用任何中心控制，让各个站通过争用信道来获取发送权，必须要实现的功能。</li><li>点协调功能PCF：使用集中控制的接入算法，用类似于探询的方法把发送数据权轮流交给各个站，可选。</li></ul></li><li><p>时间间隔</p><ul><li>在完成发送后，必须再等待一段很短的时间（继续监听）才能发送下一帧。这段时间的通称是帧间间隔IFS。</li><li>两种常用的帧间间隔：分布协调功能帧间间隔 DIFS。短帧间间隔SIFS。</li><li>使用SIFS帧的类型有：ACK帧、CTS帧、过长MAC帧分片后的数据帧、回答AP探测请求帧、PCF方式中AP发出的任何帧。</li><li>使用DIFS帧的类型有：凡在空闲时间想发送数据的站点，必须等待时间 DIFS后才能发送。保证了确认帧 ACK 得以优先发送。</li></ul></li></ul></li><li><p>几种为了缓解碰撞的trick</p><ul><li>虚拟载波监听：发送信息的站点会把自己所占用的一段时间广播，所以在它广播范围内的各站都能收到这一消息，并创建自己的网络分配向量NAV。NAV就是一段不能发送数据的时间。</li><li>随机退避算法：当某个站发现信道变为空闲时，要等待一个 DIFS的间隔，再执行退避算法，维护一个退避计时器，计时器归零后就立即发送。这样也可以降低碰撞概率。</li><li>信道预约(RTS/CTS)：<img src="image-20241227161716574.png" /><ul><li>本质就是上发送数据帧先对信道预约一段时间。因为重发数据会比较耗时间，但RTS/CTS帧比较小，所以如果这时候碰也没啥。信道预约可选，适用于长度较大的数据帧。</li><li>A要发消息，先等待DIFS时间，然后发一个RTS, 跟一个SIFS，AP回一个CTS,跟一个SIFS。这时候A直接把data发过去了, 跟一个SIFS。然后AP回一个ACK,跟一个SIFS，只不过SIFS太短了被下一次发消息前要等待的DIFS时间吞了。</li></ul></li></ul></li><li><p>802.11 局域网的 MAC 帧：控制帧、数据帧和管理帧</p><ul><li>MAC 帧的首部有 30 字节，尾部是 4 字节的帧检验序列。</li><li>802.11 的 MAC 帧有三种类型：控制帧、数据帧、管理帧。</li></ul></li></ul><h3 id="补充">补充</h3><ol type="1"><li>校验和<ul><li>MAC帧的校验和同时检验首部和数据。</li><li>IP首部校验和只检验首部。</li><li>UDP/TCP校验和同时检验伪首部、首部和数据。</li></ul></li><li><strong>长度</strong></li></ol><p>​ <img src="image-20241228220614589.png" /></p><p>​丢到物理层还要在以太网帧前面加上7字节的前同步码和1字节的帧开始定界符。</p><ol start="3" type="1"><li><p>脉冲编码调制的过程（模拟数据变数字信号）简单的说可分为三个过程，它们是：采样、量化、编码。</p></li><li><p>局域网协议把OSI数据链路层分为LLC子层和MAC子层。</p></li><li><p>PPP有两个子协议：链路控制协议LCP、网络控制协议NCP。</p></li><li><p>WLAN的MAC层有两种工作模式：分布协调DCF、点协调PCF。</p></li><li><p>IP数据报在传播过程中源地址和目的地址都不会改变。</p></li><li><p>IP数据报的首部长度是20~60字节，控制IP数据报总长度的参数有16bit，故IP数据报的最长长度为<spanclass="math inline">\(2^{16}-1\)</span>。IP数据报长度必须&lt;=MTU，所以就会在路由器上边分片，在目的主机那再组装起来。</p></li><li><p>从上到下是封装的过程，从下到上是解封装的过程。</p></li><li><p>VLAN是跨越多个端口的逻辑分组，因此不存在冲突域的概念。各个VLAN之间不能直接通信，不是不能通信，它们之间可以通过路由器或三层交换机即可通信。</p></li><li><p>TCP/IP参考模型的网络层提供的是无连接不可靠的数据报服务。</p></li><li><p>路由器实现路由的依据是数据包中的目的IP地址。</p></li><li><p>IEEE 802.11 采用的介质访问机制是 CSMA/CA。</p></li><li><p>交换机的每一个端口都是一个独立的冲突域，意味着每个用户都可以独占带宽，而不会受到其它用户的影响。</p></li><li><p>互联网的核心部分采用分组交换技术（存储转发）。</p></li><li><p>基本概念</p><p><strong>实体</strong>:(N)层中任何可发送或接受信息的软件进程或硬件设备</p><p><strong>协议</strong>:两个对等实体间进行通信所遵循的规则集合</p><p><strong>服务</strong>:下层向上层提供的支持(服务)（功能）</p><p><strong>接口</strong>:上下层间交换信息的方式（接口）</p></li><li><p>网际组管理协议 IGMP不知道IP多播组包含的成员数，也不知道这些成员都分布在哪些网络上。</p></li><li><p>拥塞控制与流量控制的区别</p><ul><li><p>拥塞控制：防止过多的数据注入到网络中，避免网络中的路由器或链路过载。是一个全局性的过程，涉及到所有的主机、路由器，以及与降低网络传输性能有关的所有因素。</p></li><li><p>流量控制：抑制发送端发送数据的速率，以使接收端来得及接收。点对点通信量的控制，是个端到端的问题。</p></li></ul></li><li><p>海明码公式：<span class="math inline">\(2^p - 1 \ge p +m\)</span></p></li><li><p>数据链路层是如何实现点对点的速度匹配和可靠性传输的？</p><ul><li><strong>封装成帧</strong>：在一段数据的前后分别添加首部和尾部，构成一个帧。</li><li><strong>透明传输</strong>：无论发送什么样的比特组合的数据，这些数据都能够按照原样没有差错地通过这个数据链路层。</li><li><strong>差错控制</strong>：纠正信道的噪声导致的数据传输问题。</li></ul></li><li><p>UDP和IP都是无连接的不可靠传输，那可不可以去掉UDP层，直接让IP层为应用层提供服务？</p><ul><li>差错检测</li><li>端到端逻辑链路，端口</li><li>复用（UDP数据包组装成不同IP数据报），分用（根据UDP用户数据报首部中的目的端口号，将数据报分别传送到相应的端口）</li></ul></li><li><p>网络层的两种观点：面向连接的可靠虚电路服务（先建立虚电路）；无连接不可靠的数据报服务。</p></li><li><p>点对点协议PPP、高级数据链路控制HDLC(面向比特)</p><ul><li>HDLC可靠，PPP不可靠</li><li>HDLC现在不用了，PPP现在在用</li><li>PPP简单,HDLC复杂</li><li>PPP可以在其上面封装多种网络层协议，HDLC不可以</li><li>PPP是面向帧的，HDLC是面向比特的</li></ul></li><li><p><strong>数据报与虚电路</strong></p></li></ol><p><img src="image-20241230194907728.png" alt="" style="zoom:67%;" /></p><ol start="25" type="1"><li>ISO/OSI参考模型中，可同时提供无连接服务和面向连接服务的是网络层，运输层只有有连接。TCP/IP是运输层提供俩，然后网络层提供无连接。</li><li><strong>RIP和OSPF区别：</strong><ul><li>RIP基于距离矢量算法，通过定期广播整个路由表，使用跳数作为度量标准，收敛较慢。</li><li>OSPF基于链路状态算法，每个路由器通过链路状态广告广播自身链路状态信息，只有在网络拓扑变化时才更新，利用最短路径优先算法计算最短路径，收敛速度较快，且支持区域划分以适应更复杂的网络。</li></ul></li><li><strong>距离向量与路径向量</strong><ul><li>距离向量：每个节点知道到其邻居的链路距离。每个节点向所有邻居广播已知的最短距离向量。每个节点使用接收到的向量来更新自己的向量。周期性地重复上述过程。</li><li>路径向量：每个节点通过链路状态分组泛洪其邻居信息。所有节点学习完整的网络图。每个节点运行Dijkstra算法来计算到达每个目的地的路径。</li></ul></li><li>本地域名服务器、根域名服务器、顶级域名服务器、权限域名服务器。</li><li>数据链路层采用停止等待协议，是为了解决该层在传输中的哪些问题？是通过什么样的机制来解决这些问题的？<ul><li>数据链路层采用停止等待协议是为了解决传输中的可靠性、错误检测与恢复、以及流量控制问题。它通过发送方发送数据帧后等待接收方确认（ACK），如果超时或发生错误，则重新发送帧，从而保证数据可靠传输，并避免接收方缓冲区溢出。</li></ul></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;参考视频：&lt;a
href=&quot;https://www.bilibili.com/video/BV1c4411d7jb?p=4&amp;amp;spm_id_from=pageDriver&amp;amp;vd_source=ca9a71bb3c1806ce48ae27d95e4e8bd0&quot;&gt;计算机网络微课堂-湖科大教书匠&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="4. 大学" scheme="http://error666.top/categories/4-%E5%A4%A7%E5%AD%A6/"/>
    
    <category term="计算机专业课" scheme="http://error666.top/categories/4-%E5%A4%A7%E5%AD%A6/%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%93%E4%B8%9A%E8%AF%BE/"/>
    
    
  </entry>
  
  <entry>
    <title>2024算法竞赛游记</title>
    <link href="http://error666.top/2024/06/22/2024%E7%AE%97%E6%B3%95%E7%AB%9E%E8%B5%9B%E6%B8%B8%E8%AE%B0/"/>
    <id>http://error666.top/2024/06/22/2024%E7%AE%97%E6%B3%95%E7%AB%9E%E8%B5%9B%E6%B8%B8%E8%AE%B0/</id>
    <published>2024-06-21T16:49:09.000Z</published>
    <updated>2024-12-03T16:04:47.366Z</updated>
    
    <content type="html"><![CDATA[<p>内容包括：ccpc全国邀请赛(山东)、ccpc重庆市塞、ccpc四川省赛、CCPC哈尔滨区域赛、ICPC成都区域赛、ICPC杭州区域赛、CCPC郑州区域赛</p><span id="more"></span><h3 id="ccpc全国邀请赛山东">2024/5 ccpc全国邀请赛（山东）</h3><p>赛后没有第一时间写游记，题目忘了。</p><p>酒店不错，比赛场地很大队伍很多，中午的塔斯汀很好吃，题目也能给我们这些菜鸡做对个五六道。</p><p>我记得有俩签到，一个二分，一个模拟。</p><p>然后一个贪心，做过类似的题：<ahref="https://www.acwing.com/problem/content/907/">区间选点</a></p><p>一个普通的最小生成树变式，一个涉及到后缀和的思维题，一个找规律填空题。</p><p>我们应该是做了6题，最后是铜。</p><p>尽力了，从高二以来已经3年没打算法了，这个结果对我来说挺满意。</p><h3 id="ccpc重庆市赛">2024/5 ccpc重庆市赛</h3><p>赛后没有第一时间写游记，题目忘了。</p><p>志愿者培训没做好，在比赛的时候发出了“讨论请安静点！”的逆天言论，以及比赛开始后才一个个发纸质版题目，差评。以及测评网站用的一个免费的网站，比赛中出现了账号无法登录的问题，差评。</p><p>仨签到。</p><p>然后一个前缀和，做过类似的题，牛客训练赛里的，找不到了。</p><p>然后一个贪心题+简单dfs题。</p><p>还有一题队友写的，不知道是啥算法的题，好像是一道思维题。</p><p>最后金尾。</p><p>遗憾的是有一题hash+二分+dfs的题做了很久没调出来，事后证明思路是正确的，debug能力差了点没弄出来，可惜。</p><h3 id="ccpc四川省赛">2024/6 ccpc四川省赛</h3><p>赛后没有第一时间写游记，题目忘了。</p><p>场地偏小但凑合，中午的食物质量正常，比赛发的衣服不错，赛后奖品还行。</p><p>但是是上半年发挥最差的一场。</p><p>上去先把仨签到迅速切了，此时的我们是金的排名。</p><p>后面4个小时一题没开出来，难蚌。</p><p>我和另一个队友卡在一个非常非常简单的计算几何，另一个队友卡在一道简单的贪心。</p><p>如果状态好的话，就是5题，银首。</p><p>但世界上没那么多如果，菜就是菜了，最后铜。</p><h3 id="ccpc哈尔滨区域赛">2024/10 CCPC哈尔滨区域赛</h3><p>第一次参加区域赛，写详细点吧。</p><p>本来想着东北捡漏来着，结果一看参赛名单预估排名200多，铁牌水平（网络赛在睡觉）。</p><p>周五晚到的哈尔滨，可能赛站地点并不是郊区？感觉车流挺多的，并不是我想象中九点钟后哈尔滨街上空无一人。</p><p>队友订的小红书推荐酒店，300多一晚，挺不错的。除了酒店被子透气差的通病外，自动窗帘+加热马桶的体验不错，以后自己在家也搞一个。</p><p>热身赛B题不会做，后来猜结论猜出来了。C感觉就是先把问题转化为点集到全部点的距离，按每个点到其余所有点排序。然后俩人从大到小挨个取即可。D没看，机子配置常规。</p><p>晚上回酒店简单vp了一下，补了个裴蜀定理和扩欧，就睡觉了。</p><p>正式赛开头挺顺的，队友先把M、C签了。然后G题讨论了一下，只需要不考虑忙碌状态的点前提下，看其余点是否连通就行了。3题都是一发A，不错的开局。</p><p>然后我看了会K，手玩了一下觉得可以假设第K个物品解除限制，然后第k和第k+1的物品解除限制的答案是可以转移的。剩下的时候就贪心的从大往小取就行。用前缀和+ 二分优化这个取的过程。推了一遍样例没问题，于是码了一发切掉。</p><p>J题队友在很早之前就写了一版，但是RE了两发，讨论后发现是一个小细节没考虑到。于是改了第二版，但还是WA，于是开始坐牢。</p><p>还剩一小时的时候，队友写了B，但是WA。</p><p>还剩三分钟，队友随缘改了一点点J，比赛前结束一分钟的时候交了一发，居然过了（？）</p><p>赛后总结，J很早就应该过了，就是因为最后改的那个小细节（队友很早就想到了，但是觉得问题不出那）。然后B凸包那题有一个变量名打错了，不然也是可以过的。</p><p>结果5题，铜牌收尾。</p><p>其实可以是7题的，这样就有银牌了。可是没有那么多如果，菜就是菜了。</p><p>不过往好处想，第一次参加XCPC就拿到了铜牌，并且可以报销机票酒店钱，还是很不错的。</p><h3 id="icpc成都区域赛">2024/10 ICPC成都区域赛</h3><p>赛后忘了写游记，题目忘了。</p><p>这把是参赛强队比较少的一场。滚榜小哥很激情，bgm不错。</p><p>这把有上任acm社长组队，社长一人几乎单挑5题。</p><p>全程社长劈里啪啦敲键盘，咱在旁边想题即可，有一题我有点思路，过了。</p><p>社长来助力直接给咱们干到银牌了。厉害！</p><h3 id="icpc杭州区域赛">2024/11 ICPC杭州区域赛</h3><p>给我们打自闭了，这一次的题目真写不来。两题，打铁离场。</p><p>写游记的心情都没有。</p><p>还是自己菜了。</p><h3 id="ccpc郑州区域赛">2024/11 CCPC郑州区域赛</h3><p>据难度统计最难的一场... ...来参赛的都是强队。</p><p>银牌5题且要快，铜牌4题且要快，我们是4题尾部，再次打铁。</p><p>队友先把L签到签了，然后思考一段时间后把F切了。我B题因为读错题犯病了两次。最后三个人一起干M题，另一个队友在封榜前就已经推出正解了，但是码完发现一直WA，没找出原因。比赛最后5min改eps开始暴交，在最后一分钟的时候过了。（这精度卡的......）</p><p>虽然4题，可是因为M题暴交以及前三题签的比较慢，喜提铁牌。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;内容包括：ccpc全国邀请赛(山东)、ccpc重庆市塞、ccpc四川省赛、CCPC哈尔滨区域赛、ICPC成都区域赛、ICPC杭州区域赛、CCPC郑州区域赛&lt;/p&gt;</summary>
    
    
    
    <category term="3. 竞赛" scheme="http://error666.top/categories/3-%E7%AB%9E%E8%B5%9B/"/>
    
    
  </entry>
  
  <entry>
    <title>Matlab基础</title>
    <link href="http://error666.top/2024/04/25/Matlab%E5%9F%BA%E7%A1%80/"/>
    <id>http://error666.top/2024/04/25/Matlab%E5%9F%BA%E7%A1%80/</id>
    <published>2024-04-24T16:53:52.000Z</published>
    <updated>2024-04-26T15:45:27.353Z</updated>
    
    <content type="html"><![CDATA[<p>学习Matlab，一方面是数模需要，另一方面是Matlab +FigureBest绘制出来的图片非常精美。科研绘图时会用到，所以就学习一下。</p><span id="more"></span><h3 id="界面">界面</h3><p>clear：清空工作区</p><p>clc：清除命令行窗口</p><p>；：语句后加分号结果不显示到命令行中，不加会显示到命令行中</p><p>F5：运行</p><p>ctrl + enter：分块运行</p><p>%%：分块分割注释</p><p>ctrl + r：添加多行注释</p><p>ctrl + t：取消多行注释</p><p>ctrl + 0：跳转到命令行窗口</p><p>ctrl + shift + 0：跳转到编辑窗口</p><p>ctrl + 2：跳转到工作目录窗口</p><p>ctrl + 3：跳转到工作区</p><p><ahref="https://ww2.mathworks.cn/help/index.html">Matlab官方文档</a>：查询各种所需功能&amp; 函数</p><p>填写函数参数时按tab：打开参数提示功能，ctrl + down展开提示</p><h3 id="运算">运算</h3><ul><li><p>特殊变量：ans、pi、inf、-inf</p></li><li><p>数据类型：数字、字符串、矩阵</p></li><li><p>运算符：</p><ol type="1"><li>基本运算符：+、-、*、/、^（乘方）</li><li>常用预算符：abs()、mod(x,y)、sqrt()、exp()、log()、log2()、log10()、round()</li></ol></li></ul><h3 id="矩阵">矩阵</h3><p>Note：下标从1开始</p><ol type="1"><li>矩阵的创建<ol type="1"><li>直接输入：用[]作为标识符，同一行用,分隔，不同行用;分隔</li><li>用预设函数创建：<ol type="1"><li>zeros(x, y)：生成x行y列全0矩阵</li><li>ones(x, y)：生成x行y列全1矩阵</li><li>eye(x)：生成x行x列的单位阵</li><li>rand(x, y)：生成x行y列的矩阵，每个元素在(0, 1)内</li><li>randi([imin, imax], x, y)：生成x行y列的矩阵，每个元素在[imin,imax]内</li><li>randn(x, y)：生成x行y列的矩阵，每个元素服从标准正态分布</li></ol></li><li>导入本地数据创建：<ul><li>支持格式：txt、dat、csv、xls、... ...</li><li>导入方法：在菜单栏选择导入数据即可</li></ul></li></ol></li><li>矩阵的修改<ul><li>A(2, 3) = 0：单点修改</li><li>A(2, :) = 0：第2行全部变为0</li><li>A([1, 2], [1, 2, 3]) = 0：第1、2行的第1、2、3列改为0</li></ul></li><li>矩阵的运算<ol type="1"><li>M1 + M2：对应元素相加</li><li>M + c：矩阵M每个元素加上c</li><li>M1 * M2：矩阵乘法</li><li>M * c：矩阵M每个元素乘上c</li><li>M1 .* M2：矩阵M1、M2对应元素相乘</li><li>M1 ./ M2：矩阵M1、M2对应元素相除</li><li>M ^ c：矩阵M的幂运算</li><li>M'：矩阵M的转置</li><li>inv(M)：矩阵M求逆</li><li>diag(diag(M))：M的对角矩阵</li></ol></li></ol><h3 id="程序结构">程序结构</h3><ol type="1"><li><p>global全局变量</p><ul><li>定义时global声明一次，函数内使用前声明一次</li></ul></li><li><p>if-elseif-else-end</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (... &amp;&amp; ...)</span><br><span class="line">    <span class="comment">% TODO</span></span><br><span class="line"><span class="keyword">elseif</span> (... &amp;&amp; ...)</span><br><span class="line">    <span class="comment">% TODO</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">    <span class="comment">% TODO</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></li><li><p>for-end</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = x:y <span class="comment">%循环变量i从x到y</span></span><br><span class="line">    <span class="comment">% TODO</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></li><li><p>自定义函数</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">res</span> = <span class="title">fun_name</span><span class="params">(var1, var2, ...)</span></span></span><br><span class="line">    <span class="comment">% TODO</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">fun_name</span><span class="params">()</span></span></span><br><span class="line">    <span class="comment">% TODO</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></li></ol><h3 id="图像">图像</h3><ol type="1"><li>axis<ol type="1"><li>axis([xmin, xmax, ymin, ymax])：生成指定坐标范围</li><li>axis equal：x/y轴使用相同的比例</li></ol></li><li>subplot<ol type="1"><li>subplot(n, m,id)：将figure分割为n*m个区域，当前使用第id个区域进行绘图</li></ol></li><li>plot<ol type="1"><li>hold on：使得多个plot画出的线在一个图上</li><li>plot(X, Y)：画出点(x1, y1), (x2, y2), ...并连线</li><li>plot(Y)：画出(1, y1), (2, y2), ...并连线</li><li>plot(x, y, '.')：画坐标点(x, y)</li></ol></li><li>title<ol type="1"><li>title('xxx')：起名</li></ol></li><li>xlabel/ylabel<ol type="1"><li>xlabel('xxx')</li></ol></li><li>legend<ol type="1"><li>legend('name1', 'name2', ...)：图例</li></ol></li><li>改样式<ul><li>交给FigureBest</li></ul></li></ol><hr /><h3 id="实战">实战</h3><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br></pre></td><td class="code"><pre><span class="line">clc;</span><br><span class="line">clear all; <span class="comment">% 相比于clear, clear all可以清除global变量</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% 定义变量</span></span><br><span class="line"><span class="keyword">global</span> Iter_Num n x r r1 r2 alpha <span class="built_in">beta</span> v s y a ans_x ans_y s_x s_y xx vv ss yy;</span><br><span class="line">Iter_Num = <span class="number">1000</span>;</span><br><span class="line">n = <span class="number">5</span>;</span><br><span class="line">x = <span class="built_in">zeros</span>(<span class="number">1</span>, n);</span><br><span class="line">r = <span class="built_in">zeros</span>(<span class="number">1</span>, n);</span><br><span class="line">r1 = [<span class="number">1</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">4</span>, <span class="number">8</span>];     <span class="comment">% 5个预设点的x坐标</span></span><br><span class="line">r2 = [<span class="number">6</span>, <span class="number">8</span>, <span class="number">8</span>, <span class="number">2</span>, <span class="number">3</span>];     <span class="comment">% 5个预设点的y坐标</span></span><br><span class="line">alpha = <span class="number">0.01</span>;</span><br><span class="line"><span class="built_in">beta</span> = <span class="number">0.01</span>;</span><br><span class="line">v = <span class="built_in">diag</span>(<span class="built_in">ones</span>(<span class="number">1</span>, n));</span><br><span class="line">s = phi(x);</span><br><span class="line">y = <span class="built_in">zeros</span>(<span class="number">1</span>, n);</span><br><span class="line">a = [</span><br><span class="line">    <span class="number">1</span>/<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>/<span class="number">2</span>, <span class="number">0</span>, <span class="number">0</span>;</span><br><span class="line">     <span class="number">1</span>/<span class="number">3</span>, <span class="number">1</span>/<span class="number">3</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>/<span class="number">3</span>;</span><br><span class="line">    <span class="number">0</span>, <span class="number">1</span>/<span class="number">2</span>, <span class="number">1</span>/<span class="number">2</span>, <span class="number">0</span>, <span class="number">0</span>;</span><br><span class="line">    <span class="number">0</span>, <span class="number">1</span>/<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>/<span class="number">2</span>, <span class="number">0</span>;</span><br><span class="line">    <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>/<span class="number">2</span>, <span class="number">1</span>/<span class="number">2</span></span><br><span class="line">    ];</span><br><span class="line">ans_x = <span class="built_in">zeros</span>(<span class="number">1</span>, <span class="number">1</span>); <span class="comment">% ans_x[i][j]表示第i个点第j次迭代的x坐标</span></span><br><span class="line">ans_y = <span class="built_in">zeros</span>(<span class="number">1</span>, <span class="number">1</span>); <span class="comment">% ans_y[i][j]表示第i个点第j次迭代的y坐标</span></span><br><span class="line">s_x = <span class="built_in">zeros</span>(<span class="number">1</span>, <span class="number">1</span>);   <span class="comment">% s_x[i][j]表示第i个点第j次迭代x坐标的sigma</span></span><br><span class="line">s_y = <span class="built_in">zeros</span>(<span class="number">1</span>, <span class="number">1</span>);   <span class="comment">% s_y[i][j]表示第i个点第j次迭代y坐标的sigma</span></span><br><span class="line">xx = x;</span><br><span class="line">vv = v;</span><br><span class="line">ss = s;</span><br><span class="line">yy = y;</span><br><span class="line"></span><br><span class="line"><span class="comment">% 开始迭代</span></span><br><span class="line">solve();</span><br><span class="line">fprintf(<span class="string">&#x27;经过%d轮迭代, 最终F(x)收敛到: %f\n&#x27;</span>, Iter_Num, cal(Iter_Num));</span><br><span class="line">draw_1(); <span class="comment">% F_k的比率图</span></span><br><span class="line">draw_2(); <span class="comment">% 画演示图</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%下面是画图子函数</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">draw_2</span><span class="params">()</span></span></span><br><span class="line">    <span class="keyword">global</span> Iter_Num;</span><br><span class="line">    <span class="built_in">figure</span> (<span class="number">2</span>);</span><br><span class="line">    subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>);</span><br><span class="line">    draw_2_sub(<span class="number">1</span>);</span><br><span class="line">    subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>);</span><br><span class="line">    draw_2_sub(Iter_Num / <span class="number">2</span>);</span><br><span class="line">    subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>);</span><br><span class="line">    draw_2_sub(Iter_Num);</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">draw_2_sub</span><span class="params">(iter_num)</span></span></span><br><span class="line">    <span class="built_in">hold</span> on;</span><br><span class="line">    <span class="keyword">global</span> n r1 r2 ans_x ans_y;</span><br><span class="line">    axis([<span class="number">0</span>, <span class="number">10</span>, <span class="number">0</span>, <span class="number">10</span>]);</span><br><span class="line">    <span class="built_in">plot</span>(<span class="number">5</span>, <span class="number">5</span>, <span class="string">&#x27;*&#x27;</span>);</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : n</span><br><span class="line">        <span class="built_in">plot</span>(ans_x(<span class="built_in">i</span>, iter_num), ans_y(<span class="built_in">i</span>, iter_num), <span class="string">&#x27;+&#x27;</span>, <span class="string">&#x27;Color&#x27;</span>, <span class="string">&#x27;blue&#x27;</span>);</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    <span class="built_in">plot</span>([r1(<span class="number">1</span>), r1(<span class="number">2</span>), r1(<span class="number">3</span>), r1(<span class="number">5</span>), r1(<span class="number">4</span>), r1(<span class="number">1</span>)], [r2(<span class="number">1</span>), r2(<span class="number">2</span>), r2(<span class="number">3</span>), r2(<span class="number">5</span>), r2(<span class="number">4</span>), r2(<span class="number">1</span>)], <span class="string">&#x27;Marker&#x27;</span>,<span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;Color&#x27;</span>, <span class="string">&#x27;red&#x27;</span>);</span><br><span class="line">    sum_x = <span class="number">0</span>;</span><br><span class="line">    sum_y = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : n</span><br><span class="line">        sum_x = sum_x + ans_x(<span class="built_in">i</span>, iter_num);</span><br><span class="line">        sum_y = sum_y + ans_y(<span class="built_in">i</span>, iter_num);</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    <span class="built_in">plot</span>(sum_x / n, sum_y / n, <span class="string">&#x27;o&#x27;</span>);</span><br><span class="line">    xlabel(<span class="string">&#x27;$x_1$&#x27;</span>, <span class="string">&#x27;Interpreter&#x27;</span>, <span class="string">&#x27;latex&#x27;</span>);</span><br><span class="line">    ylabel(<span class="string">&#x27;$x_2$&#x27;</span>, <span class="string">&#x27;Interpreter&#x27;</span>, <span class="string">&#x27;latex&#x27;</span>);</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">draw_1</span><span class="params">()</span></span></span><br><span class="line">    <span class="built_in">figure</span> (<span class="number">1</span>);</span><br><span class="line">    <span class="keyword">global</span> Iter_Num;</span><br><span class="line">    F_best = <span class="number">18.874999999999645</span>;</span><br><span class="line">    error = [];</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : (Iter_Num - <span class="number">1</span>)</span><br><span class="line">        error = [error, <span class="built_in">abs</span>(cal(<span class="built_in">i</span> + <span class="number">1</span>) - F_best) / <span class="built_in">abs</span>(cal(<span class="built_in">i</span>) - F_best)];</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    <span class="built_in">plot</span>(error);</span><br><span class="line">    xlabel(<span class="string">&#x27;Iteration k&#x27;</span>);</span><br><span class="line">    ylabel(<span class="string">&#x27;$\frac&#123;f_&#123;k+1&#125; - f^*&#125;&#123;f_k - f^*&#125;$&#x27;</span>, <span class="string">&#x27;Interpreter&#x27;</span>, <span class="string">&#x27;latex&#x27;</span>, <span class="string">&#x27;FontSize&#x27;</span>, <span class="number">20</span>);</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% 下面是计算子函数</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">res</span> = <span class="title">cal</span><span class="params">(iter_num)</span></span></span><br><span class="line">    <span class="keyword">global</span> n ans_x r1 ans_y r2 s_x s_y;</span><br><span class="line">    sum = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : n</span><br><span class="line">        sum = sum + <span class="number">0.5</span> * ((ans_x(<span class="built_in">i</span>, iter_num) - r1(<span class="built_in">i</span>)) ^ <span class="number">2</span> + (ans_y(<span class="built_in">i</span>, iter_num) - r2(<span class="built_in">i</span>)) ^ <span class="number">2</span>)...</span><br><span class="line">            + <span class="number">0.5</span> * ((ans_x(<span class="built_in">i</span>, iter_num) - <span class="number">5</span>) ^ <span class="number">2</span> + (ans_y(<span class="built_in">i</span>, iter_num) - <span class="number">5</span>) ^ <span class="number">2</span>)...</span><br><span class="line">            + ((s_x(<span class="built_in">i</span>, iter_num) - <span class="number">5</span>) ^ <span class="number">2</span> + (s_y(<span class="built_in">i</span>, iter_num) - <span class="number">5</span>) ^ <span class="number">2</span>);</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    res = sum;</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">solve</span><span class="params">()</span></span></span><br><span class="line">    <span class="keyword">global</span> Iter_Num n xx ss ans_x ans_y s_x s_y;</span><br><span class="line">    init(<span class="number">1</span>);</span><br><span class="line">    <span class="keyword">for</span> T = <span class="number">1</span> : Iter_Num</span><br><span class="line">        <span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : n</span><br><span class="line">            upd_x(<span class="built_in">i</span>);</span><br><span class="line">            ans_x(<span class="built_in">i</span>, T) = xx(<span class="built_in">i</span>);</span><br><span class="line">            upd_v(<span class="built_in">i</span>);</span><br><span class="line">            upd_s(<span class="built_in">i</span>);</span><br><span class="line">            s_x(<span class="built_in">i</span>, T) = ss(<span class="built_in">i</span>);</span><br><span class="line">            upd_y(<span class="built_in">i</span>);</span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">        backup();</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    init(<span class="number">2</span>);</span><br><span class="line">    <span class="keyword">for</span> T = <span class="number">1</span> : Iter_Num</span><br><span class="line">        <span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : n</span><br><span class="line">            upd_x(<span class="built_in">i</span>);</span><br><span class="line">            ans_y(<span class="built_in">i</span>, T) = xx(<span class="built_in">i</span>);</span><br><span class="line">            upd_v(<span class="built_in">i</span>);</span><br><span class="line">            upd_s(<span class="built_in">i</span>);</span><br><span class="line">            s_y(<span class="built_in">i</span>, T) = ss(<span class="built_in">i</span>);</span><br><span class="line">            upd_y(<span class="built_in">i</span>);</span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">        backup();</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">upd_y</span><span class="params">(i)</span></span></span><br><span class="line">    <span class="keyword">global</span> a ss yy y <span class="built_in">beta</span> n;</span><br><span class="line">    sum = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">1</span> : n</span><br><span class="line">        sum = sum + a(<span class="built_in">i</span>, <span class="built_in">j</span>) * ss(<span class="built_in">j</span>);</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    yy(<span class="built_in">i</span>) = y(<span class="built_in">i</span>) + <span class="built_in">beta</span> * (ss(<span class="built_in">i</span>) - sum);</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">upd_s</span><span class="params">(i)</span></span></span><br><span class="line">    <span class="keyword">global</span> a s ss x xx vv n v;</span><br><span class="line">    sum = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">1</span> : n</span><br><span class="line">        sum = sum + a(<span class="built_in">i</span>, <span class="built_in">j</span>) * s(<span class="built_in">j</span>);</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    ss(<span class="built_in">i</span>) = sum + phi(xx(<span class="built_in">i</span>)) / vv(<span class="built_in">i</span>, <span class="built_in">i</span>) - phi(x(<span class="built_in">i</span>)) / v(<span class="built_in">i</span>, <span class="built_in">i</span>);</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">upd_v</span><span class="params">(i)</span></span></span><br><span class="line">    <span class="keyword">global</span> n a v vv;</span><br><span class="line">    sum = <span class="built_in">zeros</span>(<span class="number">1</span>, n);</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">1</span> : n</span><br><span class="line">        sum = sum + a(<span class="built_in">i</span>, <span class="built_in">j</span>) * v(<span class="built_in">j</span>, :);</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    vv(<span class="built_in">i</span>, :) = sum;</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">upd_x</span><span class="params">(i)</span></span></span><br><span class="line">    <span class="keyword">global</span> xx x alpha r n y s v;</span><br><span class="line">    xx(<span class="built_in">i</span>) = x(<span class="built_in">i</span>) - alpha * ((x(<span class="built_in">i</span>) - r(<span class="built_in">i</span>)) + (x(<span class="built_in">i</span>) - <span class="number">5</span>) + (<span class="number">1</span> / n) * (y(<span class="built_in">i</span>) + <span class="number">2</span> * (s(<span class="built_in">i</span>) - <span class="number">5</span>) / n / v(<span class="built_in">i</span>, <span class="built_in">i</span>)));</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">backup</span><span class="params">()</span></span></span><br><span class="line">    <span class="keyword">global</span> x v s y xx vv ss yy</span><br><span class="line">    x = xx;</span><br><span class="line">    v = vv;</span><br><span class="line">    s = ss;</span><br><span class="line">    y = yy;</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">init</span><span class="params">(op)</span></span></span><br><span class="line">    <span class="keyword">global</span> n x r v s y xx vv ss yy r1 r2</span><br><span class="line">    x = randi([<span class="number">1</span>, <span class="number">5</span>], <span class="number">1</span>, n);</span><br><span class="line">    <span class="keyword">if</span> (op == <span class="number">1</span>)</span><br><span class="line">        r = r1;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        r = r2;</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    v = <span class="built_in">diag</span>(<span class="built_in">ones</span>(<span class="number">1</span>, n));</span><br><span class="line">    s = phi(x);</span><br><span class="line">    y = <span class="built_in">zeros</span>(<span class="number">1</span>, n);</span><br><span class="line">    xx = x;</span><br><span class="line">    vv = v;</span><br><span class="line">    ss = s;</span><br><span class="line">    yy = y;</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">res</span> = <span class="title">phi</span><span class="params">(x)</span></span></span><br><span class="line">    <span class="keyword">global</span> n;</span><br><span class="line">    res = x / n;</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><p>经过FigureBest美化后的图片：</p><p><img src="1.png" /></p><p><img src="2.png" /></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;学习Matlab，一方面是数模需要，另一方面是Matlab +
FigureBest绘制出来的图片非常精美。科研绘图时会用到，所以就学习一下。&lt;/p&gt;</summary>
    
    
    
    <category term="2. 技能栈" scheme="http://error666.top/categories/2-%E6%8A%80%E8%83%BD%E6%A0%88/"/>
    
    <category term="Matlab" scheme="http://error666.top/categories/2-%E6%8A%80%E8%83%BD%E6%A0%88/Matlab/"/>
    
    
  </entry>
  
  <entry>
    <title>各种工具使用手册</title>
    <link href="http://error666.top/2024/04/25/%E5%90%84%E7%A7%8D%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/"/>
    <id>http://error666.top/2024/04/25/%E5%90%84%E7%A7%8D%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/</id>
    <published>2024-04-24T16:28:29.000Z</published>
    <updated>2024-12-15T08:42:55.353Z</updated>
    
    <content type="html"><![CDATA[<p>工具能大大提升效率。所以掌握有必要掌握一些常用工具。</p><span id="more"></span><h3 id="一.-chatgpt">一. ChatGPT</h3><h4 id="如何订阅gpt4-plus">如何订阅GPT4 PLUS？</h4><ol type="1"><li>去<a href="https://chat.openai.com/">官网链接(openai.com)</a>注册一个OpenAI账号并登录（建议通过Google邮箱注册）</li><li>注册过程中，需要用到手机号验证，使用<ahref="https://sms-activate.org/cn">SMS-Activate</a>解决</li><li>1、2步完成后，就可以使用ChatGPT服务了，但是只能使用3.5而且有次数限制。点击升级，会看到一个支付界面，界面里要填银行卡相关信息。银行卡只能使用美国的。所以在<ahref="https://www.fomepay.com/">FOMEPay</a>上购买一张虚拟美国银行卡，往里充钱。然后将卡号信息填到刚才的界面中即可。</li><li>充值成功，可正常使用ChatGPT4</li></ol><p>（注意，充值过程尽量要全程使用美国IP的梯子！）</p><h4 id="如何使用">如何使用？</h4><p>想怎么用怎么用，推荐几个插件：</p><ol type="1"><li>WebPilot：帮助ChatGPT联网搜索信息</li><li>Wolfram：科学计算，图标绘制</li><li>Tutory：可以帮你制定任意领域的学习路线</li><li>Ai Tool Hunt：找插件的插件</li><li>MixerBox Scholar：可以访问一些学术资源</li></ol><hr /><h3 id="二.-vs-code">二. VS Code</h3><p>VS Code作为最常用的IDE，掌握其使用方法以及相关插件十分必要。</p><p>而且这玩意还可以通过Github同步配置，所以配置一次，永久享用。</p><p>而且VS Code里有终端，于是写项目配合git使用很方便。</p><h4 id="快捷键">快捷键</h4><ul><li>ctrl + ,：设置</li><li>ctrl + shift + p：命令面板</li><li>ctrl + p：最近文件列表</li><li>ctrl + j：切出下面板（用来在代码和终端输入间跳转）</li><li>ctrl + b：隐藏/显示目录</li><li>ctrl + shift + n：新建vscode窗口</li><li>ctrl + 1/2/3：分屏/不同分屏中跳转</li><li>ctrl + alt + right/left：将文件移动到不同分屏中</li><li>alt + ijkl/[]：上下左右/HOME,END（自己改的键）</li><li>alt + up/down：行移动</li></ul><h4 id="插件">插件</h4><h5 id="code-runner">code runner</h5><ul><li>ctrl + alt + n：编译运行程序</li><li>ctrl + c：退出当前命令（死循环时退出运行）</li></ul><h5 id="wsl">WSL</h5><ul><li>直接在本地vscode登入进wsl子系统中的vscode</li><li>或者在wsl中输入code .进入vscode界面</li></ul><h5 id="git-graph">Git Graph</h5><ul><li>装了之后，直接在vscode中就可以查看提交/分支状态</li><li>可以查看每次commit的id/author/date/parents/与上一次commit的不同之处<ul><li>查看与任意一次commit的不同之处：先点一个，再按住ctrl点另一个</li></ul></li></ul><h5 id="todo-tree">Todo Tree</h5><ul><li>TODO表示待办</li><li>FIXME表示这段程序是错的，以后再来改</li><li>BUG表示这里是漏洞</li><li>HACK表示心里觉得这里可能有问题的问题</li><li>NOTE表示这里要做点笔记</li><li>XXX表示希望改进</li></ul><h5 id="bookmarks">Bookmarks</h5><ul><li>打标签，当程序很长的时候，用鼠标跳转很不方便，用书签跳转就很快。</li></ul><h5 id="jupyter">Jupyter</h5><ul><li>无需安装jupyter notebook即可在vscode实现相同功能</li><li>Esc + M：markdown模式</li><li>Esc + Y：代码模式</li><li>ctrl + enter：运行</li><li>Esc + D, D：删除该单元</li><li>Esc + B：在下方添加一个单元</li><li>Esc + A：在上方添加一个单元</li><li>Esc + L：显示行号</li></ul><h5 id="draw.io-integration">Draw.io Integration</h5><ul><li>画流程图</li></ul><hr /><h3 id="三.-sai2">三. SAI2</h3><p>SAI2的最最最基本使用（我不玩板绘）。主要用来方便授课。</p><p>因为最近接了一个线上一对一的算法家教，所以买了一块数位板（高漫1060pro）方便授课，然后下了个SAI2。</p><p>首先改板子的映射区，这个型号的板子对我来说太大了，手移动距离太多很累。所以把板子工作区域改小。记得去官方下驱动。</p><p>然后是改板子的快捷键，我板子的快捷键从上到下分别是：（对应着SAI2里的功能）</p><ul><li>选中、剪切</li><li>复制、粘贴</li><li>画笔、橡皮</li><li>ctrl、shift</li><li>文字、ESC</li><li>合并图层、画面居中</li></ul><p>所以设置好快捷键后，基本上只需要打开SAI2，然后在板子上操作就好了。不怎么需要去SAI2里操作了。</p><p>说一下SAI2的操作逻辑，首先是文字，每次输入文字SAI2都会新建一个图层，所以在输入完文字后要按ESC+ 合并图层，才能将文字和原本内容合并到同一图层中。</p><p>然后是粘贴，在你选中、复制、粘贴后，粘贴的内容会新开一个图层。所以需要按住ctrl移动到恰当位置后，按下合并图层，才能实现粘贴的内容和原内容在同一图层中。</p><p>最后是操作时遇到的一些问题：</p><ol type="1"><li>为什么切换到画笔后，写不出东西？<ul><li>可能是因为你选中了某个区域，没有取消就切换到画笔模式了。所以可以先按剪切后，再切换到画笔模式即可正常工作。</li></ul></li></ol><hr /><h3 id="四.-adobe-illustrator">四. Adobe illustrator</h3><p>通俗的理解，Adobeillustrator就是针对于矢量图的画图工具。发英文期刊/会议，配图格式经常要求是矢量图且质量比较高。所以matplotlib/Matlab+ Adobe illustrator + MathType就足以制作论文的配图。</p><h4 id="界面">界面</h4><ul><li>视图 -&gt; 标尺：打开标尺</li><li>右键标尺：选择标尺单位</li><li>视图 -&gt; 智能参考线：打开自动吸附功能</li><li>文件 -&gt; 存储：即保存，格式有eps等</li><li>文件 -&gt; 导出：即导出，格式有jpg/png等</li><li>文件 -&gt; 置入：插入图片到该画板</li><li>窗口 -&gt; 描边：里面有更多关于描边的参数（例如画箭头/虚线）</li></ul><h4 id="操作">操作</h4><ul><li>鼠标中键：移动</li><li>alt + 滚轮：放大/缩小</li><li>双击对象：对象进入隔离模式，防止操作时误操作到其它对象</li></ul><h4 id="工具栏">工具栏</h4><ul><li>空心箭头（普通选择）<ul><li>单击对象：用于选择然后移动/放大/缩小/旋转<ul><li>右键：编组/取消编组</li></ul></li><li>按住ctrl：显示所有锚点，点击锚点可编辑锚点</li><li>按shift + 单击其它对象：可多选其它对象</li><li>按alt + 拖动：可复制一份对象出来</li></ul></li><li>形状工具<ul><li>右键可以选择画不同的形状，按住shift可画标准化图形</li><li>矩形、椭圆、多边形、星形、直线</li></ul></li></ul><h4 id="属性图层库">属性/图层/库</h4><ul><li>属性：调节对象的属性，比如改变位置/设置颜色/填充</li></ul><h3 id="五.-origin">五. Origin</h3><p>画图神器。我觉得比matlab那一套方便多了，图形化的操作更加容易上手，不会把时间浪费在很多无意义的细节上。</p><p>Origin的组织结构：项目(.opju) -&gt; 文件夹 -&gt;book(数据)/graph(图)</p><p>对于book，使用内置python编程导入数据（菜单栏 -&gt; 连接 -&gt;.py），下面是一个导入的例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">wks = op.new_sheet()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">    wks.from_list(i * <span class="number">2</span>, ans_x[i])</span><br><span class="line">    wks.from_list(i * <span class="number">2</span> + <span class="number">1</span>, ans_y[i])</span><br></pre></td></tr></table></figure><p>对于graph，其组织结构为：画布 -&gt; 图层(坐标轴) -&gt; 点/线。</p><p>上面是简单概念介绍，下面将对操作细节进行更多的阐述：</p><ol type="1"><li>A + 鼠标拖拽滚轮：实现页面的移动 + 放大/缩小</li><li>右上角有抗锯齿功能</li><li>最后画完图后，菜单栏 -&gt; 格式 -&gt;调整页面至图层大小，可以把白边裁掉。<ul><li>建议对于宽度选择边界为5，对于高度选择紧凑</li></ul></li><li>画风格类似的图时，可以使用复制风格功能</li><li>画子图时，建议先把每一张图画好，再使用“菜单栏 -&gt; 图 -&gt;合并图表”完成子图绘制</li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;工具能大大提升效率。所以掌握有必要掌握一些常用工具。&lt;/p&gt;</summary>
    
    
    
    <category term="杂项" scheme="http://error666.top/categories/%E6%9D%82%E9%A1%B9/"/>
    
    
  </entry>
  
  <entry>
    <title>计算机组成原理实验笔记</title>
    <link href="http://error666.top/2024/03/28/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86%E5%AE%9E%E9%AA%8C%E7%AC%94%E8%AE%B0/"/>
    <id>http://error666.top/2024/03/28/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86%E5%AE%9E%E9%AA%8C%E7%AC%94%E8%AE%B0/</id>
    <published>2024-03-28T13:12:34.000Z</published>
    <updated>2024-09-03T18:13:21.444Z</updated>
    
    <content type="html"><![CDATA[<p>用Verilog实现一个简易RISC-V指令集CPU软核。</p><p>本次实验是重庆大学2022级弘深计算机拔尖班计算机组成原理的实验项目。</p><span id="more"></span><h3 id="一.-想说的话">一. 想说的话</h3><p>在开始做实验之前，首先要对CPU的物理结构和数据是如何在regfile,data_memory, instr_memory、alu中传输的有比较清晰的认知。建议观看视频: <ahref="https://www.bilibili.com/video/BV1wi4y157D3?p=1">从0到1设计一台计算机</a>，掌握理论知识。</p><p>在看完视频后，可以先跟着这个<ahref="https://www.bilibili.com/video/BV1pK4y1C7es?p=1&amp;vd_source=ca9a71bb3c1806ce48ae27d95e4e8bd0">教你写一个简单的CPU</a>视频，实现一个简单的MIPS指令集CPU软核，掌握实践知识。不用完全写对，跟着写一遍知道各个模块是如何互相运作的即可。因为视频中没有测试文件测试各个模块，都是一口气写下来的。所以最终版仿真是跑不起来的，因为存在诸多bug。</p><p>然后，最好在系统学习完一遍计组的理论知识（尤其是流水线冒险）后，再去写RISC-V指令集的软核。或者边学边做也行。我就是在全部写完后，才去学的计组理论知识，所以写出来的软核在冒险那一块是存在几个bug的，已经懒得调了。到时候下半年参加龙芯杯的时候反正也要再一个cpu，把这个遗憾留到龙芯杯解决就是了。</p><h3 id="二.-思路">二. 思路</h3><p>有啥思路？拿到设计图干就完事了，把线连好就行了，没啥难度。</p><p>代码已开源：<ahref="https://github.com/potatoQi/RISCV_CPU_Chongqing-University-Computer-Organization-Principles-Course-Project">RISCV_CPU(ChongqingUniversity Computer Organization Principles Course Project)</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;用Verilog实现一个简易RISC-V指令集CPU软核。&lt;/p&gt;
&lt;p&gt;本次实验是重庆大学2022级弘深计算机拔尖班计算机组成原理的实验项目。&lt;/p&gt;</summary>
    
    
    
    <category term="4. 大学" scheme="http://error666.top/categories/4-%E5%A4%A7%E5%AD%A6/"/>
    
    <category term="计算机专业课" scheme="http://error666.top/categories/4-%E5%A4%A7%E5%AD%A6/%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%93%E4%B8%9A%E8%AF%BE/"/>
    
    
  </entry>
  
</feed>
