<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Error_666</title>
  
  
  <link href="http://error666.top/atom.xml" rel="self"/>
  
  <link href="http://error666.top/"/>
  <updated>2024-10-09T13:04:34.441Z</updated>
  <id>http://error666.top/</id>
  
  <author>
    <name>Error_666</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>RL代码学习框架</title>
    <link href="http://error666.top/2024/10/09/RL%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/"/>
    <id>http://error666.top/2024/10/09/RL%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/</id>
    <published>2024-10-08T18:07:09.000Z</published>
    <updated>2024-10-09T13:04:34.441Z</updated>
    
    <content type="html"><![CDATA[<p>为巩固RL算法的掌握程度，所以我打算写一个学习框架。测试自己写的算法是否正确以及观察算法的各种指标。既巩固了所学理论，又弥补了实战编程经验的不足，也会以后算法创新的仿真测试做了铺垫。所以我认为写这么一个学习框架性价比很高。</p><span id="more"></span><p>框架正在开发中，点击<ahref="https://github.com/potatoQi/Grid_World">链接</a>前往。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;为巩固RL算法的掌握程度，所以我打算写一个学习框架。测试自己写的算法是否正确以及观察算法的各种指标。既巩固了所学理论，又弥补了实战编程经验的不足，也会以后算法创新的仿真测试做了铺垫。所以我认为写这么一个学习框架性价比很高。&lt;/p&gt;</summary>
    
    
    
    <category term="1. 科研" scheme="http://error666.top/categories/1-%E7%A7%91%E7%A0%94/"/>
    
    <category term="强化学习" scheme="http://error666.top/categories/1-%E7%A7%91%E7%A0%94/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>强化学习3</title>
    <link href="http://error666.top/2024/10/07/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A03/"/>
    <id>http://error666.top/2024/10/07/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A03/</id>
    <published>2024-10-07T15:30:16.000Z</published>
    <updated>2024-10-10T16:40:37.837Z</updated>
    
    <content type="html"><![CDATA[<p>值函数近似(DQN)、策略梯度方法(REINFORCE)、Actor-Critic方法</p><span id="more"></span><h3 id="值函数近似">值函数近似</h3><h4 id="引入">引入</h4><p>上一章介绍了时序差分方法，也就是model-free下求state/actionvalue/optimal action value的迭代法。</p><p>其思路是在MC ExploringStarts上演变过来的，即完全的记忆化。既然使用了记忆化，那么就有记忆化数组，即tabular。</p><p>这一章的值函数近似不是基于tabular，而是基于函数。这是有实际意义的，比如actionvalue是连续的时候，离散化后用tabular存就很有可能存不下，所以我们需要一个连续的算法。</p><p>从另一个方面考虑，假如state-actionpair太多太多，那么我很难把全部的state-actionpair都估计到，那么假如我们有一个函数，那么无论你状态有多少个，因为我是表达式，所以随便给一个状态我都能代入表达式估计出来。</p><p>想到啥了吗？用函数拟合任意一个散点图，是的，神经网络最喜欢干这件事了。</p><p>Interesting, right？</p><h4 id="目标函数">目标函数</h4><p>我们的目标就是通过拟合的方法估计<spanclass="math inline">\(v_\pi(s)\)</span>嘛，所以<spanclass="math inline">\(v_\pi(s)\)</span>是真实的值，我们拟合的函数是<spanclass="math inline">\(\hat{v}(s, w)\)</span>。</p><p>注意这是个函数哦，<spanclass="math inline">\(s\)</span>是自变量，<spanclass="math inline">\(w\)</span>是参数。</p><p>显然我们的优化函数为： <span class="math display">\[J(w) = \mathbb{E}[(v_\pi(S) - \hat{v}(S, w))^2]\]</span> 显然，我们希望<spanclass="math inline">\(J(w)\)</span>尽可能小。</p><p>这个优化函数是标准形式，但实际计算的时候我们需要将里面的随机变量和期望替换为样本。</p><h4 id="优化算法和函数设计">优化算法和函数设计</h4><p>前面我们有了目标函数，那么现在我们就来minimize <spanclass="math inline">\(J(w)\)</span>.</p><p>来计算下<span class="math inline">\(J(w)\)</span>的导数： <spanclass="math display">\[\begin{align*}\nabla J(w) &amp;= \nabla \mathbb{E}[(v_\pi(S) - \hat v(S, w))^2] \\            &amp;= \mathbb{E}[\nabla (v_\pi(S) - \hat v(S, w))^2] \\            &amp;= 2\mathbb{E}[(v_\pi(S) - \hat v(S, w))(-\nabla_w \hatv(S, w))] \\            &amp;= -2\mathbb{E}[(v_\pi(S) - \hat v(S, w))\nabla_w \hatv(S, w)]\end{align*}\]</span> 当然，用SGD即可，那么迭代式为： <span class="math display">\[w_{t+1} = w_t + \alpha_t(v_\pi(s_t) - \hat v(s_t, w_t))\nabla_w \hatv(s_t, w_t)\]</span> 可以看出，上面这个迭代式是没法用的。因为<spanclass="math inline">\(v_\pi(s_t)\)</span>我们不知道啊。</p><p>所以可以结合MC或者TD algorithm，这里我就结合TDalgorithm其中的迭代式： <span class="math display">\[v_{t+1}(s_t) = v_t(s_t) - \alpha_t(s_t)[v_t(s_t) - [r_{t+1} + \gammav_t(s_{t+1})]]\]</span> 那么上面的迭代式可以改写为： <span class="math display">\[w_{t+1} = w_t + \alpha_t(r_{t+1} + \gamma \hat v(s_{t+1}, w_t) - \hatv(s_t, w_t)) \cdot \nabla_w \hat v(s_t, w_t)\]</span> 那么我们可以得到TD + 值函数近似算法： <spanclass="math display">\[\begin{align*}&amp;\textbf{Initialization: } \text{A function $\hat v(s, w)$ that is adifferentiable in $w$. Initial parameter $w_0$.} \\&amp;\text{For each episode generated following the polticy $\pi$, do}\\&amp;\quad\quad \text{For each step $(s_t, r_{t+1}, s_{t+1})$, do} \\&amp;\quad\quad\quad\quad w_{t+1} = w_t + \alpha_t[r_{t+1} + \gamma \hatv(s_{t+1}, w_t) - \hat v(s_t, w_t)] \cdot \nabla_w \hat v(s_t, w_t)\end{align*}\]</span> 简洁而优美。</p><p>好了，就剩一件事了。<span class="math inline">\(\hat v(s,w)\)</span>这个函数如何设计？</p><p>一般现在有两种设计方案，第一种是设计为线性的，第二种是用神经网络去拟合。</p><p>先来看第一种：</p><p><span class="math inline">\(\hat v(s, w) =\phi(s)^\mathrm{T}w\)</span>，其中<spanclass="math inline">\(\phi\)</span>是特征<spanclass="math inline">\(s\)</span>的特征向量。</p><p>如何理解呢？就是你对于一个状态的value，看你想用什么特征来描述它，假设你用特征<spanclass="math inline">\(a_1, a_2, a_3, 1\)</span>来描述一个状态<spanclass="math inline">\(s\)</span>，那么其特征向量就为<spanclass="math inline">\([a_1, a_2, a_3,1]^{\mathrm{T}}\)</span>，那么函数就为：<span class="math inline">\(\hatv(s,w) = a_1w_1 + a_2w_2 + a_3w_3 + w_4\)</span>。</p><p>所以这种方法的关键就是选好特征很关键。举个例子，比如想描述某人某时刻的state，那么特征就可以选择：身高、体重、性别。</p><p>再来看第二种：略，神经网络没啥数学推导，这里没必要再展开。</p><p>相同的，我们还可以得到Sarsa + 值函数近似算法： <spanclass="math display">\[w_{t+1} = w_t + \alpha_t[r_{t+1} + \gamma \hat q(s_{t+1}, a_{t+1}, w_t)- \hat q(s_t, a_t, w_t)] \cdot \nabla_w \hat q(s_t, a_t, w_t)\]</span> 相同的，我们还可以得到Q-learning + 值函数近似算法： <spanclass="math display">\[w_{t+1} = w_t + \alpha_t[r_{t+1} + \gamma \max_{a \in\mathcal{A(s_{t+1})}} \hat q(s_{t+1}, a, w_t) - \hat q(s_t, a_t, w_t)]\cdot \nabla_w \hat q(s_t, a_t, w_t)\]</span></p><h4 id="deep-q-learning">Deep Q-learning</h4><p>Deep Q-learning，也可以叫deep Q-network，DQN。</p><p>此方法将DL那一套搬了过来，而且效果还很好。<del>（这世界的本质难道真的是无限拟合？）</del></p><p>Deep Q-learning就是在Q-learning + 值函数近似的基础上，<spanclass="math inline">\(\hat q(s_t, a_t,w_t)\)</span>用神经网络去算的一个算法。</p><p>回顾一下一下optimal bellman equation：</p><p><span class="math display">\[q(s,a) = \mathbb{E}\left[ R_{t+1} + \gamma \max_{a \in\mathcal{A}(S_{t+1})} q(S_{t+1}, a) | S_t = s, A_t = a \right], \foralls, a\]</span> 其实Q-learning本质就是在用迭代法使得<spanclass="math inline">\(q_t(s_t, a_t) \to r_{t+1} + \gamma \max_{a \in\mathcal{A}} q_t(s_t,a)\)</span></p><p>当用神经网络来拟合actionvalues时，它与Q-learning一样，本质是在minimize这个函数： <spanclass="math display">\[J(w) = \mathbb{E}\left[ (R + \gamma \max_{a \in \mathcal{A}(S&#39;)}\hat q(S&#39;, a, w) - \hat q(S, A, w))^2 \right]\]</span> 但是<span class="math inline">\(J(w)\)</span>这个函数对<spanclass="math inline">\(w\)</span>的梯度很难求，因为第二项和第三项都包含了<spanclass="math inline">\(w\)</span>。所以这里原文作者用了一个trick。就是设置了两个<spanclass="math inline">\(w\)</span>，一个叫<spanclass="math inline">\(w\)</span>，一个叫<spanclass="math inline">\(w_T\)</span>。<spanclass="math inline">\(w\)</span>是持续更新的，<spanclass="math inline">\(w_T\)</span>是各种一段时间更新一次的。那么，loss函数可以写为下面这种形式：<span class="math display">\[J(w) = \mathbb{E}\left[ (R + \gamma \max_{a \in \mathcal{A}(S&#39;)}\hat q(S&#39;, a, w_\mathrm{T}) - \hat q(S, A, w))^2 \right]\]</span> 这样的话，<spanclass="math inline">\(w_\mathrm{T}\)</span>就是个常数，那么<spanclass="math inline">\(\nabla_w J(w)\)</span>就可以写出来了： <spanclass="math display">\[\nabla_w J(w) = -2 \mathbb{E}\left[ (R + \gamma \max_{a \in\mathcal{A}(S&#39;)} \hat q(S&#39;, a, w_\mathrm{T}) - \hat q(S, A, w))\cdot \nabla_w \hat q(S, A, w) \right]\]</span>然后既然都用神经网络了，那么全部的思路都转换为深度学习。现在有了目标函数，梯度，就差数据了。</p><p>这里的数据就是很多<span class="math inline">\((s, a, r,s&#39;)\)</span> pairs。</p><p>那么可以写出下列算法： <span class="math display">\[\begin{align*}&amp;\text{Store the experience samples generated by $\pi_b$ in a replaybuffer $\mathcal{B} = \{(s,a,r,s&#39;)\}$} \\&amp;\quad\quad \text{For each iteration, do} \\&amp;\quad\quad\quad\quad \text{Uniformly draw a mini-batch of samplesfrom $\mathcal{B}$} \\&amp;\quad\quad\quad\quad \text{For each sample $(s,a,r,s&#39;)$,calculate the target values as $y_{\mathrm{T}} = r + \gamma \max_{a \in\mathcal{A}(s&#39;)}\hat q(s&#39;,a,w_{\mathrm{T}})$, where} \\&amp;\quad\quad\quad\quad \text{$w_\mathrm{T}$ is the parameter of thetarget network} \\&amp;\quad\quad\quad\quad \text{Update the main network to minimize$(y_\mathrm{T} - \hat q(s,a,w))^2$ using the mini-batch$\{(s,a,y_\mathrm{T})\}$} \\&amp;\quad\quad \text{Set $w_\mathrm{T} = w$ every $C$ iterations}\end{align*}\]</span> 思考累了？换种角度重新看看DQN，会发现很简单。</p><p>首先，它有很多样本，每个样本会得到一个输出<spanclass="math inline">\(y_\mathrm{T}\)</span>（通过optimal bellmanequation得到的输出），我们的目的，就是让我们的网络<spanclass="math inline">\(\hat q(s,a,w)\)</span>（其中<spanclass="math inline">\(s,a\)</span>是输入，<spanclass="math inline">\(w\)</span>是模型参数）尽可能拟合所以样本的<spanclass="math inline">\(y_\mathrm{T}\)</span>。所以就是个简单的深度学习问题。</p><h3 id="策略梯度方法">策略梯度方法</h3><h4 id="引入-1">引入</h4><p>其实就是用连续函数去直接拟合policy，而非像以前那样关注中间量statevalues、action values。</p><p>具体来说，即通常用神经网络去拟合一个函数<spanclass="math inline">\(\pi(a | s, \theta)\)</span>，其中<spanclass="math inline">\(\theta\)</span>是网络参数。</p><p>那如何评价我们拟合的这个<span class="math inline">\(\pi(a | s,\theta)\)</span>是否好坏呢？</p><p>所以我们需要一个指标<spanclass="math inline">\(J(\theta)\)</span>，我们的任务，就是通过大量经验(样本)，去训练拟合这些样本，从而改变<spanclass="math inline">\(\theta\)</span>，去maximize这个指标。</p><p>Interesting，越来越像深度学习的感觉了。</p><p>回顾一下，RL从MDP开始，MDP就是建立在标准的数学动态规划、矩阵论、概率论上的数学框架。解决RL问题就是在解决这个数学框架。求解方法有迭代法或者直接解方程。</p><p>后面因为概率很难提前获得，也就是我们通常不能开“上帝视角”，所以解数学框架的时候会缺失一些信息。因此我们通过大量采样来近似模拟这些信息，进行解题。这就诞生MC系列算法、时序差分系列算法。</p><p>再后来，我们的视角逐渐不再放在最底层的MDP框架公式中，而是在借助MDP框架的关键定义和公式上，试图直接利用数学，拟合出最佳的statevalues / action values / optimal policy。这就是DeepQ-learning、REINFORCE、Actor-Critic。</p><p>未来，至少在短期内可以预见的是，RL将于DL深度结合，且大量的DL技巧将会被运用到RL中来。</p><p>未来RL会走出一条什么样的路，我们不知道。但是，万一哪天RL突破了“RL”的定义和边界，我想，会是件令人激动的事。</p><h4 id="目标函数-1">目标函数</h4><p>也就是引入中说到的<spanclass="math inline">\(J(\theta)\)</span>是啥？</p><p>有两大类metrics，第一类是average state value，就是statevalues的加权平均。记作<spanclass="math inline">\(\bar{v}_\pi\)</span></p><p>第二类是average reward，就是<spanclass="math inline">\(r_\pi(s)\)</span>的加权平均。记作<spanclass="math inline">\(\bar{r}_\pi\)</span></p><p>似乎这两个metrics，前一个更加远视，后一个更加近视（因为考虑的是immediateexpected reward），但其实，对这两个指标做优化是等价的，因为在<spanclass="math inline">\(\gamma &lt; 1\)</span>的时候，满足：<spanclass="math inline">\(\bar{r}_\pi = (1 - \gamma)\bar{v}_\pi\)</span></p><p>average state value, <spanclass="math inline">\(\bar{v}_\pi\)</span>，可写为： <spanclass="math display">\[\bar{v}_\pi = \sum_{s}d_\pi(s)v_\pi(s) = \mathbb{E}\left[\sum_{t=0}^{\infty}\gamma^t R_{t+1} \right]\]</span> 第一个定义就是本身的定义，<spanclass="math inline">\(d_\pi(s)\)</span>是不同state的权重。</p><p>第二个定义不太直观，我来推导一下： <span class="math display">\[\mathbb{E}\left[\sum_{t=0}^{\infty}\gamma^{t}R_{t+1}\right]=\sum_{s\in\mathcal{S}}d(s)\mathbb{E}\left[\sum_{t=0}^{\infty}\gamma^{t}R_{t+1}|S_{0}=s\right]=\sum_{s\in\mathcal{S}}d(s)v_{\pi}(s)\]</span> average reward, <spanclass="math inline">\(\bar{r}_\pi\)</span>，可写为： <spanclass="math display">\[\bar{r}_\pi = \sum_{s}d_\pi(s)r_\pi(s) = \lim_{n \to\infty}\frac{1}{n}\mathbb{E}(\sum_{t=1}^{n}R_t)\]</span> 第一个定义就是本身的定义，第二个定义很好理解，<spanclass="math inline">\(\bar{r}_\pi\)</span>表达的就是所有immediateexpected reward的期望，那么你把所有所有的<spanclass="math inline">\(R_t\)</span>求平均就是<spanclass="math inline">\(\bar{r}_\pi\)</span>了。</p><h4 id="梯度计算">梯度计算</h4><p>直接给出结论，详细证明去书里看： <span class="math display">\[\nabla_\theta\bar{r}_\pi\simeq\sum_sd_\pi(s)\sum_a\nabla_\theta\pi(a|s,\theta)q_\pi(s,a),\\\nabla_\theta\bar{v}_\pi=\frac1{1-\gamma}\nabla_\theta\bar{r}_\pi\]</span> 当<span class="math inline">\(\gamma &lt; 1\)</span>时<spanclass="math inline">\(\nabla_\theta\bar{r}_\pi\)</span>是约等于右边那一坨，当<spanclass="math inline">\(\gamma = 1\)</span>时是严格等于。<spanclass="math inline">\(\gamma &lt; 1\)</span>时第二个式子成立。</p><p>但是上面那个式子还可以进行化简：</p><p>不妨对<span class="math inline">\(\ln \pi(a|s,\theta)\)</span>求导，<span class="math inline">\(\nabla_\theta \ln\pi(a|s, \theta) = \frac{1}{\pi(a|s, \theta)} \cdot \nabla_\theta\pi(a|s, \theta)\)</span></p><p><span class="math inline">\(\therefore \nabla_\theta \pi(a|s, \theta)= \pi(a|s, \theta)\nabla_\theta\ln\pi(a|s, \theta)\)</span></p><p>带回上面的式子，得：<span class="math inline">\(\nabla_\theta\bar{r}_\pi = \sum_{s}d_\pi(s)\sum_{a}\pi(a|s,\theta)\nabla_\theta\ln\pi(a|s, \theta)q_\pi(s, a)\)</span></p><p>那么就可以把<spanclass="math inline">\(\sum\)</span>写为期望的方式：<spanclass="math inline">\(\nabla_\pi \bar{r}_\pi = \mathbb{E}_{\mathcal{S}\sim d, \mathcal{A} \sim \pi}\left[ \nabla_\theta\ln\pi(A|S, \theta)\cdot q_\pi(S, A) \right]\)</span></p><p>这样有什么好处呢？相当于我们只需要有state actionpairs的样本，就可以去拟合<spanclass="math inline">\(\nabla_\pi\bar{r}_\pi\)</span>了，相比于前面求和的形式，训练简直不要简单太多。很牛的idea。</p><p>但是既然你取了<spanclass="math inline">\(\ln\)</span>，那么就要保证<spanclass="math inline">\(\pi(a|s, \theta) &gt;0\)</span>，所以对于神经网络的话，在最后一层就要做一个softmax：</p><p><img src="1.png" style="zoom:67%;" /></p><p>就行了，但是这样搞的话，policy就具有探索性了需要注意。</p><p>所以我们的为了更新matrics的梯度上升就可以这么写，用SGD： <spanclass="math display">\[\theta_{t+1} = \theta_t + \alpha\nabla_\theta\ln\pi(a_t|s_t,\theta_t)q_\pi(s_t,a_t)\]</span> 但上面这个式子目前还用不了，因为<spanclass="math inline">\(q_\pi(s_t,a_t)\)</span>我们不知道。所以用MC系列或者TD系列呗，如果你用MC去拟合<spanclass="math inline">\(q_\pi(s_t,a_t)\)</span>，那么你就得到了REINFORCE算法，表示如下： <spanclass="math display">\[\begin{align*}&amp;\textbf{Initialization: }\text{A parameterized function $\pi(a|s,\theta)$} \\&amp;\text{At time $k$, do} \\&amp;\quad\quad \text{Select $s_0$ and generate an episode following$\pi(\theta_k)$. Suppose the episode is $\{s_0, a_0, r_1, \cdots,s_{T-1}, a_{T_1}, r_T\}$.} \\&amp;\quad\quad \text{For $t = 0,1,\cdots, T-1$, do} \\&amp;\quad\quad\quad\quad q_t(s_t, a_t) = \sum_{k=t+1}^{T}\gamma^{k - t- 1}r_k \\&amp;\quad\quad\quad\quad \theta_{t+1} = \theta_t +\alpha\nabla_\theta\ln\pi(a_t|s_t, \theta_t)q_t(s_t, a_t)\end{align*}\]</span></p><h3 id="actor-critic">Actor-Critic</h3><h4 id="qac">QAC</h4><p>我们仍然是直接估计最优策略<span class="math inline">\(\pi(a | s,\theta)\)</span>，然后前面已经推导出了<spanclass="math inline">\(\theta\)</span>的更新式： <spanclass="math display">\[\theta_{t+1} = \theta_t + \alpha\nabla_\theta\ln\pi(a_t|s_t,\theta_t)q_\pi(s_t,a_t)\]</span> 关键这个<span class="math inline">\(q_\pi(s_t,a_t)\)</span>我们不知道，所以用MC方法去估计得到的方法就叫REINFORCE。</p><p>但是其实可以通过神经网络的方法去估计它：<spanclass="math inline">\(q(s_t,a_t,w_t)\)</span>，更新方式如下（为什么更新方式是这样，去看“值函数近似-优化算法和函数设计”部分）：<span class="math display">\[w_{t+1} = w_t + \alpha_w \left[ r_{t+1} + \gamma q(s_{t+1}, a_{t+1},w_t) - q(s_t, a_t, w_t) \right] \nabla_w q(s_t, a_t, w_t)\]</span> 所以我直接给出算法流程：</p><p><img src="2.png" style="zoom:67%;" /></p><p>但是直到现在为止，我还是没解释Actor-Critic这名字啥意思。其实我觉得这名字没啥意思。只需要记住直接估计最优policy的系列方法都是AC系列算法（除了REINFORCE开除AC学籍）</p><p>上面的算法叫QAC，首先是它属于AC系列算法，然后它的<spanclass="math inline">\(q(s,a)\)</span>是通过神经网络算的，所以叫QAC。</p><h4 id="a2c">A2C</h4><p>Advantage actor-critic, A2C，因为全称有俩A，所以叫A2。</p><p>它是QAC的一个改进版本，在更新<spanclass="math inline">\(\theta\)</span>的那一步进行了优化，具体来说，通过添加偏置项，减小了梯度的方差，但是期望不变。</p><p>这么做的好处，就是在通过SGD优化目标函数时，因为梯度期望不变，所以优化结果不会改变。但是梯度方差减小，所以采样带来的误差会减小。</p><h4 id="重要性采样">重要性采样</h4><p>略</p><h4 id="dpg">DPG</h4><p>前面的三个AC系列算法，在<span class="math inline">\(\pi(a|s,\theta)\)</span>这个神经网络中，最后一层都是加了softmax的，所以无论如何都是具有探索性的。</p><p>所以如何让其变为一个greedy的算法呢，Deterministic Policy Gradient,DPG，就是greedy的直接估计最优policy的算法。</p><p>略</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;值函数近似(DQN)、策略梯度方法(REINFORCE)、Actor-Critic方法&lt;/p&gt;</summary>
    
    
    
    <category term="1. 科研" scheme="http://error666.top/categories/1-%E7%A7%91%E7%A0%94/"/>
    
    <category term="强化学习" scheme="http://error666.top/categories/1-%E7%A7%91%E7%A0%94/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>强化学习2</title>
    <link href="http://error666.top/2024/10/06/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/"/>
    <id>http://error666.top/2024/10/06/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/</id>
    <published>2024-10-05T18:11:53.000Z</published>
    <updated>2024-10-10T16:10:36.626Z</updated>
    
    <content type="html"><![CDATA[<p>值/策略迭代算法、蒙特卡洛、随机近似理论、时序差分方法</p><span id="more"></span><h3 id="一.-值策略迭代算法">一. 值/策略迭代算法</h3><h4 id="值迭代算法">值迭代算法</h4><p>其实在BOE那一章的结尾我已经给出了值迭代算法的流程了：</p><ol type="1"><li>设定好<span class="math inline">\(\gamma\)</span>，<spanclass="math inline">\(r\)</span>，<spanclass="math inline">\(p(r|s,a)\)</span>，<spanclass="math inline">\(p(s&#39;|s,a)\)</span></li><li>随意取一个<span class="math inline">\(v_0\)</span>，然后通过<spanclass="math inline">\(q_\pi(s, a) = \sum_{r}p(r|s, a)r + \gamma\sum_{s&#39;}p(s&#39; | s, a)v_\pi(s&#39;)\)</span>算出对应的<spanclass="math inline">\(q_0\)</span></li><li>For each state <span class="math inline">\(s_i\)</span>, at time<span class="math inline">\(k\)</span>：<ul><li>算出<span class="math inline">\(q_{k}(s_i, a)\)</span></li><li>Find the <span class="math inline">\(a_k^*(s_i)\)</span>, s.t, <spanclass="math inline">\(q_k(s_i, a_k^*(s_i))\)</span>最大</li><li><span class="math inline">\(\pi_{k+1}(a|s_i)=\begin{cases} 1 \quada=a_k^*(s_i) \\ 0 \quad a \ne a_k^*(s_i) \end{cases}\)</span></li><li><span class="math inline">\(v_{k+1}(s_i) =\sum_{a}\pi_{k+1}(a|s)q_k(s,a)\)</span></li></ul></li></ol><p>现在，用正式的语言描述这个algorithm： <span class="math display">\[\begin{align*}&amp;\textbf{Initialization: }\text{The probability model $p(r|s,a)$ and$p(s&#39;|s,a)$ for all $(s,a)$ are known. Initial guess $v_0$.} \\&amp;\text{At time $k$, do} \\&amp;\quad\quad \text{For every state $s \in \mathcal{S}$, do} \\&amp;\quad\quad\quad\quad \text{q-value: $q_k(s,a)=\sum_{r}p(r|s,a)r +\gamma \sum_{s&#39;}p(s&#39;|s,a)v_k(s&#39;)$} \\&amp;\quad\quad \text{Maximum action value: $a_k^*(s) =\text{argmax}_{a}q_k(a,s)$} \\&amp;\quad\quad \text{Policy update: $\pi_{k+1}(a|s)=1$ if $a=a_k^*(s)$,and $\pi_{k+1}(a|s)=0$ otherwise} \\&amp;\quad\quad \text{Value update: $v_{k+1}=q_k(a_k^*(s), s)$}\end{align*}\]</span></p><h4 id="策略迭代算法">策略迭代算法</h4><p>思想就是首先先初始化一个策略，然后先得到该策略下的statevalue（即Policy evaluation, PE），然后得到statevalue后就可以算出对应的action value，然后选择actionvalue最大的action，即优化当前policy（Policyimprovement），得到新的policy。依次类推下去，最终即可得到<spanclass="math inline">\(\pi^*, v^*\)</span>。</p><p>用正式的语言描述这个algorithm： <span class="math display">\[\begin{align*}&amp;\textbf{Initialization: }\text{The probability model $p(r|s,a)$ and$p(s&#39;|s,a)$ for all $(s,a)$ are known. Initial guess $\pi_0$.} \\&amp;\text{At time $k$, do} \\&amp;\quad\quad \text{Policy evaluation:} \\&amp;\quad\quad \text{Initialization: an arbitrary initial guess$v_{\pi_k}^{(0)}$} \\&amp;\quad\quad \text{While $v_{\pi_k}^{(j)}$ has not converged, for the$j$th iteration, do} \\&amp;\quad\quad\quad\quad \text{For every state $s \in \mathcal{S}$, do}\\&amp;\quad\quad\quad\quad\quad\quad v_{\pi_k}^{(j+1)}(s) =\sum_{a}\pi_k(a|s)\left[ \sum_{r}p(r|s,a)r +\gamma\sum_{s&#39;}p(s&#39;|s,a)v_{\pi_k}^{(j)}(s&#39;) \right] \\&amp;\quad\quad \text{Policy improvement:} \\&amp;\quad\quad \text{For every state $s \in \mathcal{S}$, do} \\&amp;\quad\quad\quad\quad q_{\pi_k}(s,a) = \sum_{r}p(r|s,a)r +\gamma\sum_{s&#39;}p(s&#39;|s,a)v_{\pi_k}(s&#39;) \\&amp;\quad\quad a_k^*(s) = \text{argmax}_{a}q_{\pi_k}(s,a) \\&amp;\quad\quad \text{$\pi_{k+1}(a|s) = 1$ if $a = a_k^*(s)$, and$\pi_{k+1}(a|s)=0$ otherwise}\end{align*}\]</span></p><h4 id="两者比较">两者比较</h4><p>值迭代算法是从一个初始state value开始，有了statevalue，就可以算出action value，进而得出当前最优策略，然后去更新statevalue，依次类推。</p><p>策略迭代算法是从一个初始policy开始，然后通过迭代算法求出当前policy下的最优statevalue，然后再通过state value得到actionvalue，进而更新当前最优策略。依次类推。</p><p>可以发现，不同点就在于，同样是得到一个policy，值迭代是立马用其代入bellman-equation算出迭代一次后的statevalue。而策略迭代是代入bellman-equation迭代很多次算出的statevalue。所以直观上来说，策略迭代的收敛次数会更少，但是单次计算量会更大。</p><p><img src="1.png" style="zoom:50%;" /></p><h3 id="二.-蒙特卡洛">二. 蒙特卡洛</h3><h4 id="引入">引入</h4><p>前面的值/策略迭代算法都是model-basedRL，蒙特卡洛是我们接触到的第一个model-free的方法。</p><p>model不知道的时候怎么办呢？蒙特卡洛其实就是大量采样，用样本的分布来估计model的分布。</p><p>蒙特卡洛，Monte Carlo，MC。</p><h4 id="mc-basic">MC Basic</h4><p>MCBasic算法其实就跟policy迭代算法一样，只不过把policy迭代算法里的model-based部分，即计算<spanclass="math inline">\(v_{\pi_k},q_{\pi_k}\)</span>的部分，换成了依靠采样直接算出基于一个策略<spanclass="math inline">\(\pi_{k}\)</span>的<spanclass="math inline">\(q_{\pi_k}\)</span>。第二步policyimprovement就一样了。</p><p>原本policy迭代算法里求<spanclass="math inline">\(q_{\pi_k}\)</span>是依赖于这个公式：<spanclass="math inline">\(q_{\pi_k}(s,a)=\sum_{r}p(r|s,a)r +\gamma\sum_{s&#39;}p(s&#39;|s,a)v_{\pi_k}(s&#39;)\)</span></p><p>但是MC Basic算<spanclass="math inline">\(q_{\pi_k}\)</span>是依赖于它的原始定义：<spanclass="math inline">\(q_{\pi_k}(s,a)=\mathbb{E}(G_t | S_t = s, A_t =a)\)</span></p><p>即对于state-action pairs, 通过大量采样估计出所有的<spanclass="math inline">\(q_{\pi_k}(s,a)\)</span>，然后再进行policyimprovement。</p><p>用数学语言来描述如下： <span class="math display">\[\begin{align*}&amp;\textbf{Initialization: }\text{Initial guess $\pi_0$.} \\&amp;\text{At time $k$, do} \\&amp;\quad\quad \text{For every state $s \in \mathcal{S}$, do} \\&amp;\quad\quad\quad\quad \text{For every action $a \in \mathcal{A}(s)$,do} \\&amp;\quad\quad\quad\quad\quad\quad \text{Collect sufficiently manyepisodes starting from $(s,a)$ following $\pi_k$} \\&amp;\quad\quad\quad\quad\quad\quad \text{$q_{\pi_k}(s,a)=$ averagereturn of all the episodes starting from $(s,a)$} \\&amp;\quad\quad\quad\quad \text{Policy improvement step:} \\&amp;\quad\quad\quad\quad a_k^*(s) = \text{argmax}_{a}q_{\pi_k}(s,a) \\&amp;\quad\quad\quad\quad \text{$\pi_{k+1}(a|s)=1$ if $a=a_k^*()s$, and$\pi_{k+1}(a|s)=0$ otherwise}\end{align*}\]</span> 很简单，right？</p><p>为啥这里要用episode这个词而非trajectory这个词呢？因为trajectory可能是无限的，而采样是离散的，所以通常我们设置一个采样长度上限，那么每采样一条trajectory其实就是有限的，也叫一条episode。</p><h4 id="mc-exploring-starts">MC Exploring Starts</h4><p>MC Exploring Starts其实就是对MC Basic算法的一个时间复杂度优化。</p><p>MC Basic是对每一个<span class="math inline">\((s,a)\)</span>pair都采样很多episode来估计其<span class="math inline">\(q_{\pi_k}(s,a)\)</span>，采样的途中可能会路过很多其余的<spanclass="math inline">\((s&#39;,a&#39;)\)</span>pair，其实采样出来的return也可以用来估计它们的actionvalue。下面这个图就可以很好的解释了MC Basic的数据浪费：</p><p><img src="2.png" style="zoom: 67%;" /></p><p>看第一条episode，在MCBasic算法里那么一长条episode，我们只用它来估计了<spanclass="math inline">\((s_1, a_2)\)</span>的actionvalue。但其实，还可以用来估计<span class="math inline">\((s_2, a_4),\cdots\)</span>的action value，它们的return之间只差了一个<spanclass="math inline">\(\gamma\)</span>和reward。</p><p>所以MC ExploringStarts就是抓住了这点进行优化，就是类似记忆化搜索的思想 +dp填表法的思想。它的具体思想如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> T;                <span class="comment">// episode的长度</span></span><br><span class="line"><span class="type">int</span> q_sum[][];        <span class="comment">// (s,a)的action value的总和</span></span><br><span class="line"><span class="type">int</span> q_cnt[][];        <span class="comment">// (s,a)的action value的采样次数</span></span><br><span class="line"><span class="type">int</span> q[][];            <span class="comment">// (s,a)的action value</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> (<span class="number">1</span>) &#123;</span><br><span class="line">   <span class="keyword">if</span> (如果不想再迭代了) <span class="keyword">break</span>;</span><br><span class="line">   <span class="keyword">else</span> 确定起点(s0, a0)，按照当前policy生成一条长度为T的episode，将episode路上的(si, ai, ri)存到vector: path中</span><br><span class="line">       </span><br><span class="line">   <span class="type">int</span> G = <span class="number">0</span>;    <span class="comment">// episode的return</span></span><br><span class="line">   <span class="keyword">for</span> (<span class="type">int</span> i = path.<span class="built_in">size</span>() - <span class="number">1</span>; i &gt;= <span class="number">0</span>; i--) &#123;</span><br><span class="line">       G = gamma * G + path[i].r;</span><br><span class="line">       q_sum[s][a] += G;</span><br><span class="line">       q_cnt[s][a] += <span class="number">1</span>;</span><br><span class="line">       </span><br><span class="line">       q[s][a] = q_sum[s][a] / q_cnt[s][a];</span><br><span class="line">       更新<span class="built_in">pi</span>(a|s)</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>用数学语言描述如下： <span class="math display">\[\begin{align*}&amp;\textbf{Initialization: }\text{ Initial policy $\pi_0(a|s)$ andinitial value $q(s,a)$ for all $(s,a)$.} \\&amp;\quad\quad\quad\quad\quad\quad\quad~\text{Returns(s,a) = 0 andNum(s,a) = 0 for all $(s,a)$.} \\&amp;\text{For each episode, do} \\&amp;\quad\quad \text{Episode generation: Select a starting state-actionpair $(s_0, a_0)$}  \\&amp;\quad\quad \text{and ensure that all pairs can be possibly selected(this is the exploring-starts condition).} \\&amp;\quad\quad \text{Following the current policy, generate an episodeof length $T$: $s_0, a_0, r_1, \cdots, s_{T-1}, a_{T-1}, r_T$.} \\&amp;\quad\quad \text{Initialization for each episode: $g \gets 0$} \\&amp;\quad\quad \text{For each step of the episode, $t = T - 1, T - 2,\cdots, 0, $ do} \\&amp;\quad\quad\quad\quad g \gets \gamma g + r_{t+1} \\&amp;\quad\quad\quad\quad \text{Returns($s_t, a_t$) $\gets$Returns($s_t, a_t$) + $g$} \\&amp;\quad\quad\quad\quad \text{Policy evaluation:} \\&amp;\quad\quad\quad\quad q(s_t, a_t) \gets \text{Returns($s_t, a_t$) /Num($s_t,a_t$)} \\&amp;\quad\quad\quad\quad \text{Policy improvement:} \\&amp;\quad\quad\quad\quad \text{$\pi(a|s_t)=1$ if $a =\text{argmax}_aq(s_t, a)$ and $\pi(a|s_t)=0$ otherwise}\end{align*}\]</span> 最后我们来看看这个“ExploringStarts”是什么意思。Exploring我个人理解就是由于是dp填表法，所以episode就需要自己去生成，也就是exploring的过程。Starts是因为此时的算法不是像MCBasic样每个state-actionpair都去强制估计了，所以为了尽量确保每个state-action都被估计到，每个episode的起点的选法就很有讲究，最好每个state-action都被作为起点选择一次（当然这就退化为MCBasic了）</p><h4 id="mc-varepsilon-greedy">MC <spanclass="math inline">\(\varepsilon\)</span>-Greedy</h4><p>MC Exploring Starts算法很好，但是它不能保证每个state-actionpair都被估计到，所以选多少个episode，每个episode的起点是啥就很有讲究。可能起点选少了直接导致效果不好，选多了又速度慢。</p><p>为了解决上述问题，MC <spanclass="math inline">\(\varepsilon\)</span>-Greedy 算法应运而生。</p><p><span class="math inline">\(\varepsilon\)</span>-Greedy与MC ExploringStarts的区别就在Policy improvement这一步，MC ExploringStarts在这一步是直接将最大的actionvalue对应的action的概率设为1其余为0，但是<spanclass="math inline">\(\varepsilon\)</span>-Greedy是最大的actionvalue对应的action概率设为<span class="math inline">\(1 -\varepsilon\)</span>，其余的action概率设为<spanclass="math inline">\(\varepsilon\)</span>。</p><p>这样的好处就是不需要对全部的state-actionpair都作为起点生成episode，只要生成一些episode（起点随便），并且只要保证这条episode的长度T很长，那你在就几乎可以路过所有state-actionpairs，从而估计它们。就不需要从很多不同的起点开始了。</p><p>坏处就是通过<spanclass="math inline">\(\varepsilon\)</span>-Greedy得到的policy并不是最优的，因为它始终带着探索的概率。所以通常我们的做法是：</p><p>初始化policy为<spanclass="math inline">\(\varepsilon\)</span>策略，进行多次episode，每次的episode的<spanclass="math inline">\(\varepsilon\)</span>递减。这样就可以保证前面的episode随机性很强，从而可以覆盖到大多数state-actionpair，但是毕竟我们是要求最优解的，所以后期的episode的<spanclass="math inline">\(\varepsilon\)</span>就得减小。最后，在通过<spanclass="math inline">\(\varepsilon\)</span>-Greedy得到一个policy后，还是要将其转为确定性的策略（即不带概率的）。</p><p><spanclass="math inline">\(\varepsilon\)</span>-Greedy这个算法的目的就是为了避免要进行多次全部state-actionpair起点选择，仅此而已。它是如何做到的？尝试不是最优的策略，通过尝试不是最优的策略，从而尽可能的，覆盖所有的state-actionparis。所以从原理上，这个算法就是会降低准确率。但是它也很具有实际意义，因为你想啊，你在仿真中设计的算法是要用到实际环境中去的。你的机器人通常就从安全的起点出发开始探索，如果你要让它从全部的state-action开始，那显然不现实。例如深海作业，你都是让robot从浅水区开始，然后让它自己去探索，尽可能覆盖所有state-actionpairs。你总不可能让它从深海区开始吧，因为你人类到不了深海区。</p><p>下面是用数学描述： <span class="math display">\[\begin{align*}&amp;\textbf{Initialization: } \text{Initial policy $\pi_0(a|s)$ andinitial value $q(s,a)$ for all $(s,a)$. Returns(s,a)=0 and Num(s,a)=0}\\&amp;\text{for all $(s,a)$. $\varepsilon \in (0, 1]$} \\&amp;\text{For each episode, do} \\&amp;\quad\quad \text{Episode generation: Select a starting state-actionpair $(s_0, a_0)$. Following the current policy,} \\&amp;\quad\quad \text{generate an episode of length $T: s_0, a_0, r_1,\cdots, s_{T-1}, a_{T-1}, r_T$.} \\&amp;\quad\quad \text{Initialization for each episode: $g \gets 0$} \\&amp;\quad\quad \text{For each step of the episode, $t = T-1, T-2,\cdots, 0,$ do} \\&amp;\quad\quad\quad\quad g \gets \gamma g + r_{t+1} \\&amp;\quad\quad\quad\quad \text{Returns($s_t, a_t$) $\gets$Returns($s_t, a_t$) + $g$} \\&amp;\quad\quad\quad\quad \text{Num($s_t, a_t$) $\gets$ Num($s_t, a_t$)+ 1} \\&amp;\quad\quad\quad\quad \text{Policy evaluation:} \\&amp;\quad\quad\quad\quad q(s_t, a_t) \gets \text{Returns($s_t, a_t$) /Num($s_t, a_t$)} \\&amp;\quad\quad\quad\quad \text{Let } a^* = \text{argmax}_a q(s_t, a)\text{ and} \\&amp;\quad\quad\quad\quad\quad\quad \pi(a|s_t) = \begin{cases}1 -\frac{|\mathcal{A}(s_t)|-1}{|\mathcal{A}|}\varepsilon, \quad a = a^* \\\frac{1}{|\mathcal{A}(s_t)|}\varepsilon, \quad a \ne a^* \end{cases}\end{align*}\]</span> 个人觉得，<spanclass="math inline">\(\varepsilon\)</span>-Greedy的探索性与收敛性是严重矛盾的，因为明明已经通过采样得到state-actionvalue值了，只需要一直不断的更新deterministic的策略，就可以收敛了。但是<spanclass="math inline">\(\varepsilon\)</span>-Greedy为了探索性，会将"探索"这个元素，加入进自己的"策略"里。所以，就像一个明知道最优解的人，在做事情的时候，仍然小概率选择不是最优的东西。所以，<spanclass="math inline">\(\varepsilon\)</span>-Greedy这个算法收敛性，有些靠天。</p><h4 id="总结">总结</h4><p>我们从model-based的算法（值/策略迭代算法）开始说起，model-based算法的收敛性和最优性都是有保证的，在"强化学习1"中有提到证明。</p><p>随后我们进入了model-free算法，此时我们只能依靠采样来估计<spanclass="math inline">\(p(s,a)\)</span>，所以MCBasic只要采样数量无限大，那么其准确性和收敛性也是可以得到保证的。</p><p>但是MC Basic效率太慢了，为此MC ExploringStarts应运而生，运用了记忆化的思想加速了收敛。只要保证每个action-pair都被大量采样到，该算法也能保证准确性和收敛性。</p><p>但是问题就是为了保证“每个action-pair都被大量采样到”，MC ExploringStarts就需要从不同的action-pair起点去生成episode进行采样。而现实环境中这是有难度的，例如你不能让机器人从深海区开始，一般都是从浅水区开始。</p><p>所以MC <spanclass="math inline">\(\varepsilon\)</span>-Greedy算法应运而生，增加了"探索"机制，从而不必使每一个action-pair都要作为起点去生成episode进行采样。但是因为探索是直接加到policy里，所以该算法的准确率会下降，甚至收敛都不一定。不过这对未来的算法具有启发意义。</p><h3 id="三.-随机近似理论">三. 随机近似理论</h3><p>这一章的内容是为了下一章时序差分方法打基础。</p><h4 id="引入-1">引入</h4><p>以前我们用采样估计一个平均数的时候，都是收集m个样本，然后求它们的平均数作为估计值。</p><p>但这样做的话，你需要等到所有的样本都收集到了，才能进行估计。</p><p>所以我们可以用增量式的方法来解决这个问题：</p><p>令：<span class="math inline">\(w_{k+1} = \frac{1}{k}\sum_{i=1}^kx_i, k=1,2,\cdots\)</span></p><p>所以有：<span class="math inline">\(w_k =\frac{1}{k-1}\sum_{i=1}^{k-1}x_i, k=1,2,\cdots\)</span></p><p>那么：<span class="math inline">\(w_{k+1} = w_k - \frac{1}{k}(w_k -x_k)\)</span></p><p>所以来一个样本，就做一次迭代，最终估计的效果跟全部收集到是一样的。</p><p>其实，上面的算法可以进一步推广为： <span class="math display">\[w_{k+1} = w_k - \alpha_k(w_k - x_k)\]</span> 当<span class="math inline">\(\alpha_k &gt;0\)</span>并且满足一定条件时，上面的<spanclass="math inline">\(w_k\)</span>也能收敛于<spanclass="math inline">\(\mathbb{E}(X)\)</span></p><p>上面的算法其实就是一种特殊的Stochastic Approximation algorithm.</p><h4 id="rm">RM</h4><p>Stochastic Apporximation (SA)algorithm其实是一大类算法的总称，描述的是“涉及到随机变量采样”、"迭代式"的算法。</p><p>Robbins-Monro (RM)算法是SA algorithm算法里的一项开创性工作。</p><p>下面来思考这么一个问题，解方程： <span class="math display">\[g(w) = 0\]</span> 其中，<spanclass="math inline">\(g(\cdot)\)</span>未知，但是<spanclass="math inline">\(\nabla g\)</span>是正数且有上下界。</p><p>那么，RM algorithm可以解决这个问题： <span class="math display">\[w_{k+1} = w_k - a_k \tilde{g}(w_k, \eta_k), \quad k=1,2,3,\cdots\]</span></p><ul><li><span class="math inline">\(w_k\)</span> 是第k次迭代对<spanclass="math inline">\(g(w)=0\)</span>解的估计</li><li><span class="math inline">\(\tilde{g}(w_k, \eta_l)=g(w_k) +\eta_k\)</span>是第k次迭代的黑盒输出(这个输出跟真实的输出可能存在误差)</li><li><span class="math inline">\(a_k\)</span>是第k次迭代的正系数</li></ul><p>只要一直迭代下去，那么最终迭代到的<spanclass="math inline">\(w^*\)</span>就是<spanclass="math inline">\(g(w)=0\)</span>的解。</p><p>为啥成立呢？</p><p>从直观上很容易理解，因为<span class="math inline">\(\nablag\)</span>是正数且有上下界，所以如果<spanclass="math inline">\(w_k\)</span>越过了零点，那么其函数值就&gt;0了，那么<spanclass="math inline">\(w_k\)</span>就要往回走一点，也就是减去一个正数，用系数* 函数值刚好可以用作这个系数。如果<spanclass="math inline">\(w_k\)</span>还没到零点，那么其函数值&lt;0，那么<spanclass="math inline">\(w_k\)</span>就要前进一点，也就是减去一个负数，用系数* 函数值刚好可以用作这个系数。一直迭代下去，<spanclass="math inline">\(w_k\)</span>自然趋近零点。</p><p>以上，都是直观上的描述。现在，让我们给出Robbins-Monor (RM)算法的严谨数学表述： <span class="math display">\[\begin{align*}&amp;\text{In the Robbins-Monro algorithm: } w_{k+1} = w_k - a_k\tilde{g}(w_k, \eta_k), \quad k=1,2,3,\cdots\\&amp;if \\&amp;\quad\quad \text{1) $0 &lt; c_1 \le \nabla_w g(w) \le c_2$ for all$w$;} \\&amp;\quad\quad \text{2) $\sum_{k=1}^{\infty}a_k = \infty$ and$\sum_{k=1}^{\infty}a_k^2 &lt; \infty$;} \\&amp;\quad\quad \text{3) $\mathbb{E}[\eta_k | \mathcal{H}_k] = 0$ and$\mathbb{E}[\eta_k^2 | \mathcal{H}_k] &lt; \infty$}; \\&amp;\text{where $\mathcal{H}_k = \{w_k, w_{k-1}, \cdots, \}$, then$w_k$ converges with probability 1 (w.p.1) to the root $w^*$ satisfying$g(w^*)=0$.}\end{align*}\]</span></p><ul><li>这里用依概率收敛(w.p.1)是因为<spanclass="math inline">\(w_k\)</span>是涉及到随机变量采样的一个数，所以为了严谨，这里用了w.p.1</li><li>(1)是对梯度的要求，即要求梯度是大于0的且有上下界</li><li>(2)是对系数的要求，<spanclass="math inline">\(\sum_{k=1}^{\infty}a_k^2 &lt;\infty\)</span>保证了<spanclass="math inline">\(a_k\)</span>会收敛到0，<spanclass="math inline">\(\sum_{k=1}^{\infty}a_k =\infty\)</span>保证了<spanclass="math inline">\(a_k\)</span>收敛的速度不会很快</li><li>(3)是对测量误差的要求，就是说假设你采样的误差的期望要是0，且误差的平方的期望不能发散</li></ul><p>这里，我想讨论一下为什么第二个条件很重要：</p><p>若<span class="math inline">\(\sum_{k=1}^{\infty}a_k^2 &lt;\infty\)</span>，则会保证<spanclass="math inline">\(a_k\)</span>收敛到0，则会使得<spanclass="math inline">\(w_{\infty+1} = w_{\infty}\)</span>，即<spanclass="math inline">\(w_k\)</span>收敛。</p><p>若<span class="math inline">\(\sum_{k=1}^{\infty}a_k =\infty\)</span>呢？有啥用？不妨写出下列式子： <spanclass="math display">\[\begin{cases}&amp;w_2 = w_1 - a_1\tilde{g}(w_1, \eta_1) \\&amp;w_3 = w_2 - a_2\tilde{g}(w_2, \eta_2) \\&amp;\cdots\end{cases}\]</span> 将以上式子全加起来，可得到：<spanclass="math inline">\(w_{\infty} - w_1 = \sum_{k=1}^{\infty}a_k\tilde{g}(w_k, \eta_k)\)</span></p><p>如果<span class="math inline">\(\sum_{k=1}^{\infty}a_k =\infty\)</span>，就可以保证<spanclass="math inline">\(\sum_{k=1}^{\infty}a_k \tilde{g}(w_k,\eta_k)\)</span>发散，这样我们的<spanclass="math inline">\(w_1\)</span>就随便取都行了。如果有界的话，那我的<spanclass="math inline">\(w_1\)</span>的取值就被限定在一个范围了。</p><h4 id="sgd">SGD</h4><p>GD、BGD、SGD其实是一个系列的算法，它们的目的，都是解决下列这个优化问题：<span class="math display">\[\min_w J(w) = \mathbb{E}\left[ f(w, X) \right]\]</span> （即找到<span class="math inline">\(w\)</span>，使得<spanclass="math inline">\(J(w)\)</span>最小）</p><p>梯度下降大家都很熟悉了，这里直接给出定义和简单解释。</p><p>首先是gradient descent, GD, 梯度下降算法： <spanclass="math display">\[w_{k+1} = w_k - \alpha_k \mathbb{E}\left[ \nabla_wf(w_k, X) \right]\]</span>但是这个形式涉及到期望，是理想的式子，在现实中，我们往往用样本去估计这个期望，所以就有了batchgradient descent, BGD： <span class="math display">\[\mathbb{E}\left[ \nabla_wf(w_k, X) \right] \approx\frac{1}{n}\sum_{i=1}^{n} \nabla_wf(w_k, x_i) \\w_{k+1} = w_k - \alpha_k \frac{1}{n} \sum_{i=1}^{n} \nabla_w f(w_k, x_i)\]</span>但是毕竟还是要求出一个batch后才能迭代更新一次嘛，还是慢了，那就来一个样本就更新一次，于是就有了stochasticgradient descent, SGD, 随机梯度下降： <span class="math display">\[w_{k+1} = w_k - \alpha_k \nabla_w f(w_k, x_k)\]</span> 证明SGD收敛的过程我这里大概证一下：</p><p>首先，SGD可以写为RM算法形式： <span class="math display">\[w_{k+1} = w_k - \alpha_k \tilde{g}(w_k, \eta_k) \\\tilde{g}(w_k, \eta_k) = \nabla_wf(w_k, x_k) =\mathbb{E}[\nabla_wf(w,X)] + \left( \nabla_w f(w_k,x_k) -\mathbb{E}[\nabla_w f(w,X)] \right)\]</span> 令<span class="math inline">\(g(w,X) = \nabla_w f(w,X)\)</span>，其实我们就是想求解<span class="math inline">\(g(w, X) =0\)</span>这个方程。</p><p>那么上面的<span class="math inline">\(\tilde{g}(w_k,\eta_k)\)</span>就可写为<span class="math inline">\(g(w, X) +\eta\)</span>的形式，所以SGD被写为了RM算法的形式。</p><p>只要保证<span class="math inline">\(g(w,X)\)</span>的梯度是正数且有上下界（即<span class="math inline">\(f(w,X)\)</span>是凸的），且系数满足那俩条件，且误差<spanclass="math inline">\(\eta\)</span>满足那俩条件，那么SGD算法的收敛性就可以得到保证。</p><p>用数学语言描述SGD的收敛条件如下： <span class="math display">\[\begin{align*}&amp;\text{In the SGD algorithm, if} \\&amp;\quad\quad \text{1) } 0 &lt; c_1 \le \nabla_w^2 f(w, X) \le c_2 \\&amp;\quad\quad \text{2) } \sum_{k=1}^{\infty}a_k = \infty \text{ and }\sum_{k=1}^{\infty} a_k^2 &lt; \infty \\&amp;\quad\quad \text{3) } \{x_k\}_{k=1}^{\infty} \text{ is iid} \\&amp;\text{then $w_k$ converges to the root of $\nabla_w\mathbb{E}[f(w,X)] = 0$ with probability 1.}\end{align*}\]</span> SGD这个算法究竟好不好呢？其实是挺好的，当<spanclass="math inline">\(w_k\)</span>与<spanclass="math inline">\(w^*\)</span>相距较远时，它的表现和GD的表现差不多。我们可以通过误差来看看：<span class="math display">\[\delta_k\doteq\frac{|\nabla_wf(w_k,x_k)-\mathbb{E}[\nabla_wf(w_k,X)]|}{|\mathbb{E}[\nabla_wf(w_k,X)]|}\]</span> 那么通过理论分析，我们可以得到，SGD算法下，这个误差满足：<span class="math display">\[\delta_k\leq\frac{\mid\overbrace{\nabla_wf(w_k,x_k)}^\text{stochasticgradient}-\overbrace{\mathbb{E}[\nabla_wf(w_k,X)]}^\text{truegradient}\mid}{\underbrace{c|w_k-w^*|}_{\text{distance to the optimalsolution}}}\]</span> 所以当<span class="math inline">\(w_k\)</span>与<spanclass="math inline">\(w^*\)</span>相距较远时，它的表现和GD的表现差不多，这是个很不错的算法。</p><h4 id="总结-1">总结</h4><ul><li>这一章其实上是介绍了优化算法。</li><li>首先先介绍了解决<span class="math inline">\(g(w) =0\)</span>的RM算法：<span class="math inline">\(w_{k+1} = w_k - a_k\tilde{g}(w_k, \eta_k)\)</span>，它需要满足下列三个条件才能收敛：<ol type="1"><li><span class="math inline">\(0 &lt; c_1 \le \nabla_w g(w) \lec_2\)</span></li><li><span class="math inline">\(\sum_{k=1}^{\infty}a_k = \infty\)</span>and <span class="math inline">\(\sum_{k=1}^{\infty}a_k^2 &lt;\infty\)</span></li><li><span class="math inline">\(\mathbb{E}[\eta_k | \mathcal{H}_k] =0\)</span> and <span class="math inline">\(\mathbb{E}[\eta_k^2 |\mathcal{H}_k] &lt; \infty\)</span></li></ol></li><li>GD系列算法也属于RM算法，它们负责解决<spanclass="math inline">\(\min_w J(w) = \mathbb{E}\left[ f(w, X)\right]\)</span>问题，换句话说，就是解决<spanclass="math inline">\(\mathbb{E}\left[ \nabla_wf(w_k, X) \right] =0\)</span>问题，所以也可以转换为RM去证明收敛性。最终证明出需要满足下列三个条件才能收敛：<ol type="1"><li><span class="math inline">\(0 &lt; c_1 \le \nabla_w^2 f(w, X) \lec_2\)</span></li><li><span class="math inline">\(\sum_{k=1}^{\infty}a_k = \infty \text{and } \sum_{k=1}^{\infty} a_k^2 &lt; \infty\)</span></li><li><span class="math inline">\(\{x_k\}_{k=1}^{\infty} \text{ isiid}\)</span></li></ol></li></ul><h3 id="四.-时序差分方法">四. 时序差分方法</h3><h4 id="td-algorithm">TD algorithm</h4><p>TD算法通常是指一大类算法，但是这一小节的TD算法就是具体的一个小算法，它用来在已知一个策略<spanclass="math inline">\(\pi\)</span>下，来估计<spanclass="math inline">\(v_\pi\)</span>的值。</p><p>首先回想一下MC ExploringStarts的思路，进行很多次起点选择，每次选择一个起点后生成一条episode，然后倒着更新一路上的<spanclass="math inline">\(q(s,a)\)</span></p><p>是有点记忆化的味道了，不过还不够记忆化，因为</p><p>我直接给出TD algorithm： <span class="math display">\[\begin{cases}v_{t+1}(s_t) = v_t(s_t) - \alpha_t(s_t)\left[ v_t(s_t) - \left[ r_{t+1}+ \gamma v_t(s_{t+1}) \right] \right], \\v_{t+1}(s) = v_t(s), \forall s \ne s_t.\end{cases}\]</span>这就是完全的记忆化了。在程序上可以这么实现：进行若干次迭代，每次迭代循环所有的state:<spanclass="math inline">\(s_t\)</span>，根据policy可以找到它的下一个状态:<spanclass="math inline">\(s_{t+1}\)</span>，然后按照上面的迭代式更新一遍<spanclass="math inline">\(v(s_t)\)</span>。若干次迭代结束后，<spanclass="math inline">\(v(s) \to v_{\pi}(s), \forall s\)</span></p><p>上面的式子从直观上也很好理解，<spanclass="math inline">\(v_t(s_t)\)</span>就是对<spanclass="math inline">\(v_\pi(s_t)\)</span>的估计，所以<spanclass="math inline">\(v_t(s_t)\)</span>是不断在迭代更新的，咋更新的呢？就是不断的逼近<spanclass="math inline">\(r_{t+1} + \gammav_t(s_{t+1})\)</span>。这就是bellmanequation啊。所以本质上我觉得就是迭代法求bellmanequation，只不过与最开始那个迭代法求bellmanequation是在model-based的情况下，现在这个迭代法是无需知道model的。</p><p>我们来从数学上证明一下上面那个算法为什么会收敛到<spanclass="math inline">\(r_{t+1} + \gamma v_t(s_{t+1})\)</span>：</p><p>令：<span class="math inline">\(y = r_{t+1} + \gammav_t(s_{t+1})\)</span></p><p>则迭代式可写为：<span class="math inline">\(v_{t+1}(s_t) = v_t(s_t) -\alpha_t(s_t)[v_t(s_t) - y]\)</span></p><p>两边同减y：<span class="math inline">\(v_{t+1}(s_t) - y = (v_t(s_t) -y) - \alpha_t(s_t)[v_t(s_t) - y]\)</span></p><p>整理：<span class="math inline">\(v_{t+1}(s_t) - y = [1 -\alpha_t(s_t)](v_t(s_t) - y)\)</span></p><p><span class="math inline">\(\therefore \|v_{t+1}(s_t) - y\| \le \| 1- \alpha_t(s_t) \| \cdot \|v_t(s_t) - y\|\)</span></p><p>所以最终<span class="math inline">\(v_{t}(s_t) \to y\)</span></p><p>当<span class="math inline">\(v_t(s_t) \to r_{t+1} + \gammav_t(s_{t+1})\)</span>时其实就是bellman equation了。</p><p>（你可能会问，你这个bellmanequation没带概率啊。别急，概率在迭代过程中policy选择当前state下一个状态<spanclass="math inline">\(s_{t+1}\)</span>进行更新的时候用到了，所以估计出来的这个<spanclass="math inline">\(v_t(s)\)</span>，就是<spanclass="math inline">\(v_\pi(s)\)</span>）</p><p>（数学证明出，当<span class="math inline">\(\sum_{t} \alpha_t(s) =\infty\)</span>, <span class="math inline">\(\sum_{t} \alpha_t^2(s) &lt;\infty\)</span>, <span class="math inline">\(\foralls\)</span>时，上述TD算法就能使<span class="math inline">\(v_t(s) \tov_\pi(s), \forall s\)</span>）</p><h4 id="sarsa">Sarsa</h4><p>前面的TD是在model-free的情况下估计出statevalue。Sarsa就是在model-free的情况下估计出action value。</p><p>直接给出算法，非常好理解： <span class="math display">\[\begin{cases}q_{t+1}(s_t, a_t) = q_t(s_t, a_t) - \alpha_t(s_t, a_t)\left[ q_t(s_t,a_t) - [r_{t+1} + \gamma q_t(s_{t+1}, a_{t+1})] \right], \\q_{t+1}(s,a) = q_t(s,a), \quad \forall (s,a) \ne (s_t, a_t)\end{cases}\]</span>有意思的小故事，为啥叫Sarsa？其实就是state-action-reward-state-action的缩写，难蚌。</p><p>程序也很好写，进行若干次迭代，每次迭代双重循环枚举state、action，然后按照上面迭代式更新就好了。最终<spanclass="math inline">\(q_t(s,a) \to q_\pi(s,t)\)</span></p><p>其收敛条件为：<span class="math inline">\(\sum_{t} \alpha_t(s,a) =\infty\)</span>, <span class="math inline">\(\sum_{t}\alpha_t^2(s,a)&lt; \infty\)</span>, <span class="math inline">\(\forall(s,a)\)</span></p><p>action value都求出来了，后续你是greedy还是<spanclass="math inline">\(\varepsilon\)</span>-Greedy去update你的policy都可以。</p><h4 id="q-learning">Q-learning</h4><p>跟Sarsa不同，Sarsa是估计action value，结合policyimprovement才可以得到最优policy。而Q-learing是一步到位直接估计optimalaction value。</p><p>其实就修改了Sarsa迭代式里的一个地方，直观感觉就是把policyimprovement这一步直接换成在更新时就优化actionvalue了。这样直接可以得到最优action value，再greedy的求出最优策略即可。<span class="math display">\[\begin{cases}q_{t+1}(s_t, a_t) = q_t(s_t, a_t) - \alpha_t(s_t, a_t)\left[ q_t(s_t,a_t) - [r_{t+1} + \gamma \max_{a \in \mathcal{A}}q_t(s_{t+1}, a)]\right], \\q_{t+1}(s,a) = q_t(s,a), \forall (s,a) \ne (s_t, a_t)\end{cases}\]</span>因为Q-learning用的很多，所以这里我给出其正式的流程描述，分为on-policy和off-policy两个版本：</p><p><strong>On-policy version：</strong> <span class="math display">\[\begin{align*}&amp;\text{For each episode, do} \\&amp;\quad\quad \text{If the current $s_t$ is not the target state, do}\\&amp;\quad\quad\quad\quad \text{Collect the experience $(s_t, a_t,r_{t+1}, s_{t+1})$: In particular, take action $a_t$ follwing} \\&amp;\quad\quad\quad\quad \pi_t(s_t), \text{ generate } r_{t+1},s_{t+1}. \\&amp;\quad\quad\quad\quad \text{Update q-value:} \\&amp;\quad\quad\quad\quad\quad\quad q_{t+1}(s_t, a_t) = q_t(s_t, a_t) -\alpha_t(s_t, a_t) - \alpha_t(s_t, a_t)\left[ q_t(s_t, a_t) - \left[r_{t+1} + \gamma \max_{a}q_t(s_{t+1}, a) \right] \right] \\&amp;\quad\quad\quad\quad \text{Update policy:} \\&amp;\quad\quad\quad\quad\quad\quad \pi_{t+1}(a|s_t) = 1 -\frac{\varepsilon}{|\mathcal{A}|}(|\mathcal{A}| - 1) \text{ if } a =\text{argmax}_a q_{t+1}(s_t, a) \\&amp;\quad\quad\quad\quad\quad \quad \pi_{t+1}(a|s_t) =\frac{\varepsilon}{|\mathcal{A}|} \text{ otherwise}\end{align*}\]</span> <strong>Off-policy version：</strong> <spanclass="math display">\[\begin{align*}&amp;\text{For each episode $\{s_0, a_0, r_1, s_1, a_1, r_2, \cdots \}$generated by $\pi_b$, do} \\&amp;\quad\quad \text{For each step $t = 0,1,2,\cdots$ of the episode,do} \\&amp;\quad\quad\quad\quad \text{Update q-value:} \\&amp;\quad\quad\quad\quad\quad\quad q_{t+1}(s_t, a_t) = q_t(s_t, a_t) -\alpha_t(s_t, a_t) - \alpha_t(s_t, a_t)\left[ q_t(s_t, a_t) - \left[r_{t+1} + \gamma \max_{a}q_t(s_{t+1}, a) \right] \right] \\&amp;\quad\quad\quad\quad \text{Update target policy:} \\&amp;\quad\quad\quad\quad\quad\quad \pi_{T,t+1}(a|s_t) = 1 \text{ of } a= \text{argmax}_a q_{t+1}(s_t, a) \\&amp;\quad\quad\quad\quad\quad\quad \pi_{T,t+1}(a|s_t) = 0 \text{otherwise}\end{align*}\]</span>可以发现，off-policy与on-policy的区别就是，off-policy生成experience数据的policy与优化出的policy不同。</p><p>所以off-policy的一个特点就是最终的优化结果跟生成数据的policy相关性很大，因为它是在生成experience数据的policy基础上优化的。</p><p>但是on-policy，即使初始策略很烂，但是因为是持续优化，最终仍可以收敛到全局最优policy。</p><p>那么off-policy就没有好处了吗？并不是的，它有着自身的优点，在后续与DL结合的时候你就知道了。</p><h4 id="总结-2">总结</h4><p>这一章叫时序差分方法，但是我更愿意把其叫做model-free的迭代法求statevalue(TD)、action value(Sarsa)、optimal actionvalue(Q-learing)。与MC系列算法相比，我觉得是MC系列的完全记忆化版本，即MCExploring Starts算法的优化版本。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;值/策略迭代算法、蒙特卡洛、随机近似理论、时序差分方法&lt;/p&gt;</summary>
    
    
    
    <category term="1. 科研" scheme="http://error666.top/categories/1-%E7%A7%91%E7%A0%94/"/>
    
    <category term="强化学习" scheme="http://error666.top/categories/1-%E7%A7%91%E7%A0%94/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>动手学强化学习</title>
    <link href="http://error666.top/2024/10/05/%E5%8A%A8%E6%89%8B%E5%AD%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    <id>http://error666.top/2024/10/05/%E5%8A%A8%E6%89%8B%E5%AD%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/</id>
    <published>2024-10-05T13:23:32.000Z</published>
    <updated>2024-10-05T17:25:28.258Z</updated>
    
    <content type="html"><![CDATA[<p>作为《强化学习》系列的补充，专注于代码实操。Follow的上海交大张老师的<ahref="https://hrl.boyuai.com/chapter/1/初探强化学习/">教程</a>。</p><span id="more"></span><h2 id="基础篇">基础篇</h2><h3 id="初探强化学习">初探强化学习</h3><p>”人生中充满选择，每次选择就是一次决策，我们正是从一次次决策中，把自己带领到人生的下一段旅程中。“</p><p>如何衡量一个policy是好是坏？其实在RL里就是通过state value /action来衡量的，因为通常模型已知（即<spanclass="math inline">\(p(s&#39;|s,a),p(r|s,a)\)</span>已知），所以通过bellman-equation可知，不同的policy（即<spanclass="math inline">\(\pi(a|s)\)</span>）会产生不同的statevalue。因此通过观察state value，就可以知道policy是好是坏。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;作为《强化学习》系列的补充，专注于代码实操。Follow的上海交大张老师的&lt;a
href=&quot;https://hrl.boyuai.com/chapter/1/初探强化学习/&quot;&gt;教程&lt;/a&gt;。&lt;/p&gt;</summary>
    
    
    
    <category term="1. 科研" scheme="http://error666.top/categories/1-%E7%A7%91%E7%A0%94/"/>
    
    <category term="强化学习" scheme="http://error666.top/categories/1-%E7%A7%91%E7%A0%94/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>强化学习1</title>
    <link href="http://error666.top/2024/10/03/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A01/"/>
    <id>http://error666.top/2024/10/03/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A01/</id>
    <published>2024-10-03T14:35:02.000Z</published>
    <updated>2024-10-05T17:22:49.659Z</updated>
    
    <content type="html"><![CDATA[<p>基本概念、MDP、贝尔曼公式、贝尔曼最优公式</p><span id="more"></span><p>Follow的西湖大学赵老师的<ahref="https://www.bilibili.com/video/BV1sd4y167NS/?p=2&amp;vd_source=ca9a71bb3c1806ce48ae27d95e4e8bd0">B站课程</a></p><hr /><h3 id="一.-基本概念">一. 基本概念</h3><p>考虑一个在强化学习里很常见的例子“Grid-world”，网格世界：</p><p><img src="1.png" style="zoom:67%;" /></p><p>网格有以下几种类型：</p><ol type="1"><li>Acessible cell：可到达，即白色</li><li>Forbidden cell：进去会得到惩罚的地方，即黄色</li><li>Target cell：希望到达的地方，即蓝色</li></ol><p>网格世界有边界，且机器人只能在相邻块间移动。</p><p>网格世界的任务就是：找到一个“好”的路径到达target cell。</p><p>下面借用grid-world例子，来介绍几个在RL里常见的名词</p><ul><li><p><code>state</code>：</p><ul><li>符号：<span class="math inline">\(s_i\)</span></li><li>解释：agent的可能会处于的状态，在grid-world里robot的state就是它的location</li></ul></li><li><p><code>state-space</code>：</p><ul><li>符号：<span class="math inline">\(\mathcal{S} =\{s_i\}\)</span></li><li>解释：所有state的集合，在grid-world里state-space就是全部的location</li></ul></li><li><p><code>action</code>：</p><ul><li>符号：<span class="math inline">\(a_i\)</span></li><li>解释：action的概念都是基于state，即当前state为了达到下一个state所采取的一个动作，在grid-world里任意一个state的action都是上/下/左右/不动</li></ul></li><li><p><code>action-space</code>：</p><ul><li>符号：<span class="math inline">\(\mathcal{A}(s_i) =\{a_i\}\)</span></li><li>解释：action-space的概念也是基于state，即当前state所有action的集合，在grid-world里所有state的action-space都是上下左右不动五个动作</li></ul></li><li><p><code>state-transition</code>：</p><ul><li><p>符号：<span class="math inline">\(s_1 \stackrel{a_3}{\to}s_2\)</span></p></li><li><p>解释：从某个state，通过其某个action，转移到另一个state的过程</p></li><li><p>简单的state-transition可以用表格的形式表现出来：</p><p><img src="2.png" /></p></li><li><p>还可以用条件概率来描述state-transition：<spanclass="math inline">\(p(s_2 | s_1, a_3) = 1\)</span>（当前在<spanclass="math inline">\(s_1\)</span>通过<spanclass="math inline">\(a_3\)</span>，跳到<spanclass="math inline">\(s_2\)</span>的概率是1）</p></li></ul></li><li><p><code>policy</code>：</p><ul><li><p>解释：告诉agent当它处于某state时，应该采取什么action</p></li><li><p>符号：</p><ul><li><span class="math inline">\(\pi(a_1 | s_1) = 0\)</span>：在<spanclass="math inline">\(s_1\)</span>下，采取<spanclass="math inline">\(a_1\)</span>策略的概率是0</li><li><span class="math inline">\(\pi(a_2 | s_1) = 0.5\)</span>：在<spanclass="math inline">\(s_1\)</span>下，采取<spanclass="math inline">\(a_2\)</span>策略的概率是0.5</li><li><span class="math inline">\(\sum_{i=1}^{m} \pi(a_i | s_1) =1\)</span></li></ul></li><li><p>简单的policy也可以用表格的形式表现出来：</p><p><img src="3.png" /></p></li></ul></li><li><p><code>reward</code>：</p><ul><li>解释：在agent处于某个state，采取某个action后得到的一个real number<ul><li>A positive reward represents encouragement to take suchactions.</li><li>A negative reward represents punishment to take such actions.</li><li>reward依赖于state + action</li></ul></li><li>符号：<span class="math inline">\(r_i\)</span></li><li>同样可以用条件概率来表达reward：<span class="math inline">\(p(r=1 |s_1, a_1)\)</span> = 1</li></ul></li><li><p><code>trajectory</code>：</p><ul><li>解释：一条“state-action-reward”的链</li><li>符号：<span class="math inline">\(s_1 \xrightarrow[r=0]{a_2} s_2\xrightarrow[r=0]{a_3} s_5 \xrightarrow[r=0]{a_3} s_8\xrightarrow[r=1]{a_2} s_9\)</span></li></ul></li><li><p><code>return</code>：</p><ul><li>解释：沿着一条trajectory的reward总和</li></ul></li><li><p><code>discount rate</code>：</p><ul><li>符号：<span class="math inline">\(\gamma, 0 &lt; \gamma &lt;1\)</span></li><li>解释： 一个控制着agent策略“近视”/“远视”的参数<ul><li>在trajectory路上每一个新得到一个reward都叠加一个<spanclass="math inline">\(\gamma\)</span>，即<spanclass="math inline">\(\gamma r_1 + \gamma^2 r_2 + \cdots + \gamma^kr_k\)</span></li><li>若<spanclass="math inline">\(\gamma\)</span>越接近0，则agent的策略会更加关注眼前结果</li><li>若<spanclass="math inline">\(\gamma\)</span>越接近1，则agent的策略会考虑的更长远</li></ul></li></ul></li><li><p><code>discount return</code>：</p><ul><li>解释：沿着一条trajectory的叠加过discount rate后的reward总和</li></ul></li><li><p><code>episode</code>：</p><ul><li>解释：一段有terminal state的trajectory</li></ul></li></ul><h3 id="二.-mdp">二. MDP</h3><p>MDP，Markov decisionprocess，马尔可夫决策过程，是一个描述RL的框架。</p><p>MDP的组成如下：</p><ul><li>Sets（集合）<ul><li>state-space：<span class="math inline">\(\mathcal{S}\)</span></li><li>action-space：<spanclass="math inline">\(\mathcal{A}(s)\)</span></li><li>reward-space：<span class="math inline">\(\mathcal{R}(s,a)\)</span></li></ul></li><li>Probability distribution（概率分布）<ul><li><p>state transition probability：<spanclass="math inline">\(p(s&#39; | s, a)\)</span></p></li><li><p>reward probability：<span class="math inline">\(p(r | s,a)\)</span></p></li><li><p>policy：<span class="math inline">\(\pi(a | s)\)</span></p></li></ul></li><li>Markov property（马尔可夫性质）<ul><li><span class="math inline">\(p(s_{t+1} | a_{t + 1}, s_t, \cdots, a_1,s_0) = p(s_{t+1} | a_{t+1}, s_t)\)</span></li><li><span class="math inline">\(p(r_{t+1} | a_{t+1}, s_t, \cdots, a_1,s_0) = p(r_{t+1} | a_{t+1}, s_t)\)</span></li><li>即每一步transition后的state和reward的概率都是与历史无关的，只与action的当前这一步有关</li></ul></li></ul><h3 id="三.-贝尔曼公式">三. 贝尔曼公式</h3><h4 id="引入">引入</h4><p>前面我们学到了<code>return</code>这个概念，<code>return</code>其实就是起点到最终一路上的reward之和。那么记忆化的思想，我们可以给每个state定义一个return，不妨叫做<spanclass="math inline">\(v_i\)</span>。在grid-world例子里，每个格子都有自己的<spanclass="math inline">\(v\)</span>。</p><p><img src="4.png" style="zoom:67%;" /></p><p>对于上图，很容易可以写出下列的递推式： <span class="math display">\[\begin{cases}    &amp;v_1 = r_1 + \gamma v_2 \\    &amp;v_2 = r_2 + \gamma v_3 \\    &amp;v_3 = r_3 + \gamma v_4 \\    &amp;v_4 = r_4 + \gamma v_1 \\\end{cases}\]</span> 不妨写成矩阵形式： <span class="math display">\[v = r + \gamma \begin{bmatrix} 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0&amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 1 \\ 1 &amp; 0 &amp; 0 &amp;0\end{bmatrix} v \\ \therefore (I - \gamma P)v = r\]</span> 若<span class="math inline">\((I - \gammaP)\)</span>可逆，则可求出<span class="math inline">\(v\)</span>。</p><p>上面的公式其实就是贝尔曼公式，只不过是非常简单的对于确定性问题的bellmanequation.</p><h4 id="state-value">state value</h4><p>回顾一下单步转移的过程： <span class="math display">\[S_t \xrightarrow{A_t} R_{t+1}, S_{t+1}\]</span></p><ul><li><span class="math inline">\(S_t\)</span>：当前的状态</li><li><span class="math inline">\(A_t\)</span>：当前采取的action</li><li><span class="math inline">\(R_{t+1}\)</span>：在<spanclass="math inline">\(S_t\)</span>采取<spanclass="math inline">\(A_t\)</span>后获得的reward。这里写<spanclass="math inline">\(R_t\)</span>也行，但是习惯写为<spanclass="math inline">\(R_{t+1}\)</span>。</li><li><span class="math inline">\(S_{t+1}\)</span>：下一步的状态</li><li>Note：这里的<span class="math inline">\(S_t, R_{t},A_t\)</span>均为随机变量</li></ul><p>再回顾一下MDP里的Probability distribution：</p><ul><li><span class="math inline">\(S_t \to A_t\)</span>由<spanclass="math inline">\(\pi(A_t = a | S_t = s)\)</span>决定</li><li><span class="math inline">\(S_t, A_t \to R_{t+1}\)</span>由<spanclass="math inline">\(p(R_{t+1} = r | S_t = s, A_t =a)\)</span>决定</li><li><span class="math inline">\(S_t, A_t \to S_{t+1}\)</span>由<spanclass="math inline">\(p(S_{t+1}=s&#39; | S_t = s, A_t =a)\)</span>决定</li></ul><p>再回顾一下"引入"里<spanclass="math inline">\(v_i\)</span>里的概念，这里我们给它一个具体的符号：<spanclass="math inline">\(G_t\)</span></p><ul><li>对于一个<span class="math inline">\(S_t\)</span>，其discountedreturn，即<span class="math inline">\(G_t := R_{t+1} + \gamma R_{t+2} +\gamma^2 R_{t+3} + \cdots\)</span></li><li><spanclass="math inline">\(G_t\)</span>也是一个随机变量。因为因为policy的随机性，以及<spanclass="math inline">\(R_{t}\)</span>同为随机变量。</li></ul><p>现在，可以引出state value的定义了。</p><p>state value就是某个状态的discountedreturn的期望值，用数学符号表示就是：<span class="math inline">\(v_\pi(s)= \mathbb{E}(G_t | S_t = s)\)</span></p><p>Hummm，statevalue是个很好的衡量工具，它能告诉我一个state的期望discountedreturn是多少，这样就可以衡量一个state是否具有价值了。</p><p>那它跟return有什么区别？return是针对一个确定性problem来说的，但是statevalue套了概率与期望。</p><p><img src="5.png" style="zoom:50%;" /></p><p>上面是一个state value的例子，三个图代表三种policy。每种policy下<spanclass="math inline">\(s_1\)</span>的state value计算如上。</p><h4 id="bellman-equation-derivation">Bellman equation: Derivation</h4><p>贝尔曼公式用一句话来概括，就是它描述了不同state value之间的关系。</p><p>下面我们来推导一下贝尔曼公式： <span class="math display">\[\begin{align*}    v_\pi(s)&amp;=\mathbb{E}(G_t | S_t = s) \\            &amp;=\mathbb{E}(R_{t+1} + \gamma G_{t+1} | S_t = s) \\            &amp;=\mathbb{E}(R_{t+1} | S_t = s) +\gamma\mathbb{E}(G_{t+1} | S_t = s)\end{align*}\]</span> 先分析第一项： <span class="math display">\[\begin{align*}    \mathbb{E}(R_{t+1} | S_t = s) &amp;= \sum_{a} \left( \pi(a |s)\left( \sum_{r}p(r | s, a)r \right) \right)\end{align*}\]</span> 上面推导的思路就是你的<spanclass="math inline">\(R_{t+1}\)</span>是依赖于<spanclass="math inline">\(S_t, A_t\)</span>的，所以先把<spanclass="math inline">\(A_t\)</span>搞出来，然后有了<spanclass="math inline">\(a, s\)</span>后，再把<spanclass="math inline">\(r\)</span>搞出来</p><p>再分析第二项： <span class="math display">\[\begin{align*}    \mathbb{E}(G_{t+1} | S_t = s) &amp;= \sum_{s&#39;}\left( p(s&#39; |s)\mathbb{E}(G_{t+1} | S_{t+1}=s&#39;) \right) \\    &amp;=\sum_{s&#39;}\left( p(s&#39; | s)v_\pi(s&#39;)\right) \\    &amp;=\sum_{s&#39;}\left( v_\pi(s&#39;) \cdot \sum_{a}\left( \pi(a |s)p(s&#39; | s, a) \right) \right)\end{align*}\]</span> 上面的推导思路就是首先你要走到<spanclass="math inline">\(s&#39;\)</span>，然后<spanclass="math inline">\(p(s&#39;|s)\)</span>又可以展开，即先要有<spanclass="math inline">\(a\)</span>，才能基于<span class="math inline">\(s,a\)</span>走到<span class="math inline">\(s&#39;\)</span></p><p>第二项其实还可以这样推导： <span class="math display">\[\begin{align*}    \mathbb{E}(G_{t+1} | S_t = s) &amp;= \sum_{a}\left( \pi(a|s) \cdot\sum_{s&#39;}\left( p(s&#39;|s,a) \cdot \mathbb{E}(G_{t+1} |S_{t+1}=s&#39;) \right) \right) \\    &amp;=\sum_{a}\left( \pi(a|s) \cdot \sum_{s&#39;}\left(p(s&#39;|s,a) \cdot v_\pi(s&#39;) \right) \right)\end{align*}\]</span> 上面的思路是首先要有<spanclass="math inline">\(a\)</span>，这样才可以走到某个<spanclass="math inline">\(s&#39;\)</span></p><p>那么合并，即可得到： <span class="math display">\[\begin{align*}v_\pi(s) &amp;= \sum_{a} \left( \pi(a | s)\left( \sum_{r}p(r | s, a)r\right) \right) + \gamma \sum_{a}\left( \pi(a|s) \cdot\sum_{s&#39;}\left( p(s&#39;|s,a) \cdot v_\pi(s&#39;) \right) \right) \\    &amp;=\sum_{a}\left( \pi(a|s)\left[ \sum_{r}p(r|s,a)r + \gamma\sum_{s&#39;}p(s&#39;|s,a)v_\pi(s&#39;) \right] \right), \quad s \in\mathcal{S}.\end{align*}\]</span> 上面就是<strong>贝尔曼公式</strong>。</p><p>推导出来后我们来直观理解下这个式子，首先，当前state的statevalue是多少呢？</p><p>因为statevalue是期望，所以就要考虑到所有策略，每个策略会有一个value，所以statevalue就是： <span class="math display">\[\sum_{a}(\pi(a|s) \cdot \text{value})\]</span>那么value是多少呢？用记忆化的思想，就是当前这一步的reward期望，加上<spanclass="math inline">\(\gamma\)</span>乘后续的reward期望。</p><p>当前这一步的reward期望就是： <span class="math display">\[\sum_{r}(p(r|s,a)r)\]</span> 后续的reward期望是多少呢？其实就是下一步的statevalue，那么就要确定下一步的<spanclass="math inline">\(s&#39;\)</span>，所以后续的reward期望就是： <spanclass="math display">\[\sum_{s&#39;}(p(s&#39;|s,a)v_\pi(s&#39;))\]</span> Interesting，right？</p><p>观察bellman equation，可以发现<spanclass="math inline">\(v_\pi(s)\)</span>由三个东西决定：<spanclass="math inline">\(\pi(a|s), p(r|s,a), p(s&#39;|s,a)\)</span></p><p>这仨恰好是MDP里的probabilitydistribution，这就是为什么MDP框架里要抽象出这仨，因为它们很关键。</p><h4 id="bellman-equation-matrix-vector-form">Bellman equation:Matrix-vector form</h4><p>上一个小节我们已经求出了bellman equation： <spanclass="math display">\[v_\pi(s) = \sum_{a}\pi(a|s)\left[ \sum_{r}p(r|s,a)r +\gamma\sum_{s&#39;}p(s&#39;|s,a)v_\pi(s&#39;) \right]\]</span>这是易于理解的，但是做算法/求解的时候，我们需要将其写为矩阵形式。</p><p>首先可以把<spanclass="math inline">\(\sum_{a}\pi(a|s)\)</span>乘进去，得到 <spanclass="math display">\[v_\pi(s) = r_\pi(s) + \gamma \sum_{s&#39;}p_\pi(s&#39; | s)v_\pi(s&#39;)\\r_\pi(s) := \sum_{a}\pi(a|s)\sum_{r}p(r|s,a)r, \quad\quadp_\pi(s&#39;|s) := \sum_{a}\pi(a|s)p(s&#39;|s,a)\]</span></p><ul><li><spanclass="math inline">\(r_\pi(s)\)</span>表示在当前state走一步所能得到的reward的期望</li><li><span class="math inline">\(p_\pi(s&#39;|s)\)</span>表示在当前<spanclass="math inline">\(s\)</span>走到下一步状态<spanclass="math inline">\(s&#39;\)</span>的概率</li></ul><p>Suppose the states could be indexed as <spanclass="math inline">\(s_i~(i=1,2,\cdots,n)\)</span></p><p>For state <span class="math inline">\(s_i\)</span>, the Bellmanequation is <span class="math display">\[v_\pi(s_i) = r_\pi(s_i) + \gamma \sum_{s_j}p_\pi(s_j | s_i)v_\pi(s_j).\]</span> Then, put all these equations for all the states together andrewrite to a matrix-vector form: <span class="math display">\[v_\pi = r_\pi + \gamma P_\pi v_\pi,\]</span> where</p><ul><li><span class="math inline">\(v_\pi = \left[ v_\pi(s_1), \cdots,v_\pi(s_n) \right]^\mathrm{T} \in \mathbb{R}^n\)</span></li><li><span class="math inline">\(r_\pi = \left[ r_\pi(s_1), \cdots,r_\pi(s_n) \right]^\mathrm{T} \in \mathbb{R}^n\)</span></li><li><span class="math inline">\(P_\pi \in \mathbb{R}^{n \timesn}\)</span>, where <span class="math inline">\([P_\pi]_{ij} = p_\pi(s_j| s_i)\)</span></li></ul><p>熟悉的感觉right？回到了推第一篇论文时候的感觉。</p><p>上面的实际意义很好理解，就是一个状态的statevalue等于走一步的reward期望，加上<spanclass="math inline">\(\gamma\)</span>乘<spanclass="math inline">\(\sum\)</span>(走到第j个点的概率 <spanclass="math inline">\(\cdot\)</span> 第j个点出发的state value)</p><p><span class="math inline">\(v_\pi\)</span>就是state values，<spanclass="math inline">\(r_\pi\)</span>叫当前期望reward们，<spanclass="math inline">\(P_\pi\)</span>叫状态转移矩阵</p><p>下面是一个展开形式的矩阵形式：</p><p><img src="6.png" style="zoom: 67%;" /></p><p>所以只要确定了MDP里的probability distribution：<spanclass="math inline">\(\pi(a|s), p(r|s,a),p(s&#39;|s,a)\)</span>，我们就可求出<span class="math inline">\(r_\pi,P_\pi\)</span>，则可以用算出<spanclass="math inline">\(v_\pi\)</span></p><p>来看一个例子：</p><p><img src="7.png" style="zoom: 50%;" /></p><p>OK继续，如果用线代的方法求<spanclass="math inline">\(v_\pi\)</span>，需要算矩阵的逆，这个计算量是很大的，所以通常我们用迭代法来求state-values：</p><p>We have <span class="math inline">\(v_{k+1} = r_\pi + \gamma P_\piv_k\)</span>，we can show that <span class="math inline">\(v_k \to v_\pi= (I - \gamma P_\pi)^{-1}r_\pi, k \to \infty.\)</span></p><blockquote><p>Proof：</p><p>因为<spanclass="math inline">\(P_\pi\)</span>是一个马尔可夫矩阵，所以收敛性是显然可证的。</p><p>先列出已有的条件：</p><p>递推式：<span class="math inline">\(v_{k+1} = r_\pi + \gamma P_\piv_k \quad (1)\)</span></p><p>bellman equation：<span class="math inline">\(v_\pi = r_\pi + \gammaP_\pi r_\pi \quad (2)\)</span></p><p>令<span class="math inline">\(\delta_k = v_k -v_\pi\)</span>，则<span class="math inline">\(v_k = \delta_k + v_\pi\quad (3)\)</span></p><p>将(3)代入(1)，得：<span class="math inline">\(\delta_{k+1} + v_\pi =r_\pi + \gamma P_\pi(\delta_k + v_\pi)\)</span></p><p><span class="math inline">\(\therefore \delta_{k+1} = -v_\pi + (r_\pi+ \gamma P_\pi v_\pi) + \gamma P_\pi \delta_k\)</span></p><p>将(2)代入上式，得：<span class="math inline">\(\delta_{k+1} = \gammaP_\pi \delta_k\)</span></p><p><span class="math inline">\(\therefore \delta_k = \gamma^k P_\pi^k\delta_0, k \to \infty\)</span></p><p>因为<spanclass="math inline">\(P_\pi\)</span>是马尔可夫矩阵，所有其幂次同样是马尔可夫矩阵，所以其每个元素均<spanclass="math inline">\(\in [0, 1]\)</span>，又<spanclass="math inline">\(0 &lt; \gamma &lt; 1\)</span>，所以<spanclass="math inline">\(\gamma^k P_\pi^k, k \to\infty\)</span>是一个零矩阵，所以<span class="math inline">\(\delta_k =\textbf{0}, k \to \infty\)</span></p><p>证毕</p></blockquote><p>OK！现在我们知道了bellman equation的矩阵形式，并通过迭代法算出了statevalues。那么state values有什么用呢？</p><p>答案：用来评估我们的策略表现是否优秀。</p><p>来看下面这个例子：</p><p><img src="8.png" /></p><p>上图有三个子图，每个子图对应一种policy。图1的policy是很好的按计划走到targetcell，图2的policy是一直往右走，图3的policy是随机生成的。</p><p>人眼可以知道，图1的policy最好，图3的policy很一般，图2的policy很差。</p><p>通过确定policy，也就是<spanclass="math inline">\(\pi(a|s)\)</span>，通常<spanclass="math inline">\(p(s&#39;|s,a),p(r|s,a)\)</span>是模型已知的，那么就可以通过bellman equation算出statevalues。已标注在图上。</p><p>可以发现，图1的state values都是正数而且比较大，图2的statevalues都是负的，图3的state values有正有负。</p><p>所以从state values，我们就可以看出一个policy好不好。</p><h4 id="action-value">Action value</h4><p>action value和state value的区别是什么？</p><ul><li>state value：从一个state出发，所得到的average discounted return</li><li>action value：从一个state出发，执行一个action后，所得到的averagediscounted return<ul><li>符号：<span class="math inline">\(q_\pi(s, a) = \mathbb{E}(G_t | S_t= s, A_t = a)\)</span></li></ul></li></ul><p>state value与action value的联系： <span class="math display">\[v_\pi(s) = \sum_{a}\pi(a|s)q_\pi(s, a)\]</span> 上式与bellman equation中<spanclass="math inline">\(v_\pi(s)\)</span>表达式对比，可发现： <spanclass="math display">\[q_\pi(s, a) = \sum_{r}p(r|s, a)r + \gamma \sum_{s&#39;}p(s&#39; | s,a)v_\pi(s&#39;)\]</span> 所以当我们计算出state values后，可以通过上式来算出actionvalues</p><p>上式的实际意义也很好理解，就是一个actionvalue等于走一步的reward期望<spanclass="math inline">\(\sum_{r}p(r|s,a)r\)</span>，加上<spanclass="math inline">\(\gamma\)</span>乘后续的reward期望<spanclass="math inline">\(\sum_{s&#39;}p(s&#39;|s,a)v_\pi(s&#39;)\)</span></p><h4 id="总结">总结</h4><p>这一章最重要的概念就是state value和action value。</p><p>符号表示分别为：<span class="math inline">\(v_\pi(s) = \mathbb{E}(G_t| S_t = s)\)</span>，<span class="math inline">\(q_\pi(s, a) =\mathbb{E}(G_t | S_t = s, A_t = a)\)</span></p><p>state value就是从一个state出发，所得到的average discountedreturn；actionvalue就是从一个state出发，执行一个action后，所得到的average discountedreturn。</p><p>想求解state values需通过bellman equation，bellmanequation的单点形式如下： <span class="math display">\[v_\pi(s_i) = r_\pi(s_i) + \gamma \sum_{s_j}p_\pi(s_j | s_i)v_\pi(s_j)\]</span> 矩阵形式如下： <span class="math display">\[v_\pi = r_\pi + \gamma P_\pi v_\pi \\v_\pi = \left[ v_\pi(s_1), \cdots, v_\pi(s_n) \right]^\mathrm{T} \in\mathbb{R}^n \\r_\pi = \left[ r_\pi(s_1), \cdots, r_\pi(s_n) \right]^\mathrm{T} \in\mathbb{R}^n \\P_\pi \in \mathbb{R}^{n \times n}, \text{ where }[P_\pi]_{ij} =p_\pi(s_j | s_i)\]</span> 所以可解出：<span class="math inline">\(v_\pi = (I - \gammaP_\pi)^{-1}r_\pi\)</span></p><p>但是通常为了节约时间开销选用迭代法求解state values： <spanclass="math display">\[v_{k+1} = r_\pi + \gamma P_\pi v_k \\v_k \to v_\pi = (I - \gamma P_\pi)^{-1}r_\pi, k \to \infty.\]</span> 求解出state values有什么用呢？可以评估policy是好是坏。</p><p>为什么这么说，因为bellman equation本质就是通过MDP里Probabilitydistribution的<span class="math inline">\(\pi(a|s), p(r|s,a),p(s&#39;|s,a)\)</span>求出<spanclass="math inline">\(v_i\)</span>-s。</p><p>而通常<span class="math inline">\(p(r|s,a),p(s&#39;|s,a)\)</span>是已知的（即模型已知），所以<spanclass="math inline">\(v_i\)</span>-s就可以反映出<spanclass="math inline">\(\pi(a|s)\)</span>（即policy）表现如何。</p><h3 id="四.-贝尔曼最优公式">四. 贝尔曼最优公式</h3><h4 id="引入-1">引入</h4><p>对于一个模型确定的问题（即<span class="math inline">\(p(s&#39;|s,a),p(r|s,a)\)</span>已知），我们可以在确定一种policy（<spanclass="math inline">\(\pi(a|s)\)</span>）情况下，算出基于当前policy的statevalues，进而算出action values。</p><p>假设我当前处于<spanclass="math inline">\(s\)</span>，因为我已经知道了actionvalues，所以我已经知道了<span class="math inline">\(q_\pi(s, a_1),q_\pi(s,a_2),q_\pi(s,a_3)\)</span>（假设只有3个策略）。那么从直觉上，我肯定选择最大的<spanclass="math inline">\(q_\pi\)</span>对应的action作为我当前这一步的action。</p><p>你的直觉是对的。通常我们的做法是：</p><p>算出action values后，对于每一个state: <spanclass="math inline">\(s\)</span>，都选择其最大的<spanclass="math inline">\(q_\pi(s, a_*)\)</span>对应的<spanclass="math inline">\(a_*\)</span>作为它新的policy，即<spanclass="math inline">\(\pi(a_{old}|s)\)</span>变为<spanclass="math inline">\(\pi(a_*|s)\)</span>。相当于得到了一个新的policy'，那么一直这样迭代下去，最后迭代出的policy就是最优的。</p><p>这就是直观上的理解，那么如果用数学语言解释上面的现象的话，就需要用到bellmanoptimality equation（BOE）。</p><h4 id="boe">BOE</h4><p>之前我们反复提到了一个policy是否好坏是由statevalue来衡量的，这里我们给出数学定义：</p><p>If <span class="math inline">\(v_{\pi_1}(s) \ge v_{\pi_2}(s)\)</span>for all <span class="math inline">\(s \in \mathcal{S}\)</span>，then<span class="math inline">\(\pi_1\)</span> is "better" than <spanclass="math inline">\(\pi_2\)</span>.</p><p>A policy <span class="math inline">\(\pi^*\)</span> is optimal if<span class="math inline">\(v_{\pi^*}(s) \ge v_\pi(s)\)</span> for all<span class="math inline">\(s\)</span> and for any other policy <spanclass="math inline">\(\pi\)</span>.</p><p>那么我就要提出许多问题了：</p><ol type="1"><li>最优策略 <span class="math inline">\(\pi^*\)</span>是否存在？</li><li>最优策略是否unique？</li><li>最优策略是stochastic的还是deterministic的？</li><li>如何得到最优策略</li></ol><p>不急，慢慢来，这些问题都会得到解决。</p><p>我先给出bellman optimality equation： <span class="math display">\[\begin{align*}v(s) &amp;= \max_{\pi} \sum_{a} \pi(a|s) \left( \sum_{r}p(r|s,a)r +\gamma\sum_{s&#39;}p(s&#39;|s,a)v(s&#39;) \right) \\&amp;= \max_{\pi} \sum_{a} \pi(a|s) q(s,a)\end{align*}\]</span> 上面的式子其实就是bellmanequation发现没？但是有两点不同，第一，符号由<spanclass="math inline">\(v_\pi(s)\)</span>改为<spanclass="math inline">\(v(s)\)</span>了；第二，前面加了个<spanclass="math inline">\(max_\pi\)</span>前缀。</p><p>第一点不同，其实没什么原因，因为前人愿意。</p><p>第二点不同，你需要理解<spanclass="math inline">\(max_{\pi}(\cdots)\)</span>的意思。它意思就是说你的<spanclass="math inline">\(\pi\)</span>随便取，然后给我搞出<spanclass="math inline">\(\cdots\)</span>最大就行。</p><p>所以<span class="math inline">\(v(s) =max_{\pi}\sum_{a}\pi(a|s)q(s,a)\)</span>这一个式子应该这样理解：</p><p>就是首先你得找到一个<spanclass="math inline">\(\pi\)</span>，使得右边这一坨<spanclass="math inline">\(\sum_{a} \pi(a|s)q(s,a)\)</span>最大，也就是最大化<spanclass="math inline">\(v(s)\)</span>。而非常巧的是，你找到的这个<spanclass="math inline">\(\pi\)</span>，它刚好是一个最优policy。因为根据前面最优policy的定义：<spanclass="math inline">\(v_{\pi^*}(s) \gev_\pi(s)\)</span>，此时我们的<spanclass="math inline">\(v(s)\)</span>已经是最大了，所以此时的policy就是最优的。</p><p>上面的BOE可以写为矩阵形式： <span class="math display">\[v = \max_{\pi}(r_\pi + \gamma P_\pi v)\]</span> （<span class="math inline">\(r_\pi + \gamma P_\piv\)</span>是一个向量，给其套一个<spanclass="math inline">\(\max_{\pi}\)</span>的意思是给其每一个分量都套一个<spanclass="math inline">\(\max_{\pi}\)</span>）</p><p>不妨令<span class="math inline">\(f(v) = max_\pi(r_\pi + \gamma P_\piv)\)</span></p><p>那么BOE就可写成： <span class="math display">\[v = f(v)\]</span></p><blockquote><p>补充三个数学小知识：</p><ol type="1"><li>The definition of fixed point：<ul><li><span class="math inline">\(x \in X\)</span> is a fixed point of<span class="math inline">\(f\)</span>: <span class="math inline">\(X\to X\)</span> if <span class="math inline">\(f(x)=x\)</span></li></ul></li><li>Contraction mapping：<ul><li><span class="math inline">\(f\)</span> is a contraction mapping if<span class="math inline">\(\| f(x_1) - f(x_2) \| \le \gamma \| x_1 -x_2 \|\)</span>, where <span class="math inline">\(\gamma \in (0,1)\)</span>.</li></ul></li><li>Contraction Mapping Theorem：<ul><li>For any equation that has the form of <spanclass="math inline">\(x=f(x)\)</span>, if <spanclass="math inline">\(f\)</span> is a contraction mapping, then:<ul><li>Existence: there exists a fixed point <spanclass="math inline">\(x^*\)</span> satisfying <spanclass="math inline">\(f(x^*)=x^*\)</span></li><li>Uniqueness: The fixed point <span class="math inline">\(x^*\)</span>is unique.</li><li>Algorithm: Consider a sequence <spanclass="math inline">\(\{x_k\}\)</span> where <spanclass="math inline">\(x_{k+1}=f(x_k)\)</span>, then <spanclass="math inline">\(x_k \to x^*\)</span> as <spanclass="math inline">\(k \to \infty\)</span>. Moreover, the convergencerate is exponentially fast.</li></ul></li></ul></li></ol></blockquote><p>OK，我们其实可以证明出<spanclass="math inline">\(f(v)\)</span>是一个contractionmapping（证明见书），那么通过contraction mappingtheorem，我们就可以知道，它必然存在不动点解，而且通过迭代法迭代出的<spanclass="math inline">\(v^*\)</span>就是不动点解（即收敛），且不动点唯一。</p><p>那么现在就剩两个问题了：</p><ol type="1"><li><span class="math inline">\(f(v)\)</span>如何求解，即<spanclass="math inline">\(\max_{\pi}(r_\pi + \gamma P_\piv)\)</span>如何求解，进一步说，即<span class="math inline">\(\max_{\pi}\sum_{a} \pi(a|s) q(s,a)\)</span>如何求解？</li><li>如何证明这个unique不动点解<spanclass="math inline">\(v^*\)</span>就是最优的？</li></ol><p>先解决第二个问题，可以由下面这个Theorem解决：</p><blockquote><p>Policy optimality theorem：</p><p>Suppose that <span class="math inline">\(v^*\)</span> is the uniquesolution to <span class="math inline">\(v = \max_\pi(r_\pi + \gammaP_\pi v)\)</span>, and <span class="math inline">\(v_\pi\)</span> is thestate value function satisfying <span class="math inline">\(v_\pi =r_\pi + \gamma P_\pi v_\pi\)</span> for any given policy <spanclass="math inline">\(\pi\)</span>, then: <spanclass="math inline">\(v^* \ge v_\pi, \forall \pi\)</span></p><p>Proof:</p><p>见书</p></blockquote><p>那么只剩一个问题了，<span class="math inline">\(\max_{\pi} \sum_{a}\pi(a|s) q(s,a)\)</span>如何求？</p><p>因为我是要用迭代法求<span class="math inline">\(v =f(v)\)</span>的，所以初始的state values: <spanclass="math inline">\(v_0\)</span>我是已知的。所以相当于初始的actionvalues: <span class="math inline">\(q_0\)</span>我是已知的，将<spanclass="math inline">\(\sum_{a} \pi(a|s) q(s,a)\)</span>展开：<spanclass="math inline">\(\pi(a_1|s)q_0(s,a_1) + \pi(a_2|s)q_0(s,a_2) +\cdots\)</span>。</p><p>显然要使上面那一坨最大，我就要使最大那个q值最大的action: <spanclass="math inline">\(a&#39;\)</span>，让其权重分配为1: <spanclass="math inline">\(\pi(a&#39;|s)=1\)</span>。其余权重都为0即可。</p><p>那么这样就求出了<span class="math inline">\(\max_{\pi} \sum_{a}\pi(a|s) q(s,a)\)</span>，问题解决。</p><p>至此，我们已经可以通过BOE：<span class="math inline">\(v =f(v)\)</span>，求出最优state value: <spanclass="math inline">\(v^*\)</span>以及对应的最优policy: <spanclass="math inline">\(\pi^*\)</span>，算法如下：</p><ol type="1"><li>设定好<span class="math inline">\(\gamma\)</span>，<spanclass="math inline">\(r\)</span>，<spanclass="math inline">\(p(r|s,a)\)</span>，<spanclass="math inline">\(p(s&#39;|s,a)\)</span></li><li>随意取一个<span class="math inline">\(v_0\)</span>，然后通过<spanclass="math inline">\(q_\pi(s, a) = \sum_{r}p(r|s, a)r + \gamma\sum_{s&#39;}p(s&#39; | s, a)v_\pi(s&#39;)\)</span>算出对应的<spanclass="math inline">\(q_0\)</span></li><li>For each state <span class="math inline">\(s_i\)</span>, at time<span class="math inline">\(k\)</span>：<ul><li>算出<span class="math inline">\(q_{k}(s_i, a)\)</span></li><li>Find the <span class="math inline">\(a_k^*(s_i)\)</span>, s.t, <spanclass="math inline">\(q_k(s_i, a_k^*(s_i))\)</span>最大</li><li><span class="math inline">\(\pi_{k+1}(a|s_i)=\begin{cases} 1 \quada=a_k^*(s_i) \\ 0 \quad a \ne a_k^*(s_i) \end{cases}\)</span></li><li><span class="math inline">\(v_{k+1}(s_i) =\sum_{a}\pi_{k+1}(a|s)q_k(s,a)\)</span></li></ul></li></ol><p>这一小节的内容非常非常重要，对后续做数值仿真以及理解RL的基本思想很有意义。其实不难理解，静下心，多看几遍。</p><p>至此我们可以回答开头的那几个问题：</p><ol type="1"><li>最优policy当然存在。因为根据fixed point theorem，我们知道<spanclass="math inline">\(v^*\)</span>存在且唯一，那么其对应的policy就是<spanclass="math inline">\(\pi^*\)</span></li><li>不一定唯一。我们只能保证<spanclass="math inline">\(v^*\)</span>存在且唯一，但是其对应的policy不一定唯一。因为初始<spanclass="math inline">\(v_0\)</span>会不同</li><li>根据我们的algorithm，显然<spanclass="math inline">\(\pi^*\)</span>是deterministic的</li><li>根据上面给出的algorithm，即可得到最优策略</li></ol><h4 id="gamma-r的选择"><span class="math inline">\(\gamma,r\)</span>的选择</h4><p>通过前面的所学，我们知道我们需要确定好<spanclass="math inline">\(\gamma\)</span>, <spanclass="math inline">\(r\)</span>, <spanclass="math inline">\(p(r|s,a)\)</span>, <spanclass="math inline">\(p(s&#39;|s,a)\)</span>, <spanclass="math inline">\(v_0\)</span>，即可通过BOE求解出<spanclass="math inline">\(v^*, \pi^*\)</span></p><p><span class="math inline">\(v_0\)</span>是随便设定的，不用管；<spanclass="math inline">\(p(r|s,a)\)</span>, <spanclass="math inline">\(p(s&#39;|s,a)\)</span>通常是先验已知的，所以也不用管。</p><p>所以关键就是<span class="math inline">\(\gamma,r\)</span>这俩参数的选择会给我们的<span class="math inline">\(v_*,\pi^*\)</span>带来什么样的影响。</p><p><img src="9.png" style="zoom:67%;" /></p><p>看上面这个参数设置，算出来的<spanclass="math inline">\(\pi^*\)</span>左图，<spanclass="math inline">\(v^*\)</span>右图。可以发现，对于红色的几个点，它们都选择穿过forbidden到达targetcell。这是因为<spanclass="math inline">\(\gamma\)</span>的取值比较大，policy会比较远视。不会太计较短期的得失，而是考虑长期的回报。</p><p><img src="10.png" style="zoom:67%;" /></p><p>看上面这个参数设置，<spanclass="math inline">\(\gamma\)</span>变小了，所以policy会比较计较近处的得失，所以会绕过forbiddencell。</p><p><img src="11.png" style="zoom:67%;" /></p><p>再来看这个图，它的参数设置与第一幅图一样，但是把forbidden的惩罚增加了。这样同样可以达到绕过forbiddencell的效果。</p><p>这里补充一个知识，就是<spanclass="math inline">\(r\)</span>如果统一进行线性变化：<spanclass="math inline">\(r&#39; = ar +b\)</span>，其余条件不变的情况下，<spanclass="math inline">\(\pi^*\)</span>是不会有任何改变的。具体证明见书。</p><h4 id="总结-1">总结</h4><p>这一节所说的BOE就是在bellman equation前套了个<spanclass="math inline">\(\max_\pi\)</span></p><p>最重要的是要掌握fixed point theorem + Policy optimalitytheorem，这样就知道了为啥通过迭代即可算出来最优<spanclass="math inline">\(v^*, \pi^*\)</span>了</p><p>然后还需要掌握迭代算法的流程：</p><ol type="1"><li>设定好<span class="math inline">\(\gamma\)</span>，<spanclass="math inline">\(r\)</span>，<spanclass="math inline">\(p(r|s,a)\)</span>，<spanclass="math inline">\(p(s&#39;|s,a)\)</span></li><li>随意取一个<span class="math inline">\(v_0\)</span>，然后通过<spanclass="math inline">\(q_\pi(s, a) = \sum_{r}p(r|s, a)r + \gamma\sum_{s&#39;}p(s&#39; | s, a)v_\pi(s&#39;)\)</span>算出对应的<spanclass="math inline">\(q_0\)</span></li><li>For each state <span class="math inline">\(s_i\)</span>, at time<span class="math inline">\(k\)</span>：<ul><li>算出<span class="math inline">\(q_{k}(s_i, a)\)</span></li><li>Find the <span class="math inline">\(a_k^*(s_i)\)</span>, s.t, <spanclass="math inline">\(q_k(s_i, a_k^*(s_i))\)</span>最大</li><li><span class="math inline">\(\pi_{k+1}(a|s_i)=\begin{cases} 1 \quada=a_k^*(s_i) \\ 0 \quad a \ne a_k^*(s_i) \end{cases}\)</span></li><li><span class="math inline">\(v_{k+1}(s_i) =\sum_{a}\pi_{k+1}(a|s)q_k(s,a)\)</span></li></ul></li></ol><p>最后，要知道参数<span class="math inline">\(\gamma,r\)</span>的选择会对policy产生什么样的影响。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;基本概念、MDP、贝尔曼公式、贝尔曼最优公式&lt;/p&gt;</summary>
    
    
    
    <category term="1. 科研" scheme="http://error666.top/categories/1-%E7%A7%91%E7%A0%94/"/>
    
    <category term="强化学习" scheme="http://error666.top/categories/1-%E7%A7%91%E7%A0%94/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>微分方程2</title>
    <link href="http://error666.top/2024/10/02/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B2/"/>
    <id>http://error666.top/2024/10/02/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B2/</id>
    <published>2024-10-02T07:17:35.000Z</published>
    <updated>2024-10-04T15:52:49.915Z</updated>
    
    <content type="html"><![CDATA[<p>复数、复化解一阶常系数线性ODE、二阶常系数齐次线性ODE、二阶齐次线性ODE相关理论、二阶非齐次线性ODE相关理论</p><span id="more"></span><hr /><h3 id="一.-复数和复指数">一. 复数和复指数</h3><p>”数学就是一场符号的游戏，实际问题都是运用符号建的模。“</p><p>复数同理，虚部有什么实际含义吗？我想，目前暂且无需关心，当实际遇到需要建模的时候再谈。</p><p>那么，虚数就是<span class="math inline">\(i\)</span>，且满足<spanclass="math inline">\(i^2 = 1\)</span>。</p><p>所以复数可以这么表达：<span class="math inline">\(a +bi\)</span>，其中<span class="math inline">\(a\)</span>是实部，<spanclass="math inline">\(bi\)</span>是虚部。这是复数的代数表达形式。</p><p>聪明的人类希望可视化复数，于是用复平面（其实就是坐标系）来描述复数：</p><p><img src="1.png" style="zoom: 50%;" /></p><p>当我们把复数放到复平面的时候，我们就可以用坐标来描述一个复数：（a,b），<del>虽然没有啥人这么描述就是了。</del></p><p>人们更喜欢用极坐标去描述，因为在复平面上，如果知道了<spanclass="math inline">\(\theta\)</span>和<spanclass="math inline">\(r\)</span>，那么相当于就知道了一个复数。</p><p>可以发现，对于复数<span class="math inline">\(a +bi\)</span>，其模<span class="math inline">\(r = \sqrt{a^2 +b^2}\)</span>，其辐角<span class="math inline">\(\theta = \arctan\frac{b}{a}\)</span></p><p>所以，假设我知道了一个复数的模和辐角，那么其可以表示为：<spanclass="math inline">\(r(\cos \theta + i\sin\theta)\)</span>，这就是复数的极坐标表达形式。</p><p>Hummm，仍然不够简洁，right？</p><p>欧拉站出来了，给出了欧拉公式：<span class="math inline">\(\cos \theta+ i\sin \theta = e^{i\theta}\)</span></p><p>我觉得欧拉公式不要从等价推导的角度去理解它，而是要从映射的角度去理解它。如果两个东西，不管外界对它们对什么样的刺激，它们产生的影响都是相同的，那从抽象的角度我们就可以认为这两个东西是等价的。上面那个公式说的就是这么个事情。</p><p>欧拉公式的右侧，也就是指数，我们很熟悉了right？其至少满足两个性质：</p><ol type="1"><li><span class="math inline">\(e^x \cdot e^y = e^{x + y}\)</span></li><li><span class="math inline">\(\frac{dy}{dx} = ay, \quad y =e^{ax}\)</span></li></ol><p>那么假设欧拉公式是正确的，去推一推指数的这两个性质，发现也是满足的，那么我们就有理由的认为，这两者在抽象层面上是“等价的”，即这个公式是正确的。<span class="math display">\[\begin{align*}&amp;~~~~(\cos \theta_1 + i\sin \theta_1)(\cos \theta_2 + i\sin\theta_2) \\&amp;=\cos \theta_1 \cos \theta_2 - \sin \theta_1 \sin \theta_2 + i(\sin\theta_1 \cos \theta_2 + \cos \theta_1 \sin \theta_2) \\&amp;=\cos(\theta_1 + \theta_2) + i\sin(\theta_1 + \theta_2) \\&amp;=e^{i(\theta_1 + \theta_2)} \\\end{align*}\]</span>所以我们有理由说明，这个公式是正确的（严谨的证明这里没必要讨论）。</p><p>所以，有了欧拉公式的加持，我们可以将复数的极坐标形式改写，得到：<spanclass="math inline">\(re^{i\theta}\)</span>，这就是复数的指数表达形式。</p><p>至此，对于一个复数，我们有了三种表达方式：</p><ol type="1"><li><span class="math inline">\(a + bi\)</span></li><li><span class="math inline">\(r(\cos \theta + i \sin \theta), r =\sqrt{a^2 + b^2}, \theta = \arctan \frac{b}{a}\)</span></li><li><span class="math inline">\(re^{i\theta}, r = \sqrt{a^2 + b^2},\theta = \arctan \frac{b}{a}\)</span></li></ol><hr /><p>复数很有用，我们来看两个例子。</p><p>第一个是求解<span class="math inline">\(\int e^{-x}\cosx\mathrm{d}x\)</span></p><p>常规做法是两次分部积分法，但是可以将积分”复化“来做。</p><p>将<span class="math inline">\(\cos x\)</span>看作是复数<spanclass="math inline">\(e^{i\theta}\)</span>的实部，不妨记为<spanclass="math inline">\(Re(e^{ix})\)</span></p><p>复数如果乘一个实数，那么就是实部虚部分别乘这个实数，所以<spanclass="math inline">\(e^{-x}\cos x\)</span>，其实就是<spanclass="math inline">\(e^{-x} \cdote^{ix}\)</span>这个复数的实部，即<span class="math inline">\(Re(e^{-x +ix})\)</span></p><p>那么积分可写为：<span class="math inline">\(\int Re(e^{-x + ix})\mathrm{d}x\)</span></p><p><span class="math inline">\(Re\)</span>可提到积分号外面：<spanclass="math inline">\(Re \int e^{-x + ix} \mathrm{d}x\)</span></p><p>于是可得：<span class="math inline">\(Re(\frac{1}{i-1}e^{-x + ix} +c)\)</span></p><p>即求复数<span class="math inline">\(\frac{1}{i-1}e^{-x +ix}\)</span>的实部，（最后记得加个c），整理：<spanclass="math inline">\(e^{-x} \cdot \frac{\cos x + i\sin x}{i-1} = e^{-x}\cdot \frac{\cos x - \sin x + i(\cos x + \sin x)}{-2}\)</span></p><p>由于我们只需要实部，所以答案就是：<span class="math inline">\(e^{-x}\cdot \frac{\cos x - \sin x}{-2} + c\)</span></p><p>Humm，巧妙。</p><hr /><p>再来看一个例子，我们知道，对于<spanclass="math inline">\(\sqrt[n]{1}\)</span>，在实数范围内，如果n是奇数，那么只有一个解1，如果是正数，那么解为<spanclass="math inline">\(\pm1\)</span>。</p><p>但是在复数域，<spanclass="math inline">\(\sqrt[n]{1}\)</span>有n个解，这是很容易解释的，用复平面就可以很好的解释。</p><p>因为对于俩复数相乘，即<span class="math inline">\(r_1e^{i\theta_1}\cdot r_2e^{i\theta_2} =r_1r_2e^{i(\theta_1+\theta_2)}\)</span>，在复平面上来看，其实就是模相乘作为新的模，然后辐角相加作为新的辐角。</p><p>所以<spanclass="math inline">\(\sqrt[n]{1}\)</span>的解，其实就是n个自己相乘，最后在复平面上落到（1，0）处。</p><p>因为单位圆上的复数的模都是1，所以无需考虑模了。只需考虑辐角，哪些辐角的单位复数，n次自乘后会落到（1，0）？</p><p>答案是：<span class="math inline">\(e^{i \cdot 2\pi \cdot\frac{k}{n}}, k = 1,2,\cdots,n\)</span></p><p>即这n个复数，它们的n次方就是实数1。（从复平面角度考虑，模永远是1，但是辐角相加n次后都为<spanclass="math inline">\(2\pi\)</span>的倍数）</p><p>这n个复数恰好是单位圆上的n等分点。</p><h3 id="二.-复化解带三角函数的一阶常系数线性ode">二.复化解带三角函数的一阶常系数线性ODE</h3><p>在“微分方程1”中“一阶ODE解析法”的例3中，我们介绍了这种特殊的一阶线性ODE，其系数是常数，即一阶常系数线性ODE：<span class="math display">\[y&#39; + ky = kq(t)\]</span> 因为它毕竟是一阶线性ODE，所以可以用通法去解它。</p><p>在例3中，我们已经解出其通解为：<span class="math inline">\(\thereforeT = ke^{-kt}\int q(t) e^{kt} \mathrm{d}t + ce^{-kt}\)</span></p><p>但是当<spanclass="math inline">\(q(t)\)</span>为三角函数的时候，其实还可以将其“复化”去解决。</p><p>下面做一道例题：<span class="math inline">\(y&#39; + ky = k\coswt\)</span></p><p>看到三角函数，直接把它复化了：<span class="math inline">\(y&#39; + ky= k Re(e^{iwt})\)</span></p><p>一直带着<spanclass="math inline">\(Re\)</span>有点烦，所以不妨将方程左侧的解先换为“复数解”，则有：<spanclass="math inline">\(\tilde{y}&#39; + k\tilde{y} =ke^{iwt}\)</span></p><p>利用解一阶ODE的通法，解出<span class="math inline">\(u = e^{\int kdx}= e^{kt}\)</span></p><p><span class="math inline">\(\therefore (u\tilde{y})&#39; = ke^{iwt +kt}\)</span></p><p><span class="math inline">\(\therefore e^{kt} \cdot \tilde{y} =\frac{k}{iw + k}e^{iwt + kt} + c\)</span></p><p><span class="math inline">\(\therefore \tilde{y} = \frac{1}{1 +i(\frac{w}{k})}e^{iwt} + c\)</span></p><p>所以要不然就把<span class="math inline">\(\frac{1}{1 +i(\frac{w}{k})}\)</span>转为指数形式，要不然就把<spanclass="math inline">\(e^{iwt}\)</span>转为<span class="math inline">\(a+ bi\)</span>形式。</p><p>这里我们选用前者去做，<span class="math inline">\(1 +i\frac{w}{t}\)</span>是复数，<span class="math inline">\(\frac{1}{1 +i\frac{w}{t}}\)</span>显然也是复数，问题是它的模和辐角是多少？</p><p>因为复数相乘本质就是模相乘，辐角相加，而我们又知道<spanclass="math inline">\(\frac{1}{1 + i\frac{w}{t}} \cdot (1 +i\frac{w}{t}) = 1\)</span> <span class="math display">\[\therefore \begin{cases}    \arg(\alpha) + \arg(\frac{1}{\alpha}) = \arg(1) = 0 \\    \mod(\alpha) \cdot \mod(\frac{1}{\alpha}) = \mod(1) = 1 \\    \alpha = 1 + i\frac{w}{t}\end{cases}\]</span></p><p><span class="math display">\[\therefore \begin{cases}    \arg(\frac{1}{\alpha}) = -\arctan\frac{w}{t} = - \phi \\    \mod(\frac{1}{\alpha}) = \frac{1}{\sqrt{1 + (\frac{w}{t})^2}}\end{cases}\]</span></p><p><span class="math inline">\(\therefore \frac{1}{1 + i(\frac{w}{t})} =\frac{1}{\sqrt{1 + (\frac{w}{t})^2}}e^{-i\phi}, \phi =\arctan\frac{w}{t}\)</span></p><p><span class="math inline">\(\therefore \tilde{y} = \frac{1}{\sqrt{1 +(\frac{w}{t})^2}}e^{i(wt-\phi)} + c\)</span></p><p><span class="math inline">\(\therefore y = Re(\tilde{y}) =\frac{1}{\sqrt{1 + (\frac{w}{t})^2}} \cdot \cos(wt - \phi) + c, \quad\phi = \arctan\frac{w}{t}\)</span></p><hr /><p>ok，那现在换一种做法，也就是把<spanclass="math inline">\(e^{iwt}\)</span>转为<span class="math inline">\(a+ bi\)</span>形式。</p><p>回到这一步：<span class="math inline">\(\therefore \tilde{y} =\frac{1}{1 + i(\frac{w}{k})}e^{iwt} + c\)</span></p><p><span class="math inline">\(\therefore \tilde{y} = \frac{1 -i(\frac{w}{k})}{1 + (\frac{w}{k})^2} \cdot (\cos wt + i\sin wt) +c\)</span></p><p><span class="math inline">\(\therefore \tilde{y} = \frac{1}{1 +(\frac{w}{k})^2} \cdot (\cos wt + \frac{w}{k}\sin wt) + c\)</span></p><blockquote><p>补充：辅助角公式</p><p><span class="math inline">\(a\cos \alpha + b\sin \alpha = \sqrt{a^2 +b^2}\cos(\alpha - \phi), \phi = \arctan\frac{b}{a}\)</span></p><p>证明：</p><p><span class="math inline">\(a\cos \alpha + b\sin\alpha\)</span>可以表示为<span class="math inline">\((a - bi)(\cos\alpha + i\sin \alpha)\)</span>的实部</p><p>将这个复数指数化：<span class="math inline">\(\sqrt{a^2 +b^2}e^{-i\theta} \cdot e^{i\alpha} = \sqrt{a^2 + b^2} \cdot e^{i(\alpha- \theta)}\)</span></p><p>所以这个复数的实部就是：<span class="math inline">\(\sqrt{a^2 +b^2}\cos(\alpha - \theta), \theta =\arctan\frac{n}{a}\)</span>，证毕。</p><p>为什么复数指数化的时候是<spanclass="math inline">\(-\theta\)</span>？因为我们考虑符号的正负很烦，所以通常我们都假设<spanclass="math inline">\(a, b&gt;0\)</span>去做，那么复数<spanclass="math inline">\(a -bi\)</span>就在复平面的下方，那么对应的辐角就是一个负的，因为<spanclass="math inline">\(\phi =\arctan\frac{b}{a}\)</span>在假设下为正，所以要给它加个负号</p></blockquote><p><span class="math inline">\(\therefore \tilde{y} = \frac{1}{1 +(\frac{w}{k})^2} \cdot \sqrt{1 + (\frac{w}{k})^2} \cdot \cos(wt - \phi)+ c, \phi = \arctan\frac{w}{k}\)</span></p><p><span class="math inline">\(\therefore \tilde{y} = \frac{1}{\sqrt{1 +(\frac{w}{k})^2}} \cdot \cos(wt - \phi) + c, \quad \phi =\arctan\frac{w}{t}\)</span></p><h3 id="三.-二阶常系数齐次线性ode">三. 二阶常系数齐次线性ODE</h3><p>前面学习了：</p><ol type="1"><li>一阶线性ODE：<span class="math inline">\(y&#39; + p(x)y =q(x)\)</span></li><li>伯努利方程：<span class="math inline">\(y&#39; = p(x)y +q(x)y^n\)</span></li><li>一阶齐次ODE：<span class="math inline">\(y&#39; =f(\frac{y}{x})\)</span></li><li>一阶自治ODE：<span class="math inline">\(y&#39; = f(y)\)</span></li><li>一阶常系数线性ODE：<span class="math inline">\(y&#39; + ky =kq(x)\)</span></li></ol><p>今天来学习形如<span class="math inline">\(y&#39;&#39; + Ay&#39; + By=0\)</span>的方程，因为方程右边是0，所以叫齐次，所以这类方程叫：“二阶常系数齐次线性ODE”</p><p>首先先说结论，对于二阶ODE，其通解必然为：<spanclass="math inline">\(y = c_1y_1 + c_2y_2\)</span>，<spanclass="math inline">\(y_1,y_2\)</span>线性无关。想起了线性代数right？Ahahah</p><p>那么想解<span class="math inline">\(y&#39;&#39; + Ay&#39; + By =0\)</span>，我们的任务就是找出<span class="math inline">\(y_1,y_2\)</span></p><p>欧拉已经发现了，对于二阶常系数齐次线性ODE，其解形如：<spanclass="math inline">\(e^{rx}\)</span></p><p>将其代入试一下：<span class="math inline">\(r^2 \cdot e^{rx} + Ar\cdot e^{rx} + B \cdot e^{rx} = 0\)</span></p><p><span class="math inline">\(\therefore r^2 + Ar + B = 0\)</span></p><p>所以解出<spanclass="math inline">\(r\)</span>就行了，上面这个方程也叫二阶常系数齐次线性ODE的“特征方程”。</p><p>那么其实有几种情况：</p><ol type="1"><li><span class="math inline">\(r_1 \ne r_2\)</span>且都是realnumber</li><li><span class="math inline">\(r1, r2\)</span>都是复数</li><li><span class="math inline">\(r1 = r2\)</span>且是real number</li></ol><p>我们一个一个来分析。</p><p><strong>Case 1：</strong></p><p>首先先讨论第一种情况，此时俩特解就是<spanclass="math inline">\(e^{r_1x}, e^{r_2x}\)</span>，那么通解就是<spanclass="math inline">\(y = c_1e^{r_1x} + c_2e^{r_2x}\)</span></p><p><strong>Case 2：</strong></p><p>然后讨论第二种情况，此时不妨设特征方程的根是<spanclass="math inline">\(a \pm bi\)</span>，那么俩特解就是<spanclass="math inline">\(e^{(a+bi)x},e^{(a-bi)x}\)</span>，但是显然我们不希望通解里包含复数啊，怎么办呢？用下面这个定理</p><blockquote><p>定理：若<span class="math inline">\(u + vi\)</span>是<spanclass="math inline">\(y&#39;&#39; + Ay&#39; + By =0\)</span>的解，那么<span class="math inline">\(u,v\)</span>都是此方程的解</p><p>证明：</p><p><span class="math inline">\(\because (u+vi)&#39;&#39; + A(u+vi)&#39;+ B(u+vi) = 0\)</span></p><p><span class="math inline">\(\therefore (u&#39;&#39; + Au&#39; + Bu) +i(v&#39;&#39; + Av&#39; + Bv) = 0\)</span></p><p><span class="math inline">\(\therefore u, v\)</span> are thesolutions of the equation.</p></blockquote><p>而<span class="math inline">\(e^{(a\pmbi)x}\)</span>对应的复数是<span class="math inline">\(e^{ax} \cdot (\cosbx \pm i\sin bx)\)</span></p><p>所以<span class="math inline">\(e^{ax}\cos bx\)</span>和<spanclass="math inline">\(e^{ax}\sin bx\)</span>也是方程的俩特解（其实<spanclass="math inline">\(-e^{ax}\sinbx\)</span>也是，不过只需要俩线性无关的就行，所以任选一个）</p><p>所以通解为：<span class="math inline">\(y = e^{ax}(c_1\cos bx +c_2\sin bx)\)</span></p><p>（这里教授用弹簧-阻尼-木块模型描述了这个方程的物理现象，就是在不断震荡，趋近于稳态但不会到稳态。即震荡现象与特征方程复数根联系在一起）</p><p>除了用定理外，还有另一种方法同样可以得到实数解，回到得到俩特解<spanclass="math inline">\(e^{(a\pmbi)x}\)</span>这一步，那么通解可写为：<span class="math inline">\(y =c_1e^{(a+bi)x} + c_2e^{(a-bi)x}\)</span></p><p>这确实是通解，但是我们研究的问题是在实数域中的，所以我们希望求出实数通解，也就是令<spanclass="math inline">\(c_1, c_2\)</span>取某些值时，s,t, <spanclass="math inline">\(y\)</span>为实数</p><p>Well，这里用一个小trick，即实数的共轭复数就是它自己。所以假定<spanclass="math inline">\(y\)</span>为实数，然后取其共轭： <spanclass="math display">\[\overline{y} = \overline{c_1e^{(a+bi)x} + c_2e^{(a-bi)x}} =\overline{c_1e^{(a+bi)x}} + \overline{c_2e^{(a-bi)x}} =\overline{c_1}e^{(a-bi)x} + \overline{c_2}e^{(a+bi)x} = y\]</span> <span class="math inline">\(\therefore\overline{c_1}=c_2,\overline{c_2} = c_1\)</span></p><p>所以通解即为：<span class="math inline">\(y = (u + iv)e^{(a+bi)x} +(u - iv)e^{(a-bi)x}\)</span></p><p>工程领域的人很多人喜欢写成上面这个形式。</p><p>但是hummm，我还是觉得写为三角函数会更优雅直观些，我们来看看上面的形式如何转为三角形式</p><blockquote><p>补充：逆欧拉公式</p><p><span class="math inline">\(\cos \alpha = \frac{e^{i\alpha} +e^{-i\alpha}}{2}\)</span></p><p><span class="math inline">\(\sin \alpha = \frac{e^{i\alpha} -e^{-i\alpha}}{2i}\)</span></p></blockquote><p><span class="math display">\[\begin{align*}    y&amp;=(u + iv)e^{(a+bi)x} + (u - iv)e^{(a-bi)x} \\     &amp;=e^{ax}(ue^{ibx} + ive^{ibx} + ue^{-ibx} - ive^{-ibx}) \\     &amp;=e^{ax}\left(u(e^{ibx} + e^{-ibx}) + iv(e^{ibx} -e^{-ibx})\right) \\     &amp;=e^{ax}\left(2u\cos bx - 2v\sin bx\right)\end{align*}\]</span></p><p><strong>Case 3：</strong></p><p>最后讨论第三种情况，此时不妨设特征方程的重根为<spanclass="math inline">\(r\)</span>，那么特解就是<spanclass="math inline">\(e^{rx}\)</span>，另一个特解是啥呢？继续用一个定理</p><blockquote><p>定理：若知道了<span class="math inline">\(y&#39;&#39; + p(x)y&#39; +q(x)y = 0\)</span>的一个解<spanclass="math inline">\(y_1\)</span>，那么另一个解必然可以写成<spanclass="math inline">\(u(x)y_1\)</span></p></blockquote><p>ok，来找一下这个u吧！</p><p>首先我们的方程是<span class="math inline">\(y&#39;&#39; + Ay&#39; +By = 0\)</span>，然后其中一个解<spanclass="math inline">\(y_1\)</span>为<spanclass="math inline">\(e^{rx}\)</span>，另一个解<spanclass="math inline">\(y_2\)</span>为<spanclass="math inline">\(ue^{rx}\)</span></p><p>求出<span class="math inline">\(y, y_2&#39;, y_2&#39;&#39;\)</span><span class="math display">\[\begin{cases}    &amp;y_2 = ue^{rx} \\    &amp;y_2&#39; = u&#39;e^{rx} + ure^{rx} \\    &amp;y_2&#39;&#39; = u&#39;&#39;e^{rx} + 2u&#39;re^{rx} + ur^2e^{rx}\end{cases}\]</span> 因为是重根，所以<span class="math inline">\(A = -2r, B =r^2\)</span></p><p><span class="math inline">\(\therefore y_2&#39;&#39; + Ay_2&#39; +By_2 = u&#39;&#39;e^{rx} + 2u&#39;re^{rx} + ur^2e^{rx} - 2ru&#39;e^{rx}- 2r^2ue^{rx} + r^2ue^{rx} = 0\)</span></p><p><span class="math inline">\(\therefore u&#39;&#39;e^{rx} =0\)</span></p><p><span class="math inline">\(\therefore u = c_1x + c_2\)</span></p><p>因为我们只需要求出<spanclass="math inline">\(y_2\)</span>的一个特解，所以<spanclass="math inline">\(u\)</span>不妨取<spanclass="math inline">\(x\)</span>，这样即得到<spanclass="math inline">\(y_2 = xe^{rx}\)</span></p><p>所以通解为：<span class="math inline">\(y = c_1e^{rx} +c_2xe^{rx}\)</span></p><h3 id="四.-二阶齐次线性ode相关理论">四. 二阶齐次线性ODE相关理论</h3><p>本节讨论的方程形如：<span class="math inline">\(y&#39;&#39; +p(x)y&#39; + q(x)y = 0\)</span></p><p>这节课教授介绍了为什么二阶齐次线性ODE的通解是<spanclass="math inline">\(c_1y_1 +c_2y_2\)</span>，证明过程我这里就略了。</p><p>以及还提到了对通解的正交化。</p><p>什么意思呢？就是当你求出俩通解<spanclass="math inline">\(y\)</span>后，可以令<spanclass="math inline">\(y(0) = 1, y&#39;(0) =0\)</span>，解出一个特解，记为<spanclass="math inline">\(Y_1\)</span>；然后再令<spanclass="math inline">\(y(0) = 0, y&#39;(0) =1\)</span>，解出一个特解，记为<spanclass="math inline">\(Y_2\)</span>。</p><p>那么<span class="math inline">\(Y_1,Y_2\)</span>就是正交的，通解可以重新写为<span class="math inline">\(y =c_1Y_1 + c_2Y_2\)</span></p><p>这样有什么好处呢？</p><p>当你给出一个初始条件<span class="math inline">\(y(0) = a, y&#39;(0) =b\)</span>时，那么解就是<span class="math inline">\(aY_1 +bY_2\)</span></p><p>除了正交化，教授还讲了一个存在和唯一性定理：</p><blockquote><p>存在和唯一性定理：</p><p>对于二阶齐次线性ODE：<span class="math inline">\(y&#39;&#39; +p(x)y&#39; + q(x)y = 0\)</span>，若<span class="math inline">\(p,q\)</span>对<spanclass="math inline">\(x\)</span>连续，则当给定一组初始条件时，有且仅有一个解。</p></blockquote><h3 id="五.-二阶非齐次线性ode相关理论">五.二阶非齐次线性ODE相关理论</h3>]]></content>
    
    
    <summary type="html">&lt;p&gt;复数、复化解一阶常系数线性ODE、二阶常系数齐次线性ODE、二阶齐次线性ODE相关理论、二阶非齐次线性ODE相关理论&lt;/p&gt;</summary>
    
    
    
    <category term="4. 大学" scheme="http://error666.top/categories/4-%E5%A4%A7%E5%AD%A6/"/>
    
    <category term="数学" scheme="http://error666.top/categories/4-%E5%A4%A7%E5%AD%A6/%E6%95%B0%E5%AD%A6/"/>
    
    <category term="微分方程" scheme="http://error666.top/categories/4-%E5%A4%A7%E5%AD%A6/%E6%95%B0%E5%AD%A6/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/"/>
    
    
  </entry>
  
  <entry>
    <title>操作系统自学笔记</title>
    <link href="http://error666.top/2024/09/20/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/"/>
    <id>http://error666.top/2024/09/20/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/</id>
    <published>2024-09-20T13:53:43.000Z</published>
    <updated>2024-10-01T16:09:14.958Z</updated>
    
    <content type="html"><![CDATA[<p>文字、程序、图片来源包括但不限于蒋老师的课件、我OS/计组老师的课件、助教的实验文档、我的个人理解</p><span id="more"></span><hr /><h3 id="导言">导言</h3><ul><li>操作系统是一个典型的“system”——它完成对计算机硬件系统的抽象，提供应用程序的运行环境</li><li>从应用程序的视角看，操作系统定义了一系列的对象(进程/线程、地址空间、文件、设备……) 和操纵它们的 API(系统调用)。这组强大的 API把计算机的硬件资源有序地管理起来，它不仅能实现应用程序(浏览器、游戏……)，还支持着各类神奇的系统程序(容器、虚拟机、调试器、游戏外挂……)</li><li>从硬件的视角看，操作系统是一个拥有访问全部硬件功能的程序(操作系统就是个 C程序，不用怕)。硬件会帮助操作系统完成最初的初始化和加载，之后，操作系统加载完第一个程序后，从此作为“中断处理程序” 在后台管理整个计算机系统</li><li>操作系统为什么难学？最主要原因是操作系统里的主题很多，有些主题对大家来说并不太熟悉。例如，同学们到目前为止编写的大部分代码都是串行的，打个比方，就是写一个程序模仿“一个人”，一次执行一步动作。但操作系统引入了并发编程，也就是你需要协同多个共享内存的“多个人” 时，会遇到很多你也许意料之外的问题。</li><li>AMD是芯片公司造芯片的；x86是Intel提出的指令集，用在AMD造出的芯片上；windows/linux是操作系统</li></ul><h3 id="什么是程序如何理解程序">什么是程序？如何理解程序？</h3><h4 id="logisim_1.c">logisim_1.c</h4><ul><li><p>讲这个程序的目的，首先是为了让你学习宏的一些用法，其次是为了让你感受X-macro的设计美学(用于处理状态机)。</p></li><li><p>该程序的功能：两位bit，4状态循环模拟器。</p></li><li><p>X-macro 是一种在 C 和 C++编程中使用的技术，通过预处理器宏来简化代码的维护和扩展。它的核心思想是将重复代码的部分抽象为一个宏列表，从而避免硬编码多次相似的代码。X-macro技术主要用于需要在多个地方使用相同一组常量、结构或函数定义的情况，通常在枚举、状态机、错误代码处理等场景下非常有用。</p></li><li><p>你需要了解一下宏。</p></li><li><p>简单理解宏就是简单替换。</p></li><li><p>宏还可以带参数，例如<code>#define FUN(a, b) (a &gt; b ? a : b)</code>。</p></li><li><p>如果想输出参数的名字，例如<code>#define PRINT_VAR(var) printf(#var " = %d\n", var)</code></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;bits/stdc++.h&gt;</span></span></span><br><span class="line">using namespace <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> PRINT_VAR(var) (printf(#var <span class="string">&quot; = %d\n&quot;</span>, var))</span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="type">int</span> x = <span class="number">10</span>;</span><br><span class="line">    PRINT_VAR(x);</span><br><span class="line">    <span class="comment">// 宏会被展开为：printf(&quot;x = %d\n&quot;, x);</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>##起到拼接的作用，例如<code>#define DEFINE(X) static int X, X##1</code>。宏展开后相当于<code>int X, X1</code></p></li><li><p>下面让我们来阅读一下logisim.c</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;unistd.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> REGS_FOREACH(_) _(X) _(Y)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> RUN_LOGIC \</span></span><br><span class="line"><span class="meta">    X1 = (X ^ Y); \</span></span><br><span class="line"><span class="meta">    Y1 = (Y == 0 ? 1 : 0); \</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> DEFINE(X) static int X, X##1;</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> UPDATE(X) X = X##1;</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> PRINT(X) printf(#X <span class="string">&quot; = %d; &quot;</span>, X);</span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span> &#123;</span><br><span class="line">    REGS_FOREACH(DEFINE); <span class="comment">// 定义 X 和 Y</span></span><br><span class="line">    <span class="keyword">while</span> (<span class="number">1</span>) &#123; <span class="comment">// 无限循环，模拟时钟周期</span></span><br><span class="line">        RUN_LOGIC; <span class="comment">// 执行逻辑运算，计算 X1 和 Y1 的值</span></span><br><span class="line">        REGS_FOREACH(PRINT); <span class="comment">// 打印当前的 X 和 Y 的值</span></span><br><span class="line">        REGS_FOREACH(UPDATE); <span class="comment">// 更新 X 和 Y 为 X1 和 Y1</span></span><br><span class="line">        <span class="built_in">putchar</span>(<span class="string">&#x27;\n&#x27;</span>); <span class="comment">// 换行</span></span><br><span class="line">        sleep(<span class="number">1</span>); <span class="comment">// 程序暂停 1 秒，模拟时钟的周期</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><p><code>&lt;unistd.h&gt;</code> 是POSIX（可移植操作系统接口）标准中的头文件，提供了对各种操作系统服务的访问，如文件操作、进程控制、用户身份管理等。在上述程序中，<code>sleep</code>用到它了。</p></li><li><p><code>static</code> 在 C语言中用于声明具有静态存储类别的变量或函数，这些变量或函数的生命周期贯穿整个程序运行期间，但它们的作用域仅限于定义它们的文件内部，从而提供了数据的持久性和封装性，防止了全局命名空间的污染。</p></li><li><p>第一行<code>#define REGS_FOREACH(_) _(X) _(Y)</code>，这个宏的意思是对X和Y做名为_的函数操作。从main函数可以推理出，_是个函数名。</p></li><li><p>第二行的<code>RUN_LOGIC</code>意思就是根据X和Y算出X1、Y1的值。'\'表示续行</p></li><li><p>... ...</p></li><li><p>所以上面宏的逻辑就是，先写好各种功能宏（DEFINE、UPDATE、PRINT），功能宏传入的参数是一个变量，因为它是作用于一个变量的。</p></li><li><p>然后为了一次作用于多个变量，要再写个循环宏（REGS_FOREACH），循环宏传入的参数是一个功能宏，因为它展开就是把功能宏作用到各个变量上。所以各个变量的名字就要在循环宏里写好。</p><blockquote><p>练习：理解后，试着写出上述程序</p></blockquote></li></ul></li></ul><h4 id="logisim_2.c">logisim_2.c</h4><ul><li><p>讲这个程序的目的，是为了让你感受到管道符的魅力，以及前后端结合的魅力。</p></li><li><p>该程序功能：通过两个变量X、Y，4种状态对应数字0、1、2、3。控制对应的七个变量的输出（与下文的seven-seg.py配合使用）</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;unistd.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> DEFINE(x) static int x, x##1;</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> UPDATE(x) x = x##1;</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> RUN_LOGIC \</span></span><br><span class="line"><span class="meta">    x1 = (x ^ y); \</span></span><br><span class="line"><span class="meta">    y1 = (y == 0 ? 1 : 0); \</span></span><br><span class="line"><span class="meta">    A = (y == 0); \</span></span><br><span class="line"><span class="meta">    B = (x == y); \</span></span><br><span class="line"><span class="meta">    C = (1); \</span></span><br><span class="line"><span class="meta">    D = (x == 1); \</span></span><br><span class="line"><span class="meta">    E = (y == 0); \</span></span><br><span class="line"><span class="meta">    F = !(x == 1 &amp;&amp; y == 0); \</span></span><br><span class="line"><span class="meta">    G = (y == 0);</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> PRINT(x) printf(#x<span class="string">&quot; = %d; &quot;</span>, x);</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> REGS_FOREACH(_) _(x) _(y)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> OUTS_FOREACH(_) _(A) _(B) _(C) _(D) _(E) _(F) _(G)</span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span> &#123;</span><br><span class="line">    REGS_FOREACH(DEFINE);</span><br><span class="line">    OUTS_FOREACH(DEFINE);</span><br><span class="line">    <span class="keyword">while</span> (<span class="number">1</span>) &#123;</span><br><span class="line">        RUN_LOGIC;              <span class="comment">// 得到x1, y1, (ABCDEFG)由x,y得到</span></span><br><span class="line">        REGS_FOREACH(UPDATE);   <span class="comment">// 更新x, y</span></span><br><span class="line">        sleep(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        OUTS_FOREACH(PRINT);</span><br><span class="line">        <span class="built_in">putchar</span>(<span class="string">&#x27;\n&#x27;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>练习：试着自己写出这个程序（RUN_LOGIC部分可借鉴）</p></blockquote></li></ul><h4 id="seven-seg.py">seven-seg.py</h4><ul><li><p>该程序功能：接受一行字符串代表7个灯的亮灭情况，输出对应的数字</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"> </span><br><span class="line"><span class="comment"># \033[2J: 清楚整个屏幕</span></span><br><span class="line"><span class="comment"># \-33[1;1f: 将光标移到左上角</span></span><br><span class="line">TEMPLATE = <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">\033[2J\033[1;1f</span></span><br><span class="line"><span class="string">     AAAAAAAAA</span></span><br><span class="line"><span class="string">    BB       CC</span></span><br><span class="line"><span class="string">    BB       CC</span></span><br><span class="line"><span class="string">    BB       CC</span></span><br><span class="line"><span class="string">    BB       CC</span></span><br><span class="line"><span class="string">    DDDDDDDDD</span></span><br><span class="line"><span class="string">   EE       FF</span></span><br><span class="line"><span class="string">   EE       FF</span></span><br><span class="line"><span class="string">   EE       FF</span></span><br><span class="line"><span class="string">   EE       FF</span></span><br><span class="line"><span class="string">    GGGGGGGGGG</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span> </span><br><span class="line"></span><br><span class="line">BLOCK = &#123;</span><br><span class="line">    <span class="number">0</span>: <span class="string">&#x27;\033[37m░\033[0m&#x27;</span>,  <span class="comment"># \033[37m表示设置字体颜色为白色，░是一个字符，\033[0m表示重置颜色</span></span><br><span class="line">    <span class="number">1</span>: <span class="string">&#x27;\033[31m█\033[0m&#x27;</span>,  <span class="comment"># \033[31m表示设置字体颜色为红色，█是一个字符</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">VARS = <span class="string">&#x27;ABCDEFG&#x27;</span></span><br><span class="line"><span class="keyword">for</span> v <span class="keyword">in</span> VARS:</span><br><span class="line">    <span class="comment"># Example: globals()[&#x27;my_var&#x27;] = 42  # 动态创建一个名为 my_var 的全局变量并赋值为 42</span></span><br><span class="line">    <span class="built_in">globals</span>()[v] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 从 stdin 逐行读取</span></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    line = sys.stdin.readline().strip()  <span class="comment"># 使用 readline 逐行读取来自管道的数据</span></span><br><span class="line">    <span class="keyword">if</span> line:</span><br><span class="line">        <span class="built_in">exec</span>(line)</span><br><span class="line">        pic = TEMPLATE</span><br><span class="line">        <span class="keyword">for</span> v <span class="keyword">in</span> VARS:</span><br><span class="line">            pic = pic.replace(v, BLOCK[<span class="built_in">globals</span>()[v]]) <span class="comment"># &#x27;A&#x27; -&gt; BLOCK[A], ...</span></span><br><span class="line">        <span class="built_in">print</span>(pic)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Received line: <span class="subst">&#123;line&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure><blockquote><p>练习：看懂这个py程序</p></blockquote></li></ul><h4 id="hanoi-r.c">hanoi-r.c</h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">hanoi</span><span class="params">(<span class="type">int</span> n, <span class="type">char</span> from, <span class="type">char</span> to, <span class="type">char</span> via)</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (n == <span class="number">1</span>) <span class="built_in">printf</span>(<span class="string">&quot;%c -&gt; %c\n&quot;</span>, from, to);</span><br><span class="line">    <span class="keyword">else</span> &#123;</span><br><span class="line">        hanoi(n - <span class="number">1</span>, from, via, to);</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;%c -&gt; %c\n&quot;</span>, from, to);</span><br><span class="line">        hanoi(n - <span class="number">1</span>, via, to, from);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span> <span class="params">()</span> &#123;</span><br><span class="line">    hanoi(<span class="number">3</span>, <span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;C&#x27;</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>练习：自己写出这个汉诺塔程序</p></blockquote><ul><li><p>讲这个程序的目的，是为了让你会用vscodegdb调试，以及引出如何用cpu执行角度去理解c语言程序</p></li><li><p>首先你先自己写出这个程序</p></li><li><p>然后去配置一下vscode里的gdb，配置完了试下各种功能以及反汇编视图是否可用</p></li><li><p>几个快捷键要熟悉：F5启动调试，shift+F5关闭调试，F11单步调试(会进入函数内部)，F10逐过程(把函数当作一条语句处理)</p></li><li><p>然后思考一下带递归的程序是如何从汇编角度和cpu执行角度去理解：</p><ul><li><p>假设你写了一个带递归的程序hanio-r.c，然后你可以得到它的汇编。</p></li><li><p>回忆一下当时用vivado做计组项目的时候，是不是将一段汇编硬编码到rom里了？（哈佛结构）所以说，汇编程序本质上也是一段线性的，就是靠着PC在上下跳来跳去来实现的。</p></li><li><p>先思考下循环在汇编里的逻辑，本质就是PC加或者减某个数在每行的汇编里跳来跳去。</p></li><li><p>再思考下递归在汇编里的逻辑，递归里会开局部变量，这又是如何实现的呢？我以下面的代码来举例：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> <span class="title function_">fact</span><span class="params">(<span class="type">int</span> n)</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (n &lt; <span class="number">1</span>) <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">return</span> n * fact(n - <span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 参数n在寄存器x10中</span></span><br><span class="line"><span class="comment">// 为了尽可能少用通用寄存器，尽量复用，所以结果也分配在x10</span></span><br></pre></td></tr></table></figure></li><li><p>下面是对应的汇编：</p><p><img src="1.png" /></p></li><li><p>哈佛结构除了ROM里存汇编，RAM里是存数据的。数据存储本质上也是一个“数组”。你可以想象数组长度很长，其实的某一段拿来当栈空间，栈空间的起点就是sp，它是栈空间数组的下标。</p></li><li><p>cpu执行角度是没有什么全局变量/局部变量这种说法的，数据要么就是通用寄存器，那么就存在RAM里。sp在通用regs里。</p></li><li><p>除了sp外，通用寄存器里，我们还需要一个x1。x1是RAM里数组的一个下标，其对应的数据是“最后一次”发生跳转的那条汇编的PC值+4。</p></li><li><p>举个例子，比如我在主函数里调用了fact()函数，那么调用这条语句的PC加4之后的PC'，就被存到了data[x1]里。然后如果我想return回主函数，只需jalrx0, 0(x1)即可，那么PC就跳到了PC'。</p></li><li><p>所以，为了实现函数调用，x1和sp必不可少 。</p></li><li><p>那么局部变量和嵌套调用如何实现呢？</p></li><li><p>仔细看上面的汇编，进入函数的第一件事，是将当前的x1和x10(结果)存到RAM里的栈空间内，这样后续嵌套递归的话，x1和x10的值一定会被更改，但没事，因为我们已经将当前这一层调用的x1,x10存到RAM里了，当后续的调用return回当前层时，从RAM里重新取出即可。</p></li><li><p>听了我以上的讲解，相信我已经能完全看懂上面的汇编了，也就完全理解C语言里的递归在cpu执行角度是如何理解的了。</p></li><li><p>总结一下，cpu执行角度没有全局变量/局部变量之说，数据存储要不就在通用寄存器，要不就在RAM里。通用寄存器里俩很重要的分别是sp和x1，sp掌管着RAM里栈空间的下标，x1掌管着RAM里"调用PC+4"这个元素的下标。</p></li><li><p>递归本质就是一进入当前层时，把当前层信息(x1 +数据信息)压入栈中(sp减一个数)，当前层return的时候就弹栈(sp加回来)。如果在当前层嵌套调用了自己(语句1)，那么在当前层语句1之后的语句，若用到了当前层的数据，就从栈里取出来。不这么做的话，数据信息其实是已经被污染，因为CPU没有作用域这种说法。</p></li></ul></li><li><p>Well，了解了cpu执行角度的c语言程序，我们就可以做一个高抽象性的概括了，即：C语言本质就是状态机，状态为（通用寄存器,RAM），C语言里的每一条语句，即每一条汇编的执行，都是一个状态到另一个状态的改变。这个状态机不是自发运行的，而是按照ROM里的指令去实现状态之间的改变。</p></li></ul><h4 id="syscall">syscall</h4><ul><li>一条神奇的指令，即调用操作系统</li><li>从上面的讲解，我们知道，任意程序本质上就是对状态机（通用寄存器,RAM）状态改变的指挥棒。那么当一个程序调用syscall的时候，就会执行操作系统里的程序，也就是指挥棒交到了操作系统手上。</li><li>在程序眼里，操作系统就是syscall这条指令，syscall指令的各种api包罗万象。你想用屏幕？可以，syscall请求一下。你想与别的进程交互？可以，syscall请求一下。So，没什么神奇的，所有的程序与操作系统交互都是通过syscall。</li><li>由此，就实现了操作系统与任意程序的交互。</li></ul><h3 id="lab0-裸机程序">lab0-裸机程序</h3><h4 id="预备知识">预备知识</h4><ul><li><p>makefile<ahref="https://www.bilibili.com/video/BV1tyWWeeEpp/?spm_id_from=333.337.search-card.all.click&amp;vd_source=ca9a71bb3c1806ce48ae27d95e4e8bd0">快速入门</a>：</p></li><li><p><code>gcc test.c</code>：编译test.c并生成可执行文件a.out</p></li><li><p><code>gcc test.c -o yyy</code>：编译test.c并将可执行文件重命名为yyy</p></li><li><p><code>gcc test.c -c</code>：编译test.c但不链接，生成目标文件test.o</p></li><li><p><code>gcc test.c -c -o yyy</code>：编译test.c但不链接，将目标文件改名为yyy</p></li><li><p>最普通的makefile写法</p></li></ul><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">hello: hello.o print.o</span></span><br><span class="line">    gcc hello.o print.o -o hello</span><br><span class="line"></span><br><span class="line"><span class="section">hello.o: hello.c</span></span><br><span class="line">    gcc hello.c -c</span><br><span class="line"></span><br><span class="line"><span class="section">print.o: print.c</span></span><br><span class="line">    gcc print.c -c</span><br><span class="line">    </span><br><span class="line"><span class="section">clean:</span></span><br><span class="line">    rm -f *.o</span><br></pre></td></tr></table></figure><ul><li>.PHONY 后面跟着的东西，就可以使你makexxx的时候执行的是makefile里xxx的逻辑。</li></ul><hr /><ul><li><p>局部变量：函数结束就销毁</p></li><li><p>静态局部变量：静态局部变量永远在，直到程序结束。但仍只能被本函数访问</p></li><li><p>全局变量：任何文件，只要通过extern关键字，就可以访问它</p></li><li><p>静态全局变量：只有本文件能够访问它</p></li><li><p>静态变量 = 静态局部变量 + 静态全局变量</p></li></ul><hr /><ul><li><code>objdump -h test.o</code>：查看目标文件test.o的段头<ul><li>什么是段头？就是程序编译过后的ELF文件的头（元数据）</li><li><code>.text</code>段：存放程序编译后生成的机器码</li><li><code>.data</code>段：存放已初始化过的死不了的东西，即初始化过的全局变量和静态变量<ul><li>在ELF文件中，已经记录了这些变量的初始值，运行时便会加载到内存中</li></ul></li><li><code>.bss</code>段：存放未初始化过的死不了的东西，即未初始化过的全局变量和静态变量<ul><li>在ELF文件中，没有这些变量的初始值，但是会告诉OS在运行时将他们初始化为0/NULL</li></ul></li><li><code>.rodata</code>段：存在只读的常量，若试图修改，会导致运行错误</li></ul></li></ul><h4 id="实现裸机hello-world">实现裸机Hello world!</h4><ul><li>我们以往写用户程序时，通常都只关注代码本身，而将运行时的环境交给了编译器等系统软件进行处理，但我们若要编写裸机程序，就需要进一步揭开运行时环境的神秘面纱。</li><li>下表揭示了裸机程序与用户程序的区别：</li></ul><table><colgroup><col style="width: 9%" /><col style="width: 48%" /><col style="width: 42%" /></colgroup><thead><tr class="header"><th>对比对象</th><th>裸机程序</th><th>常规用户程序</th></tr></thead><tbody><tr class="odd"><td>内存地址空间</td><td>自行管理物理地址空间，可以自行对虚拟内存进行配置后使自己运行在虚拟地址空间</td><td>由操作系统管理的虚拟地址空间（不考虑Linux NOMMU模式）</td></tr><tr class="even"><td>系统调用</td><td>调用自己</td><td>调用更高特权级的操作系统/固件</td></tr><tr class="odd"><td>栈的初始化</td><td>自行完成</td><td>操作系统载入用户进程时完成（毕竟还要通过栈传递参数）</td></tr><tr class="even"><td>BSS段的清空</td><td>自行完成</td><td>操作系统分配虚拟页面时完成清零</td></tr></tbody></table><ul><li>ok，那开始吧！首先先写一个用于初始化的汇编代码。（用于初始化虚实地址映射方式、栈初始化、执行main）</li><li>注释写的非常详细了，请仔细阅读：</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line"># 告诉汇编器main这个东西是外部C语言的main函数</span><br><span class="line"># 不用担心路径，因为在makefile的时候会把C程序和汇编程序绑在一起</span><br><span class="line"># 也不用担心有多个C程序从而有多个main，如果是这样的话会报错的，因为一个项目里只能有一个main函数</span><br><span class="line">.extern main</span><br><span class="line"></span><br><span class="line"># .text 伪指令用于指定代码段的开始，通常放在需要生成机器码的指令之前</span><br><span class="line">.text</span><br><span class="line"></span><br><span class="line"># 声明_start为全局符号</span><br><span class="line">.globl _start</span><br><span class="line"></span><br><span class="line">_start:</span><br><span class="line">    # li.w的意思是load immediate word，就是加载一个立即数的意思，就是把0xa0000011加载到t0里</span><br><span class="line">    # 为什么要选择t0? 没有什么特殊含义，因为t0是通用寄存器，用于存储临时数据，所以顺手就用它了</span><br><span class="line">    # 0xa0000011 = 1010 0000 0000 0000 0000 0000 0001 0001</span><br><span class="line">    li.w    $t0, 0xa0000011</span><br><span class="line"></span><br><span class="line">    # 什么是CSR?（Control and Status Registers）</span><br><span class="line">    # 简单理解，CSR就是一组寄存器，跟registers里的寄存器一样，只不过这组寄存器的不同的取值会使cpu做一些事情（具体请看龙芯32位手册_v1.02的p65）</span><br><span class="line">    # 需要通过特殊的命令，例如csrwr, csrrw来读取和写入</span><br><span class="line">    # 下面这句话就是说，把t0的值写入地位为“0x180”的这个CSR里。csrwr的意思是csr write</span><br><span class="line">    </span><br><span class="line">    # 然后补一下关于虚拟地址、物理地址、地址映射的知识</span><br><span class="line">    /*</span><br><span class="line">        * 物理地址：是内存的实际地址，即内存芯片上的实际位置</span><br><span class="line">        * 虚拟地址：由程序生成的地址，不是实际的物理地址</span><br><span class="line">        * 地址映射：将虚拟地址转换为物理地址的过程。这种映射通常由硬件（如 MMU，内存管理单元）和操作系统共同完成</span><br><span class="line">        * 龙芯中，mmu支持两种翻译模式：直接地址翻译(直接窗口)、映射地址翻译(页表)。优先级直接 &gt; 页表</span><br><span class="line">        * 直接窗口：直接窗口是一种地址映射机制，用于将虚拟地址直接映射到物理地址</span><br><span class="line">        * 配置直接窗口：直接窗口是一种机制，所以我们需要配置其具体机制的设置，那么在龙芯里，就通过地址为“0x180”这个csr（其实叫CSR_DMWIN0）来</span><br><span class="line">                      配置第0个直接窗口的设置</span><br><span class="line">     */</span><br><span class="line">    /*</span><br><span class="line">        * 知道了CSR_DMWIN0这个csr寄存器是第0个直接窗口的配置信息之后，我们需要了解具体什么数对应着什么配置：</span><br><span class="line">        * 参考龙芯32位手册_v1.02的p76</span><br><span class="line">        * 第31-29位：虚地址的[31:29]位</span><br><span class="line">        * 第27-25位：物理地址的[31:29]位</span><br><span class="line">        * 第3位：为1表示在特权等级PLV3(用户态)下可以使用该直接窗口进行地址翻译</span><br><span class="line">        * 第0位：为1表示在特权等级PLV0(内核态)下可以使用该直接窗口进行地址翻译</span><br><span class="line">        * ... ...</span><br><span class="line">     */</span><br><span class="line">    csrwr   $t0, 0x180  # 所以这里CSR_DMWIN0被配置为：“起始地址0xa0000000, PLV0时该直接窗口激活”</span><br><span class="line"></span><br><span class="line">    # 下面这两行跟上面两行一样，只不过是配置CSR_DMWIN1，它被配置为：“起始地址0x80000000, PLV0时该直接窗口激活”</span><br><span class="line">    # 0x80000001 = 10000000000000000000000000000001</span><br><span class="line">    li.w    $t0, 0x80000001</span><br><span class="line">    csrrw   $t0, 0x181</span><br><span class="line"></span><br><span class="line">    # -----------------------------------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">    li.w    $t0, 0xb0   # 0000 0000 0000 0000 0000 0000 1011 0000</span><br><span class="line">    # 地址为0x0的csr是CSR_CRMD，所以我们需要了解CSR_CRMD（Control and Status Register for Control and Management）是什么</span><br><span class="line">    # 详细参考龙芯32位手册_v1.02的p67</span><br><span class="line">    /*</span><br><span class="line">        * PLV(Privilege Level)</span><br><span class="line">            位范围：第1-0位</span><br><span class="line">            取值：</span><br><span class="line">                00：内核模式（内核模式下的代码可以执行所有操作，包括直接访问硬件资源、修改系统状态等）</span><br><span class="line">                11：用户模式（用户模式下的代码不能执行某些敏感操作，如直接访问硬件资源、修改关键系统状态等）</span><br><span class="line">        * DA</span><br><span class="line">            位范围：第3位</span><br><span class="line">            取值：</span><br><span class="line">                0：静默使用直接翻译（实际上当DA=0, PG=1时仍然会静默开启直接翻译）</span><br><span class="line">                1：启用使用直接翻译</span><br><span class="line">        * PG(Page Enable)</span><br><span class="line">            位范围：第4位</span><br><span class="line">            取值：</span><br><span class="line">                0：禁用分页翻译</span><br><span class="line">                1：启动分页翻译（直接翻译仍会起效且优先级比页表翻译高）</span><br><span class="line">        * ... ...</span><br><span class="line">     */</span><br><span class="line">    csrwr   $t0, 0x0    # 所以这里CSR_CRMD被配置为：“内核模式 + 启用分页翻译”</span><br><span class="line"></span><br><span class="line">    # -----------------------------------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">    # la的意思是load address</span><br><span class="line">    # 初始化栈指针为bootstacktop这个地址</span><br><span class="line">    la  $sp, bootstacktop</span><br><span class="line"></span><br><span class="line">    # 将main标签的地址给t0</span><br><span class="line">    la  $t0, main</span><br><span class="line">    # 跳转到main标签的地址，即开始执行main函数</span><br><span class="line">    jr  $t0</span><br><span class="line"></span><br><span class="line"># 当main函数执行完后，会回到汇编这里，然后_stack就执行完了，自然会执行到poweroff这里</span><br><span class="line">poweroff:</span><br><span class="line">    b poweroff  # b是branch的意思，即无条件跳转到poweroff，形成死循环</span><br><span class="line">    # 通过 b poweroff 形成的无限循环，可以确保程序在 main 函数返回后不会继续执行无效的代码，防止未定义行为的发生。</span><br><span class="line"></span><br><span class="line"># ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span><br><span class="line"></span><br><span class="line"># _stack是一个标签，没用</span><br><span class="line">_stack:</span><br><span class="line"></span><br><span class="line"># 切换段为数据段</span><br><span class="line"># 需要注意.data里的分配动作在链接阶段就会分配内存空间</span><br><span class="line">.section .data</span><br><span class="line"></span><br><span class="line"># 声明bootstack为全局符号</span><br><span class="line">.global bootstack</span><br><span class="line">bootstack:</span><br><span class="line">    .space 1024 # 从bootstack这个地址开始，分配1024个空间</span><br><span class="line"></span><br><span class="line"># 声明bootstacktop为全局符号</span><br><span class="line">.global bootstacktop</span><br><span class="line">bootstacktop:</span><br><span class="line">    .space 64   # 从bootstacktop这个地址开始，分配64个空间</span><br><span class="line"></span><br><span class="line">/*</span><br><span class="line">    更多相关知识介绍：https://osdocs.cqu.ai/lab0/intro/#io</span><br><span class="line"></span><br><span class="line">    总结：</span><br><span class="line">    这段汇编首先配置了 PLV0下的俩直接翻译窗口CSR_DMWIN0, CSR_DMWIN1，即分别将虚拟地址0xa0000011、0x80000000映射到物理地址上</span><br><span class="line">    然后设置了CSR_CRMD，即启用PLV0内核模式 + 启用分页翻译模式（此时也会静默启动直接翻译，因为其优先级高）</span><br><span class="line">    然后初始化了栈顶地址($sp)，栈的大小在.data中已设置</span><br><span class="line"> */</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;文字、程序、图片来源包括但不限于蒋老师的课件、我OS/计组老师的课件、助教的实验文档、我的个人理解&lt;/p&gt;</summary>
    
    
    
    <category term="4. 大学" scheme="http://error666.top/categories/4-%E5%A4%A7%E5%AD%A6/"/>
    
    <category term="计算机专业课" scheme="http://error666.top/categories/4-%E5%A4%A7%E5%AD%A6/%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%93%E4%B8%9A%E8%AF%BE/"/>
    
    
  </entry>
  
  <entry>
    <title>机器学习课程自学笔记</title>
    <link href="http://error666.top/2024/09/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/"/>
    <id>http://error666.top/2024/09/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/</id>
    <published>2024-09-19T04:25:47.000Z</published>
    <updated>2024-10-03T07:45:41.271Z</updated>
    
    <content type="html"><![CDATA[<p>参考内容：《机器学习》周志华</p><span id="more"></span><hr /><p>成绩构成：</p><ol type="1"><li>考勤、作业、研讨：10%</li><li>项目：40%</li><li>期末：50%</li></ol><h3 id="一.-绪论">一. 绪论</h3><ol type="1"><li><p>根据训练数据是否拥有标记数据，学习任务大致可分为两类：监督学习、无监督学习。</p><ul><li>分类和回归是前者的代表，聚类是后者的代表</li><li>聚类意思是在训练过程中，机器会自动的对事物的潜在概念进行划分，并把物体分成若干组</li></ul></li><li><p>模型适用于新样本的能力，称为泛化能力</p></li><li><p>通常假设样本空间中全体样本服从一个位置分布<spanclass="math inline">\(\mathcal{D}\)</span>，我们获得的每个样本都是独立地从这个分布上采样得到的，即“独立同分布”。</p></li><li><p>假设空间</p><ul><li>简单理解，就是所有输入的状态</li><li>书中举了个例子，有A、B、C三种属性，分别有3、2、2种取值方式。学习目标是某个状态<spanclass="math inline">\((a, b, c)\)</span>是否是牛逼的？求所有状态数。<ul><li>对于属性A，其实有4种状态，<span class="math inline">\(a_1, a_2, a_3,*\)</span>，<spanclass="math inline">\(*\)</span>表示这个属性取什么无所谓。对于B、C属性同理</li><li>那么状态数就有：<span class="math inline">\(4 * 3 * 3 =36\)</span>种</li><li>其实还漏了一种，还有一种状态是世界上没有"牛逼"这个概念，也就是<spanclass="math inline">\(\phi\)</span>状态。</li><li>所以这个例子的总状态数是37种。</li><li>我来列举其中的几种：<ul><li>A是<span class="math inline">\(a_1\)</span>，B是<spanclass="math inline">\(b_2\)</span>，C是<spanclass="math inline">\(c_3\)</span>时，是牛逼的</li><li>A是<span class="math inline">\(*\)</span>，B是<spanclass="math inline">\(b_1\)</span>，C是<spanclass="math inline">\(*\)</span>时，是牛逼的</li><li><span class="math inline">\(\cdots\)</span></li><li>世界上没有"牛逼"这个东西</li></ul></li></ul></li></ul></li><li><p>版本空间</p><ul><li><p>简单理解，就是把假设空间中不符合样本的所有假设剔除掉的空间</p></li><li><p>以书中例子为例</p><ul><li>根据表1.1，我们知“好瓜”的概念是成立的，所以先删除 <spanclass="math inline">\(\phi\)</span> 的假设</li><li>根据样本（（色泽=青绿）<sup>（根蒂=蜷缩）</sup>（敲声=浊响））——&gt;好瓜，删除所有状态对不上的假设</li><li>根据样本（（色泽=乌黑）<sup>（根蒂=蜷缩）</sup>（敲声=浊响））——&gt;好瓜，删除所有状态对不上的假设</li></ul><blockquote><p>这里把（（色泽=乌黑）<sup>（根蒂=蜷缩）</sup>（敲声=浊响））删除，这个和样本2符合，不要觉得心虚，因为利用样本2进行删除的时候也会删掉（（色泽=青绿）<sup>（根蒂=蜷缩）</sup>（敲声=浊响））这样刚好留下了（（色泽=*）<sup>（根蒂=蜷缩）</sup>（敲声=浊响）</p></blockquote><ul><li>根据样本（（色泽=青绿）<sup>（根蒂=硬挺）</sup>（敲声=清脆））——&gt;不是好瓜，删除所有状态对上的假设</li><li>根据样本（（色泽=乌黑）<sup>（根蒂=稍蜷）</sup>（敲声=沉闷））——&gt;不是好瓜，删除所有状态对上的假设</li><li>所以最后剩下了三个假设，这三个假设我们称之为版本空间：(色泽 = <em>,根蒂 = 蜷缩, 敲声 = </em>)、(色泽 = <em>, 根蒂 = </em>, 敲声 =清脆)、(色泽 = *, 根蒂 = 蜷缩, 敲声 = 清脆)</li></ul></li></ul></li><li><p>归纳偏好</p><ul><li>现实问题中，我们常面临很大的假设空间，但学习过程是根据有限的样本训练集进行的，那么对于不同版本的训练集，就会有不同的版本空间。版本空间内每一个假设都可以判断上面数据集中的每一条数据，是好瓜还是不是好瓜，但是用不同的假设判断一条新数据可能会得出不一样的结果，这就属于“归纳偏好”。</li></ul></li></ol><h3 id="二.-模型评估与选择">二. 模型评估与选择</h3><ol type="1"><li>精度 = 1 - 错误率</li><li>学习器在训练集上的误差叫“训练误差”或“经验误差”</li><li>学习器在新样本上的误差叫“泛化误差”。显然，我们想要泛化误差小的学习器。</li><li>过拟合是指训练误差很小，但是泛化误差不理想。欠拟合是指俩误差都不理想。</li></ol><h4 id="模型评估方法">模型评估方法</h4><ul><li><p>评估方法存在的意义：实际中，我们不可能直接拿到泛化误差，因为泛化误差是指实际情况的误差。产品都没开发出来咋获得嘛。而训练误差又由于过拟合现象的存在而不适合作为标准。所以，这时候就需要设计一些精妙的评估方法</p></li><li><p>留出法</p><ul><li>将数据集D切为训练集S和验证集T</li><li>需要注意，S和T的划分要尽可能保持数据分布的一致性。例如在分类任务中至少要保证样本的类别比例相似</li><li>为了结果可靠，可以进行多次留出法，用平均值作为最终结果</li><li>留出法的缺点：我们希望的是评估用D训练出的模型的性能，但留出法本质上是评估的S训练出来的模型的性能。这就会陷入一个窘境：若S包含绝大多数样本，虽然S与D的差距拉近，但是T太小导致评估结果可能不稳定；若S太少，S与D的差距就更远了。所以这个bug没有完全的解决方案，常见做法是将约1/5~ 1/3的样本用于测试，剩下的用于训练</li></ul></li><li><p>交叉验证法</p><ul><li>将D划分为k个大小相似的互斥子集：<span class="math inline">\(D = D_1\cup D_2 \cup \cdots \cup D_k\)</span>。然后枚举<spanclass="math inline">\(D_i\)</span>，每次(全集 - <spanclass="math inline">\(D_i\)</span>)作为训练集，<spanclass="math inline">\(D_i\)</span>作为验证集。进行k次训练测试，最后返回k次结果的均值。</li><li>可以发现，交叉验证法的稳定性和保真性很大程度取决于k，通常取10，称为10折交叉验证</li><li>需要注意，<spanclass="math inline">\(D_i\)</span>尽可能保持数据分布的一致性</li><li>为了结果可靠，可以进行多次交叉验证法，用均值作为最终结果。常见的有：10次10折交叉验证</li><li>（当k = D样本数量时，是交叉验证法的一个特例，称为留一法）</li></ul></li><li><p>自助法</p><ul><li>无论是留出法还是交叉验证法，S !=D，所以必然会存在一定偏差。留一法虽然可以使得S <spanclass="math inline">\(\to\)</span>D，但是训练集太大计算复杂度太高。有没有两全其美的方法呢——自助法。</li><li>做法：假设D样本数为m，则进行放回随机采样m次，得到D'。用D'作为训练集，D'作为验证集。</li><li>样本在m次采样始终不被采到的概率：</li></ul><p><span class="math display">\[\lim_{m\mapsto\infty}\left(1-\frac1m\right)^m\mapsto\frac1e\approx0.368\]</span></p><ul><li>即数据集D中约36.8%的样本不会出现在D'中。</li><li>优点：评估的模型和期望评估的模型都使用了m个样本，但仍有1/3的数据供我们验证。在数据集较小或难以划分训练/验证集时很有用。</li><li>缺点：自助法生产的数据集改变了初始数据集分布，会引入估计偏差。因此数据量充足时，留出法和交叉验证法更常用</li></ul></li><li><p>在进行完模型评估（也就是训练和评估）后，需要再将数据集D全部丢进模型训练一次。这么做是因为模型评估时S!= D。</p></li></ul><h4 id="性能度量">性能度量</h4><ul><li>当得到一个模型后，如何评估它的泛化能力呢？显然需要去度量它的性能，衡量模型泛化能力的评价标准，就叫性能度量。不同的性能度量往往会导致不同的评判结果。</li><li>回归任务最常用的性能度量是“均方误差”：<ul><li>离散：<span class="math inline">\(E(f; D) =\frac1m\sum_{i=1}^m(f(x_i) - y_i)^2\)</span></li><li>一般：<span class="math inline">\(E(f; \mathcal{D}) = \int_{x \sim\mathcal{D}}(f(x) - y)^2p(x)dx\)</span></li></ul></li><li>ok，接下来介绍分类任务的性能度量</li></ul><ol type="1"><li><p>错误率和精度</p><ul><li>错误率定义<ul><li>离散：<span class="math inline">\(E(f; D) =\frac1m\sum_{i=1}^m\mathbb{I}(f(x_i) \ne y_i)\)</span></li><li>一般：<span class="math inline">\(E(f; \mathcal{D}) = \int_{x \sim\mathcal{D}}\mathbb{I}(f(x)\ne y)p(x)dx\)</span></li></ul></li><li>精度定义<ul><li>离散：<span class="math inline">\(acc(f; D) =\frac1m\sum_{i=1}^m\mathbb{I}(f(x_i)=y_i)=1-E(f; D)\)</span></li><li>一般：<span class="math inline">\(acc(f; \mathcal{D}) = \int_{x \sim\mathcal{D}}\mathbb{I}(f(x)=y)p(x)dx=1-E(f; \mathcal{D})\)</span></li></ul></li></ul></li><li><p>查准率、查全率、F1</p><ul><li>举个例子，若我们关心“挑出的西瓜中有多少比例是好瓜”以及“所有好瓜中有多少比例被挑了出来”。前者我们用“查准率”(precision)描述，后者用"查全率"(recall)来描述。</li><li>对二分类来说，我们将预测结果抽象为混淆矩阵：</li><li><img src="1.png" /></li><li>（TP, true positive表示它确实是正例，表示我们预测对了。TN, truenegative表示它确实是反例，表示我们预测对了。）</li><li>显然$TP + FN + FP + TN = $样例总数</li><li>查准率：<span class="math inline">\(P = \frac{TP}{TP +FP}\)</span></li><li>查全率：<span class="math inline">\(R = \frac{TP}{TP +FN}\)</span></li><li>可以发现，查准率和查全率都兼顾有些困难。因为如果想让查全率高，那么就要增加选瓜数量，但是选瓜数量增加后，选出的瓜中是好瓜的概率可能就下降，即查准率下降；若希望选出的好瓜比例高，那么只挑选最有把握的瓜，这样难免就会漏掉不少好瓜，即查全率较低。通常只有在一些简单任务中，才能使P和R都很高。</li><li>所以，有没有直观的比较方法呢——PR图。</li><li>我们根据学习器的预测结果对样例排序，＂最可能＂是正例的排在最前边或者说最左边，＂最不可能＂是正例的排在最后边或者说最右边．按此顺序逐个把样本作为正例进行预测，每次计算测试样本的查准率和查全率并把这两项作为PR曲线的纵轴和横轴。</li><li><img src="3.png" /></li><li>显然若一条曲线包住了另一条曲线，那么说明它在任意时刻表现都好。如果俩曲线有相交，那么就比较下俩曲线所形成的面积，谁大谁牛逼。</li><li>当然面积可能不好算，所以我们直接用“平衡点”（BEP, 即P =R时的R坐标）来衡量，谁平衡点大谁牛逼。但是平衡点这方法还是太简陋了，所以我们使用F1度量（谁大谁牛逼）：</li></ul><p><span class="math display">\[\frac{1}{F_1} = \frac12\cdot (\frac1P + \frac1R)\]</span></p><ul><li>为了更定制化，还可以使用<spanclass="math inline">\(F_\beta\)</span>度量（<spanclass="math inline">\(F_1\)</span>是调和平均，<spanclass="math inline">\(F_\beta\)</span>是加权调和平均）：</li></ul><p><span class="math display">\[\frac{1}{F_\beta} = \frac{1}{1+\beta^2}\cdot (\frac1P +\frac{\beta^2}{R})\]</span></p><ul><li><spanclass="math inline">\(\beta\)</span>度量了查全率对查准率的相对重要性，<spanclass="math inline">\(\beta=1\)</span>为一样重要，<spanclass="math inline">\(\beta &gt; 1\)</span>表示我们更重视查全率。</li><li>如果有多个混淆矩阵呢？</li><li>第一种方法，直接对所有P、R、F1取均值作为最终结果。这样得到的结果叫做：宏查准率、宏查全率、宏F1</li><li>第二种方法，先对所有混淆矩阵对应四个位置取均值，再算出对应的P、R、F1。这样得到的结果叫做：微查准率、微查全率、微F1</li></ul></li><li><p>ROC和AUC</p><ul><li>ROC曲线跟PR曲线绘制流程一样，只是横坐标换为了“假正例率”（FPR），纵坐标换为了“真正例率”（TPR）。</li><li><span class="math inline">\(TPR = \frac{TP}{TP + FN}, \quad FPR =\frac{FP}{FP + FN}\)</span></li><li>TPR表示对于全部好瓜，你预测对了百分之TPR；FPR表示对于全部坏瓜，你预测错了百分之FPR。</li><li>显然TPR越高越好，FPR越低越好。</li><li>ROC曲线画出来的感觉如下：</li><li><img src="2.png" /></li><li>AUC是ROC曲线的面积。显然如果一个曲线包住另一个，但它就更牛逼。如果俩线有相交，那么就看看谁的AUC更大，越大越牛逼。</li></ul></li><li><p>代价敏感错误率和代价曲线</p><ul><li>在现实中同样是判断错误，但是后果可能不同。比如门禁系统错误的把陌生人放进来的危害肯定比把可通行人拦在外边危害更大。所以同样是判断错误，我们需要赋予其不同的权值。最后的目标是使平均代价最小。</li><li>所以可以抽象出代价矩阵的概念：</li><li><img src="4.png" /></li><li>代价敏感错误率：</li></ul><p><span class="math display">\[E(f; D; cost) = \frac1m(\sum_{x_i \in D^+}\mathbb{I}(f(x_i) \ne y_i)\times cost_{01} + \sum_{x_i \in D^-}\mathbb{I}(f(x_i) \ne y_i) \timescost_{10})\]</span></p><ul><li>当不同后果的权重不同时，上面说的ROC曲线就不能直接反映出学习器的期望总体代价了。所以这时候我们需要“代价曲线”。</li><li>略</li></ul></li></ol><h4 id="比较检验">比较检验</h4><ul><li><p>目前，我们已经可以使用某种模型评估方法，测出某个性能度量的结果。</p></li><li><p>但是泛化性能是对新样本进行预测的性能，新样本看成一个总体，那么这个总体我们永远无法完整获得，也就是真实泛化性能永远也不知道是多少。从这个总体中抽样得到一个样本集合，也就是我们通常说的“测试集”，很显然，每次抽样，获得的测试集都不相同，从而从测试集计算得到的性能值也就不同。从测试集得到的性能值可以看成是总体泛化性能的一个估计值，基于这个估计值可以对总体泛化性能进行假设检验和区间估计。</p></li><li><p>如果<spanclass="math inline">\(\mathcal{D}\)</span>是服从二项分布的。那么测试集样本容量n你是知道的，错误次数k你也是知道的。那么你可以开始玩假设检验。比如假设<spanclass="math inline">\(H_0: p_0 \le0.5\)</span>。然后你就假设你这个假设是对的呗，然后算一算在此假设下，错误次数为k的概率。如果算出来的概率小于<spanclass="math inline">\(\alpha\)</span>（显著性水平），说明在此假设下发生这件事的概率极低，那么说明你假设是错的。如果概率大于了<spanclass="math inline">\(\alpha\)</span>，说明至少我不能信心满满的否决你的假设了，我只能说我有<spanclass="math inline">\(1 -\alpha\)</span>置信度认为你的假设是正确的。</p></li><li><p>很多时候我们并非做一次留出法估计，而是多次。所以我们会得到k个测试错误率。则我们可以计算平均错误率及其方差。那么检验统计量<spanclass="math inline">\(\tau_t = \frac{\sqrt{k}(\mu -\epsilon_0)}{\sigma}\)</span>服从t分布。（<spanclass="math inline">\(\mu\)</span>是平均错误率，<spanclass="math inline">\(\epsilon_0\)</span>是假设的错误率， <spanclass="math inline">\(\sigma\)</span>是前面算的方差）</p></li><li><p>如果算出来的<span class="math inline">\(\tau_t\)</span>落在<spanclass="math inline">\([t_{-\alpha/2},t_{\alpha/2}]\)</span>内，则可下置信度为<span class="math inline">\(1 -\alpha\)</span>的判断认为真实错误率为<spanclass="math inline">\(\epsilon_0\)</span>；反之则可下真是错误率不为<spanclass="math inline">\(\epsilon_0\)</span>的判断。</p></li><li><p>以上俩方法都是关于对单个学习器泛化性能的假设进行的检验，但在实际任务中，更多时候我们需要对不同学习器的性能进行比较，方法有：交叉验证t检验、McNemar检验、Friedman检验、Nemenyi后续检验</p></li></ul><ol type="1"><li><p>交叉验证t检验</p><ul><li>略</li></ul></li><li><p>McNemar检验</p><ul><li><p>对二分类问题，使用留出法不仅可估计出学习器A、B的测试错误率，还可以获得俩学习器分类结果的差别，即：两者都分类正确、都错误、A对B错，A错B对的次数，即“列联表”：</p></li><li><p><img src="5.png" /></p></li><li><p>若我们的假设是俩学习器性能相同，则应有<spanclass="math inline">\(e_{01} = e_{10}\)</span>。则<spanclass="math inline">\(|e_{01} - e_{10}| \sim N(1, e_{01} +e_{10})\)</span>。</p></li><li><p>则有：<span class="math inline">\(\tau_{\chi^2}=\frac{(|e_{01} -e_{10}| - 1) ^ 2}{e_{01} + e_{10}}\)</span></p></li><li><p>即<spanclass="math inline">\(\tau_{\chi^2}\)</span>服从自由度为1的卡方分布。</p></li><li><p>当<spanclass="math inline">\(\tau_{\chi^2}\)</span>小于临界值<spanclass="math inline">\(\chi_\alpha^2\)</span>时，即认为俩学习器性能没有显著差别；反之则认为有显著差别，平均错误率小的那个更牛逼。</p></li></ul></li><li><p>Friedman检验</p><ul><li>略</li></ul></li><li><p>Nemenyi后续检验</p><ul><li>略</li></ul></li></ol><h4 id="偏差与方差">偏差与方差</h4><ul><li>为取得良好的泛化能力，则需使偏差较小，此时能够充分拟合数据。并且要使方差较小，因为越小的方差表示受数据扰动的影响小。</li><li>但是往往两全不能齐美，称为偏差-方差窘境。</li><li>具体内容略。</li></ul><h3 id="三.-线性模型">三. 线性模型</h3><h4 id="线性回归">线性回归</h4><p>形如<span class="math inline">\(f(x) = w^\mathrm{T}x +b\)</span>的，就是线性回归</p><p>对于有顺序关系的属性，可以将他们赋值为连续或者离散有顺序的数字。对于无顺序关系的属性，用one-hot。</p><p>线性回归的loss函数是MSE。</p><p>对于偏置项<spanclass="math inline">\(b\)</span>，有一个小技巧就是把它纳入特征里，然后<spanclass="math inline">\(w\)</span>里多加一项，这样就相当于<spanclass="math inline">\(Xw = \hat{y}\)</span>了。然后如果<spanclass="math inline">\(X^\mathrm{T}X\)</span>满秩的话，就直接用线代解出最优解就行了。</p><p>如果不满秩，可以引入正则化项，</p><h4 id="对数几率回归">对数几率回归</h4><p>也叫逻辑回归。</p><p>起始就是把前面线性回归<span class="math inline">\(w^\mathrm{T}x +b\)</span>这个值通过sigmoid函数映射到0 ~1之间。通常用来作二分类问题。</p><p>sigmoid函数：<span class="math inline">\(y = \frac{1}{1 +e^{-z}}\)</span></p><p>逻辑回归：<span class="math inline">\(y = \frac{1}{1 +e^{-w^\mathrm{T}x + b}}\)</span></p><p>另一种写法：<span class="math inline">\(\ln \frac{y}{1 - y} =w^\mathrm{T}x + b\)</span></p><p>其loss函数是交叉熵。</p><h4 id="线性判别分析">线性判别分析</h4><p>略</p><h4 id="多分类学习">多分类学习</h4><p>OvO（One vs One）、OvR（One vs Other）、MvM（Many vs Many）</p><p>假设有n个类，OvO就是有<spanclass="math inline">\(n(n-1)/2\)</span>个二分类器，俩俩类别有一个分类器。对于一个样例，经过这<spanclass="math inline">\(n(n-1)/2\)</span>个分类器过一遍，而是看看哪些类别得分最高，就判定这个样例为哪个类别。这就是OvO的思路。</p><p>对于OvR，就是对于每个类别有一个分类器，就是分是当前这个类或者不是。所以一共n个分类器。结果就是看这n个分类器哪个的概率最大，就判定这个样例为哪个类别。</p><p>MvM就是每次将若干个类作为正类，若干个类作为负类。MvM中，最常见的一种分类技术叫“纠错输出码(ECOC)”</p><p>ECOC，具体来说，就是对n个类别做m次划分，每次划分是一个二分类器，将一部分类别划为正类，一部分划为负类。然后对于一个样例，经过这m个分类器跑一遍，得到预测向量，看看预测向量与类别自身向量的欧氏距离或者海明距离。去差距最小的那个类别作为预测值。</p><p>具体看下面这幅图，一目了然。</p><p><img src="6.png" /></p><p>上面这幅图中，一共有5个分类器，4个类别。然后对于一个样例，经过5个分类器，跑出了(-1,-1, +1, -1, +1)这个预测向量。记此向量为x。那么x与<spanclass="math inline">\(C_1\)</span>的向量(-1, +1, -1, +1,+1)的欧氏距离是<spanclass="math inline">\(2\sqrt{3}\)</span>，海明距离(即不同的个数)是3。</p><p>通过观察，可以发现预测向量与<spanclass="math inline">\(C_3\)</span>的欧氏距离和海明距离均最小，那么就判定该样例属于<spanclass="math inline">\(C_3\)</span>。</p><p>可以发现，EOOC编码越长(分类器越多)，那么纠错能力越强(鲁棒性越好)。</p><p>而且可以发现，两个类别<span class="math inline">\(C_i,C_j\)</span>的编码距离越远越好，这样子区分度就越高。所以我们称任意俩类别之间编码距离最远的编码方式为理论最优编码。</p><h4 id="类别不平衡问题">类别不平衡问题</h4><p>如果有很多个类别，但是训练集中每个类别的样本数不同，这样会不好。所以说有三种方式解决这个问题(假设正样本多，负样本少)：第一就是删掉一些正样本，第二就是增加一些负样本，第三就是使用“再放缩”。</p><p>什么叫“再放缩”，具体来说，原本二分类逻辑回归实际是在执行：若<spanclass="math inline">\(\frac{y}{1 - y} &gt;1\)</span>，则预测为正例。那么改一改，改为：若<spanclass="math inline">\(\frac{y}{1 - y} &gt;\frac{m+}{m-}\)</span>，则预测为正例，其中<spanclass="math inline">\(m+\)</span>是正样本数，<spanclass="math inline">\(m-\)</span>是负样本数。</p><h3 id="四.-决策树">四. 决策树</h3><h4 id="基本流程">基本流程</h4><p>就是一种模拟人脑决策的模型。</p><p>举个例子，就明白了。</p><table><thead><tr class="header"><th></th><th>性别</th><th>年龄</th><th>城市</th><th>类别</th></tr></thead><tbody><tr class="odd"><td>A</td><td>男</td><td>老年</td><td>北京</td><td>1</td></tr><tr class="even"><td>B</td><td>男</td><td>老年</td><td>上海</td><td>1</td></tr><tr class="odd"><td>C</td><td>女</td><td>青年</td><td>北京</td><td>1</td></tr><tr class="even"><td>D</td><td>女</td><td>中年</td><td>北京</td><td>1</td></tr><tr class="odd"><td>E</td><td>女</td><td>青年</td><td>北京</td><td>2</td></tr><tr class="even"><td>F</td><td>女</td><td>青年</td><td>北京</td><td>2</td></tr><tr class="odd"><td>G</td><td>女</td><td>中年</td><td>北京</td><td>2</td></tr><tr class="even"><td>H</td><td>女</td><td>中年</td><td>上海</td><td>2</td></tr></tbody></table><p>ok来吧，把决策树建出来把。</p><p>首先先按性别分呗，那么就会分出：</p><p><img src="7.png" style="zoom: 50%;" /></p><p>因为A、B都属于1类，属于没必要继续分下去了，所以直接以叶子节点类别1结束。</p><p>ok，那么就右边需要分了，性别这个属性用完了，那么就用年龄开分！</p><p><img src="8.png" style="zoom:67%;" /></p><p>老年，因为没有样本被分到这，所以这里的叶子节点的类别就要看父节点，也就是（C、D、E、F、G、H+年龄/城市）这个点哪个类别多，可以发现属于类别2的人多，所以老年这个节点就以类别2的叶子节点结束。</p><p>然后可以目光看向青年，发现C、E、F都在北京，所以没什么好分的了，那么直接接叶子节点，类别就看看哪个类别人多就哪个，E、F都是2，所以接类别2的叶子节点。</p><p>还剩（D、G、H + 城市）这个点了，继续分！</p><p><img src="9.png" /></p><p>分完H那个点只有它自己了，所以直接接自己类别的叶子节点即可。</p><p>然后（D、G）这个点，因为没有属性可用了，所以不用分了。哪个类别多接哪个类别的叶子节点。发现D是1，G是2，一样多，那随便了，我就接了一个类别为1的叶子节点。</p><p>至此，我们利用这个数据集，建出了一颗决策树！</p><p>相信聪明的你也发现了，我们用按顺序去用每一个属性的，那能不能调换顺序呢？顺序有什么讲究呢？这里头是有学问的，留到下一节讲。</p><h4 id="划分选择">划分选择</h4><ul><li>决策树学习的关键在于如何选择最优划分属性，一般而言，随着划分过程不断进行，我们希望决策树的分支节点所包含的样本尽可能属于同一类别。</li><li>经典的属性划分方法有：信息增益、增益率、基尼指数</li></ul><h5 id="信息增益">信息增益</h5><ul><li><p>首先要知道什么是信息量，信息量具有以下几个性质：</p><ol type="1"><li>非负性：不存在负信息量的事件</li><li>单调性：事件发生的概率越大，其信息量越小</li><li>累加性：多个相互独立事件信息量等于各事件信息量之和</li></ol></li><li><p>香农提出了信息量这个概念，然后他用了一个数学函数，以满足上面的三条性质：<spanclass="math inline">\(I(x) = \log \frac{1}{P(x)} = -\logP(x)\)</span></p><ul><li><p><span class="math inline">\(I(x)\)</span>是事件<spanclass="math inline">\(x\)</span>的信息量，<spanclass="math inline">\(P(x)\)</span>是事件<spanclass="math inline">\(x\)</span>发生的概率</p></li><li><p>图像长这样：</p><p><img src="10.png" style="zoom: 80%;" /></p></li></ul></li><li><p>信息量是针对某个具体事件的一种度量方式，香农觉得不够，于是定义了一个名叫“信息熵”的概念。信息熵度量的是随机变量的不确定性：<spanclass="math inline">\(H(X) = E[-\log P(X)] = -\sum_{x \in X} P(x)\logP(x)\)</span></p><ul><li>信息熵其实就是“随机变量的信息量的期望”，展开后就是所有事件概率与其信息量乘积之和</li><li><span class="math inline">\(H(X)\)</span>是随机变量<spanclass="math inline">\(X\)</span>的信息熵</li><li><span class="math inline">\(H(X)\)</span>是非负的，如果<spanclass="math inline">\(X\)</span>的可能性越多(越混乱)，那么<spanclass="math inline">\(H(X)\)</span>越大</li></ul></li><li><p>条件熵也是一种信息熵，它考虑的是有两个随机变量的情况，且其中一个随机变量已知的情况时的期望信息熵：</p></li></ul><p><span class="math display">\[\begin{align*}H(Y | X) &amp;= \sum_{x \in X}P(x)H(Y|X=x) \\&amp;= -\sum_{x \in X}P(x) \sum_{y \in Y} P(y|x)\log P(y|x) \\&amp;= -\sum_{x \in X}\sum_{y \in Y}P(x,y)\log P(y|x)\end{align*}\]</span></p><hr /><ul><li>OK，知道了这几个概念，我们来看看如何将信息熵的概念用到决策树的划分选择上！</li><li>对于样本集D，每个样本不是有一个label嘛，假设有k种label，那么样本集D的信息熵<spanclass="math inline">\(H(D)\)</span>就为：</li></ul><p><span class="math display">\[H(D) = -\sum p_k \cdot \log p_k\]</span></p><ul><li><spanclass="math inline">\(p_k\)</span>是第k个label占全部label的比例。这个定义很好理解，如果全部样本都是同一个label，那么信息熵就是0。如果有很多个label，那么信息熵就比较大，说明混乱程序比较高，不确定性程度高。</li><li>那如何知道按照什么属性划分会比较好呢？我们希望的是划分之后所产生的子数据集的纯度越高越好，所以有一个很自然的想法就冒出来了：我要找到一个属性A，使得<spanclass="math inline">\(H(D) -H(D|A)\)</span>最大，即划分后的信息熵尽可能小，这样就满足了划分后数据集纯度尽可能大的要求了。bingo，<spanclass="math inline">\(H(D) -H(D|A)\)</span>有个名字，“信息增益”。信息增益越大，意味着使用属性A来进行划分所获得的纯度提升越大。</li><li>用数学公式表示maybe会更直观些：</li></ul><p><span class="math display">\[\begin{align*}    &amp;G(D, A) = H(D) - H(D | A) \\    &amp;H(D) = -\sum p_k \cdot \log p_k \\    &amp;H(D | A) = \sum_{i=1}^{C} \frac{cnt_{D_i}}{cnt_D} \cdot H(D_i)\end{align*}\]</span></p><ul><li><p><span class="math inline">\(G(D, A)\)</span>就是考虑用属性<spanclass="math inline">\(A\)</span>进行划分时的信息增益；<spanclass="math inline">\(cnt_D\)</span>是数据集<spanclass="math inline">\(D\)</span>的样本数量；<spanclass="math inline">\(cnt_{D_i}\)</span>是数据集按照<spanclass="math inline">\(A\)</span>划分后，第<spanclass="math inline">\(i\)</span>类的样本数量；<spanclass="math inline">\(C\)</span>是按照<spanclass="math inline">\(A\)</span>划分后，划分出的类别数。</p></li><li><p>至此，如何构建一棵决策树就显然易见了，是一个递归的过程，每次先算出当前数据集D的信息熵<spanclass="math inline">\(H(D)\)</span>，然后依次算当前所有属性划分划分之后的信息熵<spanclass="math inline">\(H(D|(A/B/C/...))\)</span>，看看哪个的信息增益越大，选最大的划分，划分后得到了新的数据集<spanclass="math inline">\(D&#39;\)</span>和属性集，然后对新的数据集和属性集递归做就好了。递归终止条件就是当前数据集的label全一样，或者属性集空了，或者数据集空了。</p></li><li><p>上面就是用信息增益(ID3)来建树的算法</p></li></ul><h5 id="增益率c4.5">增益率、C4.5</h5><ul><li><p>上面的信息增益算法其实我们没考虑到一个东西，就是不同属性天生包含的类别数不同，例如属性A可能有高、中、矮，但是属性B可能只有瘦、胖。类别多的属性它混乱的几率就会大。为了考虑这一点，所以我们在信息增益的基础上，引入增益率的概念。</p></li><li><p>直接给出数学定义，<span class="math inline">\(G(D,A)\)</span>是信息增益(Gain)，<span class="math inline">\(Gr(D,A)\)</span>是增益率(Gain ratio)</p></li></ul><p><span class="math display">\[\begin{align*}    &amp;G(D, A) = H(D) - H(D | A) \\    &amp;Gr(D, A) = \frac{G(D, A)}{H(A)} \\    &amp;H(A) = -\sum p_k \log p_k\end{align*}\]</span></p><ul><li><p>例如属性A有2个类别，第一个类别占1/2，第二个类别占1/2，那么<spanclass="math inline">\(H(A)\)</span>就是<spanclass="math inline">\(-(\frac12 \cdot \log \frac12 \cdot2)=1\)</span></p></li><li><p>稍微改进一下上面的算法，把每次选择最大信息增益改为选择最大增益率的属性。</p></li><li><p>但简单这样改还不够好，因为增益率对类别数少的属性有偏好。</p></li><li><p>所以C4.5算法就说：先选出信息增益高于平均信息增益的属性，然后再在这些属性中选出增益率最高的属性作为划分属性！</p></li></ul><h5 id="基尼指数">基尼指数</h5><ul><li>OK，前面用熵去衡量数据集“纯度”的做法确实很优美。那现在，基尼指数就是另一种衡量数据集“纯度”的东西。</li><li>数据集<spanclass="math inline">\(D\)</span>的纯度可用基尼值(Gini)来衡量：</li></ul><p><span class="math display">\[Gn(D) = \sum_{i=1}^{C}\sum_{j=1}^{C}p_ip_j = 1 - \sum_{k=1}^{C}p_k^2,\quad i \ne j\]</span></p><ul><li>可以发现，基于指数反映了从<spanclass="math inline">\(D\)</span>中随机抽取两个样本，其类别lable不一致的概率。假设有2种label，且样本数对半开，那么<spanclass="math inline">\(Gn(D)\)</span>就是<spanclass="math inline">\(\frac12 \cdot \frac12 =\frac14\)</span>；假设有2种lable，且样本数为9:1，那么<spanclass="math inline">\(Gn(D)\)</span>就是<spanclass="math inline">\(\frac{9}{10} \cdot \frac{1}{10} =\frac{9}{100}\)</span>。可以发现，基尼系数越小，数据集纯度越高。</li><li>那么用属性A划分后的基尼指数是多少？</li></ul><p><span class="math display">\[Gn(D, A) = \sum_{k=1}^{C} \frac{cnt_{D_i}}{cnt_D}Gn(D_i)\]</span></p><ul><li>CART算法就是利用基尼指数来建决策树的算法，它每次看看用哪个属性划分后的基尼指数最小，就用它划分。</li></ul><h4 id="剪枝处理">剪枝处理</h4><ul><li><p>剪枝的目的，就是为了避免把训练集自身的一些特点当作所有数据都具有的一般性质而导致的过拟合。</p></li><li><p>剪枝的基本策略分为预剪枝和后剪枝。</p></li><li><p>判断决策树泛化性能是否提升的方法：留出法(留出一部分作为验证集)</p></li></ul><h5 id="预剪枝">预剪枝</h5><ul><li>很简单，就是在划分节点的时候，拿验证集跑一跑，如果划分后效果反而不如不划分，那么就不继续划分该节点了，直接连个叶子节点上去，类别就是该节点内人数最多的类别。（从上到下）</li><li>优点：降低过拟合风险，显著减少训练时间</li><li>缺点：欠拟合风险</li></ul><h5 id="后剪枝">后剪枝</h5><ul><li>很简单，就是先用算法生成一棵决策树，然后从下到上依次考察是否将节点替换为叶子节点会更优，如果更优，就替换</li><li>优点：比预剪枝保留了更多分支，欠拟合风险更小</li><li>缺点：训练时间大</li></ul><h4 id="连续与缺失值">连续与缺失值</h4><p>连续值和缺失值都是针对属性的值说的。因为在前面讨论的决策树中，我们属性的值都是离散的，例如高、矮、胖、瘦，都是离散的。而现实中可能会存在很多连续值，例如身高是多少cm；也有可能会存在缺失值，比如有个属性是自我性别认知，可能有些人是未知。</p><p>首先来看如何处理连续值吧，思路很简单，就是设立划分点，就拿身高这个属性举例，假设样本中，有n个不同的身高，那么划分点就有n个，每确定一个划分点t，其实就可以算出按照t去划分身高这个属性之后得到的信息增益<spanclass="math inline">\(G(D, A,t)\)</span>。以用信息增益算法建树举例，那么划分身高这个属性的时候，就是找一个划分点t，使得<spanclass="math inline">\(H(D) - G(D, A, t)\)</span>最大即可。</p><p>需要注意的是，连续值与离散值有一个地方不同就是，离散值的属性如果用过，那么后面就不会再用来划分了。但是连续值的属性可以再次使用，比如第一次划分是身高是否低于180，进入子节点后可以继续用身高这个属性划分，身高是否低于160。</p><p>OK，现在来讨论缺失值。</p><p>还是用信息增益算法来举例，当你计算<span class="math inline">\(H(D),G(D, A)\)</span>的时候，<spanclass="math inline">\(D\)</span>就是剔除<spanclass="math inline">\(A\)</span>属性有缺失值得到样本子集，然后信息增益就是<spanclass="math inline">\(\rho(H(D) - G(D, A))\)</span>，<spanclass="math inline">\(\rho\)</span>是无缺失值样本数量占总样本数量的比例。然后选择信息增益最大的属性作为当前的划分属性。</p><p>ok，划分属性确定好了，如何划分呢？答案就是给每个样本一个全局变量<spanclass="math inline">\(w_i\)</span>，为自己的权重，初始为1。对于那些属性<spanclass="math inline">\(A\)</span>无缺失的样本呢，直接划分到对应子集中，对应进入的权重为<spanclass="math inline">\(w_i\)</span>；对于那些属性<spanclass="math inline">\(A\)</span>缺失的样本，就等无缺失的样本都划分完后，然后划分到每一个子集中，对应进入的权重变为<spanclass="math inline">\(w_i \cdot\frac{\sum\text{子集中样本的}w_i}{\sum\text{当前属性无缺失的样本的}w_i}\)</span>。同时将<spanclass="math inline">\(\frac{\sum\text{子集中样本的}w_i}{\sum\text{当前属性无缺失的样本的}w_i}\)</span>记录下来作为该子节点的权重（后面验证时会用到）。</p><p>那么按照这样的算法，决策树就可以建立起来了。</p><p>那么验证时，对于属性缺失的样本，它该进入哪呢？</p><p>答案就是当建树的时候发现有缺失时，在验证的时候，就给每一个要验证的样本带一个权重<spanclass="math inline">\(w\)</span>，初始值为1。假设走到某个属性A，若该样本在属性A上无缺失，则进入到对应子节点to，<spanclass="math inline">\(dfs(to,w)\)</span>；若有缺失，则dfs每个子集都进入，但是进入的权重要乘对应子节点的权重，<spanclass="math inline">\(dfs(to_i, w \cdotw_i)\)</span>，对应子节点的权重在建树时已经算好。那么最终会进入到多个叶子节点，选择权重最大的叶子节点的类别判定为该样本的label。</p><p>但是这么做，可能会存在对应子节点没有权重，也就是训练时该属性无缺失的情况，那么此时也还是要dfs每个子集都进入，只是把进入的权重改为乘<spanclass="math inline">\(\frac{1}{\text{该节点儿子个数}}\)</span>，即等比例进入每个儿子节点。</p><h4 id="多变量决策树">多变量决策树</h4><p>略</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;参考内容：《机器学习》周志华&lt;/p&gt;</summary>
    
    
    
    <category term="4. 大学" scheme="http://error666.top/categories/4-%E5%A4%A7%E5%AD%A6/"/>
    
    <category term="计算机专业课" scheme="http://error666.top/categories/4-%E5%A4%A7%E5%AD%A6/%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%93%E4%B8%9A%E8%AF%BE/"/>
    
    
  </entry>
  
  <entry>
    <title>微分方程1</title>
    <link href="http://error666.top/2024/09/18/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B1/"/>
    <id>http://error666.top/2024/09/18/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B1/</id>
    <published>2024-09-18T10:05:34.000Z</published>
    <updated>2024-10-02T07:17:10.580Z</updated>
    
    <content type="html"><![CDATA[<p>几何法解一阶ODE、欧拉数值法解一阶ODE、分离变量法解一阶ODE、解标准一阶线性ODE、换元法解伯努利方程+一阶齐次ODE、一阶自治ODE图像分析</p><span id="more"></span><p>mit 18.03 Arthur Mattuck教授讲的微分方程。</p><hr /><h3 id="导言">导言</h3><p>常微分方程（ordinary differential equation,ODE）：函数的自变量只有一个，通常是时间</p><p>举个最简单的例子，考虑平抛问题中的垂直方向，向下为正。</p><p><span class="math inline">\(\because \dot{v} = g \quad \therefore v =gt + v_0\)</span></p><p><span class="math inline">\(\because \dot{y} = v = gt + v_0 \quad\therefore y = \frac12gt^2 + v_0t + y_0\)</span></p><p>上面这个简单的例子，其实就是在求解微分方程：<spanclass="math inline">\(\ddot{y} = g\)</span></p><p>这个微分方程非常简单，且求解非常容易。但往往实际中的大部分问题，你只能列出微分方程，但是却无法精确求解它。例如下面单摆这个例子：</p><p><img src="1.png" style="zoom:50%;" /></p><p>规定以中间垂直线为标准，向右边的角度为正，左边为负。考虑切向加速度<spanclass="math inline">\(a\)</span>，加速度前的负号表示它总指向与位移相反的方向。</p><p>可以根据图片写出一些公式： <span class="math display">\[\begin{align*}    \because x &amp;= L\theta \\    \because a &amp;= -g\sin \theta \\    \therefore \ddot{x} &amp;= -g\sin\theta \\    \therefore L\ddot{\theta} &amp;= -g\sin\theta \\    \therefore \ddot{\theta} &amp;= -\frac{g}{L}\sin\theta\end{align*}\]</span> 我们成功写出了一个关于<spanclass="math inline">\(\theta(t)\)</span>的微分方程。为了更加负号实际，我们引入空气阻力，于是微分方程变为：<span class="math display">\[\ddot{\theta} = -\mu\dot{\theta} - \frac{g}{L}\sin\theta\]</span>很好。单摆这个微分方程，是十分难求解的。既然我们求不出它的解析解，那我们如果仅通过这个方程窥探摆运动的规律呢？</p><p>既然上面那个微分方程体现了摆的运动，那我们先将这个微分方程可视化出来，具体来说，我们可以以x轴为<spanclass="math inline">\(\theta\)</span>，y轴为<spanclass="math inline">\(\dot{\theta}\)</span>。即有向量坐标<spanclass="math inline">\((\theta,\dot{\theta})\)</span>。对向量坐标求导，得到<spanclass="math inline">\((\dot{\theta},\ddot{\theta})\)</span>向量，它表示了当前该坐标在图上变化的方向和大小。</p><p>这样子的话，我们对平面上所有坐标<span class="math inline">\((\theta,\dot{\theta})\)</span>，把对应的<spanclass="math inline">\((\dot{\theta},\ddot{\theta})\)</span>向量平移到以<span class="math inline">\((\theta,\dot{\theta})\)</span>为起点上来。就可以得到一副这样的图：</p><p><img src="2.png" style="zoom:50%;" /></p><p>（为了保持美观，向量的长度保持了一样，通过颜色来区分向量的长度）</p><p>这幅图会引发我们很多的思考，可以发现，原点代表着摆的角度和角加速度都为0，即静止状态。通过旋流来看显然最后的状态都会回归静止状态，这是符合实际的。我们还会发现，在<spanclass="math inline">\((\pi, 0)\)</span>位置的旋流是静止的，说明<spanclass="math inline">\((\pi,0)\)</span>是个不动点。那它代表什么物理含义呢？它代表小球在正上方保持平衡。well，根据实际我们可以知道，这确实是个可保持静止的地方，但但凡受到一点扰动，就会打破这个平衡。</p><p>所以我们可以得出结论：<span class="math inline">\((0, 0), (\pi,0)\)</span>都是不动点，但<span class="math inline">\((0,0)\)</span>是稳定点，而<span class="math inline">\((\pi,0)\)</span>不是。</p><p>非常有趣！问题来了，如何绘制出这一幅图？</p><p>我们已知的是微分方程为<span class="math inline">\(\ddot{\theta} =-\mu\dot{\theta} - \frac{g}{L}\sin\theta\)</span>，且坐标<spanclass="math inline">\((\theta, \dot{\theta})\)</span>上的向量为<spanclass="math inline">\((\dot{\theta}, \ddot{\theta})\)</span>。</p><p>那么我们可以任意选取某点<span class="math inline">\((\theta_0,\dot{\theta}_0)\)</span>，然后根据微分方程算出<spanclass="math inline">\(\ddot{\theta}\)</span>。即可得到当前坐标上的向量<spanclass="math inline">\((\dot{\theta}_0,\ddot{\theta}_0)\)</span>。这个向量就体现了该坐标的移动趋势，所以我们将该坐标朝着对应向量方向移动一个小距离<spanclass="math inline">\(\Delta t\)</span>，即可得到新点<spanclass="math inline">\((\theta_1,\dot{\theta_1})\)</span>。重复这个步骤，即可画出一条轨迹。多次取不同的初始点进行绘制，即可得到若干条轨迹。全部的轨迹合起来就是上述那张图。</p><p>Well，讲到这里，相信你已经感受到微分方程的魅力了。通过小小方程，即可窥探事物运行的规律，没有比这更令人兴奋的事情了！</p><h3 id="一.-ode几何方法">一. ODE几何方法</h3><p>如何用作图描述ODE呢？</p><p>使用方向场。跟前言中使用的方法类似，假设目前我们有：<spanclass="math inline">\(y&#39; = f(x, y)\)</span></p><p>那我们可以作一幅二维图，x轴就是x，y轴就是y，坐标<spanclass="math inline">\((x,y)\)</span>上是一个短线（称为"线素"），这条短线的斜率就是<spanclass="math inline">\(y&#39;\)</span></p><p>那么，画出一条与所有线素相切的曲线，就是微分方程<spanclass="math inline">\(y&#39;=f(x,y)\)</span>的一个解，称之为"积分曲线"</p><p>这显然是正确的，因为对于一条画出来的曲线，其任意一点都与线素相切，说明其斜率也就是<spanclass="math inline">\(y&#39;\)</span>跟给定的<spanclass="math inline">\(y&#39;=f(x,y)\)</span>一致，也就是这条曲线符合微分方程，所以它自然就是解。</p><p>例如这个微分方程：<span class="math inline">\(y&#39; =-\frac{x}{y}\)</span>。它用方向场画出来的积分曲线如下：</p><p><img src="3.png" style="zoom:67%;" /></p><p>（<span class="math inline">\(C = 0\)</span> means <spanclass="math inline">\(y&#39; = 0\)</span>）</p><p>再来一个例子：<span class="math inline">\(y&#39; = 1 + x -y\)</span></p><p><img src="4.png" /></p><p>注意到在<span class="math inline">\(C=2\)</span>和<spanclass="math inline">\(C=0\)</span>两条之间的区域，当积分曲线进入这个区域后就再也出不去了，解函数无法逃逸。另一个需要注意的要点是，积分曲线永远不会相交。如果两曲线相交的话，则在交点处就会有两条切线、两个斜率，这与微分方程不符。因此进入此区域的曲线，无法逃逸也无法相交，只能够互相靠近，朝向<spanclass="math inline">\(y=x\)</span>的直线靠拢。</p><p>事实上，在方向场上的积分曲线满足"存在与唯一性定理"：</p><ul><li>存在性：若<span class="math inline">\(y\)</span>在<spanclass="math inline">\((x_0, y_0)\)</span>的领域内连续，则通过<spanclass="math inline">\((x_0, y_0)\)</span>的<spanclass="math inline">\(y&#39;=f(x, y)\)</span>有解。</li><li>唯一性：若<span class="math inline">\(y&#39;\)</span>在<spanclass="math inline">\((x_0, y_0)\)</span>的领域为连续，则通过<spanclass="math inline">\((x_0, y_0)\)</span>的<spanclass="math inline">\(y&#39; = f(x, y)\)</span>有且仅有唯一解。</li></ul><h3 id="二.-ode欧拉数值法及推广">二. ODE欧拉数值法及推广</h3><p>欧拉数值法解微分方程其实就是导言里用的方法。对于微分方程：<spanclass="math inline">\(y&#39; = f(x,y)\)</span>，如何在图上求出其中某条曲线(解)呢？首先你先得确定一个初始点<spanclass="math inline">\((x_0,y_0)\)</span>，然后从初始点出发，一步一步画出曲线。</p><p>对于坐标<span class="math inline">\((x,y)\)</span>，求导后可得到<span class="math inline">\((1,y&#39;)\)</span>向量，这个向量就是从<span class="math inline">\((x,y)\)</span>出发的移动趋势向量。朝着这个向量的方法走一个步长，一直迭代下去，就可以得到用欧拉数值法拟合出来的曲线。</p><p>那么移动后的坐标是多少呢？假设步长为<spanclass="math inline">\(\alpha\)</span>，那么可以得到公式<spanclass="math inline">\(f(x_k, y_k)\)</span>为函数在<spanclass="math inline">\(x_k\)</span>处的导数： <spanclass="math display">\[\begin{cases}    x_{k+1} = x_k + \alpha \\    y_{k+1} = y_k + \alpha \cdot f(x_k, y_k)\end{cases}\]</span>上面的方法就叫一阶欧拉数值法。（一阶的意思是你求了一次导数）</p><p>即如果你确定了初始点<span class="math inline">\((x_0,y_0)\)</span>和微分方程<span class="math inline">\(y&#39;=f(x,y)\)</span>，那么通过此方法就可求出以<span class="math inline">\((x_0,y_0)\)</span>为初始点且满足微分方程的曲线。</p><hr /><p>这个方法好不好呢？</p><p>当然是有的，显然我们知道，步长越小越精确。但再怎么精确，如果真正的函数是曲线的话，欧拉数值法也是得不到真正解的。</p><p>例如下面这个图，在<span class="math inline">\((x_k,y_k)\)</span>处是凸的(因为<span class="math inline">\(y&#39;&#39; &gt;0\)</span>)，所以在此点迭代的下一个<spanclass="math inline">\(y_{k+1}\)</span>是小于真正的<spanclass="math inline">\(y\)</span>的。</p><p><img src="5.png" /></p><p>同理，如果某点的<span class="math inline">\(y&#39;&#39; &lt;0\)</span>，那么在<span class="math inline">\((x_k,y_k)\)</span>便是凹的，因此在该点迭代的下一个<spanclass="math inline">\(y_{k+1}\)</span>是大于真正的<spanclass="math inline">\(y\)</span>的。</p><p>我们知道了数值法并不能求精确解，但是我们会希望误差最小。</p><p>所以步长与误差之间的关系是什么呢？答案是<span class="math inline">\(e\sim c\alpha\)</span>，<spanclass="math inline">\(c\)</span>是一个常数，即误差近似与步长为线性关系。</p><hr /><p>那么能不能使这个误差<spanclass="math inline">\(e\)</span>小点呢？</p><p>有的，求多几次导数就好了，例如我现在在<spanclass="math inline">\((x_k, y_k)\)</span>，然后通过该点的导数<spanclass="math inline">\(y&#39;_1\)</span>可迭代处<spanclass="math inline">\((\hat{x}_{k+1},\hat{y}_{k+1})\)</span>，然后我可以得到<spanclass="math inline">\((\hat{x}_{k+1},\hat{y}_{k+1})\)</span>的导数<spanclass="math inline">\(y&#39;_2\)</span>。</p><p>于是我将<span class="math inline">\(\frac{y&#39;_1 +y&#39;_2}{2}\)</span>作为<spanclass="math inline">\(y&#39;\)</span>，然后在<spanclass="math inline">\((x_k, y_k)\)</span>基础上迭代出<spanclass="math inline">\((x_{k+1}, y_{k+1})\)</span>。</p><p>这样得到的<span class="math inline">\((x_{k+1},y_{k+1})\)</span>会更加接近真实解。</p><p>从几何上理解也很直观，相当于取了两次方向向量的中间作为新的方向向量，然后朝着新方向向量移动一小步，这样的偏差会比原先的一阶欧拉数值法小。</p><p>因为在这个方法中，我们算了两次导数，所以该方法叫做二阶欧拉数值法。其误差与步长的关系是：<spanclass="math inline">\(e \sim c\alpha^2\)</span></p><hr /><p>同理，我们还可以求三步、四步的信息(导数)，这样误差就会进一步缩小。但是由此带来的问题是计算复杂度变大。所以天下没有免费的午餐。通常在计算机绘制微分方程解时，都是采用的四阶欧拉数值法。</p><hr /><p>欧拉数值法有没有局限性呢？显然是有的，当解函数不是连续的，而存在“奇点”时，欧拉数值法在越过奇点之后将无法拟合出正确的曲线，例如下图这个真实解函数：</p><p><img src="6.png" style="zoom: 50%;" /></p><h3 id="三.-一阶ode解析法">三. 一阶ODE解析法</h3><p>前面两节我们分别通过几何法（画线素）和欧拉数值法求解了ODE。这一节我们将用解析的方式求解一阶线性ODE。</p><h4 id="可分离变量的一阶ode">可分离变量的一阶ODE</h4><p>我们把能写为：<span class="math inline">\(y&#39; =f(x)g(y)\)</span>形式的微分方程，称为一阶ODE。</p><p>那么，x放一边，y放一边，两边同时积分，即可求出答案： <spanclass="math display">\[\because \frac{dy}{dx} = f(x)g(y) \\\therefore \frac{1}{g(y)}dy = f(x)dx \\\therefore \int \frac{1}{g(y)}dy = \int f(x)dx \\\]</span> 例题：求<span class="math inline">\(y&#39; = y\sinx\)</span>的通解 <span class="math display">\[\text{when } y \ne 0, \text{one has } \frac{1}{y}dy = y\sin x \\\therefore \int \frac{1}{y}dy = \int \sin x dx \\\therefore \ln |y| = -\cos x + c \\\therefore |y| = e^{-\cos x} \cdot e^c \\\therefore y = \pm c \cdot e^{-\cos x} \\\therefore y = c \cdot e^{-\cos x}, c \ne 0 \\\text{when } y = 0, \text{it can pass the check.} \\\therefore y = c \cdot e^{-\cos x}, c \ne 0\]</span></p><h4 id="一阶线性ode">一阶线性ODE</h4><p>可以写成：<span class="math inline">\(a(x)y&#39; + b(x)y =c(x)\)</span>，的方程叫做一阶ODE。</p><p>为什么叫“线性”呢？因为<spanclass="math inline">\(y&#39;\)</span>和<spanclass="math inline">\(y\)</span>呈线性关系，所以这样叫了。跟这种方程的感觉很像：<spanclass="math inline">\(ax + by = c\)</span></p><p>如果<spanclass="math inline">\(c(x)\)</span>为0那么上面的方程可以称为“齐次方程”</p><p>但上面的形式是一阶线性ODE的通式，其标准形式如下： <spanclass="math display">\[y&#39; + p(x)y = q(x)\]</span></p><hr /><p>一阶线性ODE在实际应用中很广泛，例如传导—扩散模型。</p><p>这个模型名字的来源是来自于两个物理现象，首先是温度传导现象：</p><p><img src="8.png" style="zoom:50%;" /></p><p>外头是某种液体，中间可能是某种介质，外边套了层铁皮。那么如果<spanclass="math inline">\(T\)</span>与<spanclass="math inline">\(T_e\)</span>不同，则会发生温度传导现象，由牛顿温度传导定律，可得到如下方程：<spanclass="math inline">\(\frac{dT}{dt} = k(T_e - T), k &gt; 0\)</span></p><p>以及浓度扩散现象：</p><p><img src="7.png" style="zoom:50%;" /></p><p>外头是某种液体，中间也是某种液体，外边套了层半透膜。那么如果<spanclass="math inline">\(C\)</span>与<spanclass="math inline">\(C_e\)</span>不同，则会发生浓度扩散现象，同样可得到方程：<spanclass="math inline">\(\frac{dC}{dt} = k(C_e - C), k &gt; 0\)</span></p><p>这就是传导—扩散模型，让我们用一个一般的数学式子来描述它： <spanclass="math display">\[\begin{cases}&amp;T&#39; + kT = kT_e \\&amp;k = p(t) \\&amp;kT_e = q(t)\end{cases}\]</span>显然，上面的式子显然是标准一阶线性ODE的写法，我们可以将其视为：<spanclass="math inline">\(y&#39; + p(x)y = q(x)\)</span></p><hr /><p>知道了一阶线性ODE的定义，以及标准形式，以及实际生活中的建模运用。现在我们想知道的，就是如何求解它。<span class="math display">\[y&#39; + py = q \quad(1)\]</span> <span class="math inline">\(p, q\)</span>均为关于<spanclass="math inline">\(x\)</span>的函数，我想找到一个关于<spanclass="math inline">\(x\)</span>的函数<spanclass="math inline">\(u\)</span>，使得(1)同时左乘<spanclass="math inline">\(u\)</span>后，能使得左边的部分可以化简为<spanclass="math inline">\((uy)\)</span>的导数，let's try： <spanclass="math display">\[uy&#39; + upy = uq \quad (2)\]</span> <span class="math inline">\(\because (uy)&#39; = uy&#39; +u&#39;y\)</span></p><p><span class="math inline">\(\therefore u&#39; = up \quad(3)\)</span></p><p>也就是我们想到的这个<spanclass="math inline">\(u\)</span>，满足(3)</p><p>(3)是可通过分离变量解的，因为<spanclass="math inline">\(\frac{du}{dx} = up(x)\)</span>，整理得：<spanclass="math inline">\(\frac{1}{u}du = p(x)dx\)</span></p><p>可解出：<span class="math inline">\(u = e^{\int p(d)dx}\)</span></p><p>ok，将算出的这个<spanclass="math inline">\(u\)</span>回代进(2)里，我们则可以得到： <spanclass="math display">\[(uy)&#39; = uq\]</span> 于是我们可以解出<spanclass="math inline">\(uy\)</span>为多少，然后方程就没有求导项了，整理下即可求出<spanclass="math inline">\(y\)</span></p><p>思路总结：</p><ol type="1"><li>求出<span class="math inline">\(u = e^{\int p(x)dx}\)</span></li><li>把<span class="math inline">\(u\)</span>左乘<spanclass="math inline">\(y&#39; + py = q\)</span>，方程左边可变为<spanclass="math inline">\((uy)&#39;\)</span></li><li>解<span class="math inline">\((uy)&#39; = uq\)</span></li></ol><hr /><p><strong>例1.</strong> <span class="math inline">\(xy&#39; - y =x^3\)</span></p><p>首先先化为标准形式：<span class="math inline">\(y&#39; - \frac1x y =x^2\)</span></p><p>然后求出<span class="math inline">\(u = e^{\int p(d)dx} = e^{\int-\frac1x dx} = e^{-\ln x} = \frac1x\)</span></p><p>对标准形式左乘<span class="math inline">\(u\)</span>：<spanclass="math inline">\((uy)&#39; = (\frac1x \cdot y)&#39; = \frac1x \cdotx^2 = x\)</span></p><p><span class="math inline">\(\therefore \frac{y}{x} = \frac12 x^2 +c\)</span></p><p><span class="math inline">\(\therefore y = \frac12 x^3 +cx\)</span></p><p><strong>例2.</strong> <span class="math inline">\((1 + \cos x)y&#39;- (\sin x)y = 2x\)</span></p><p>首先先化为标准形式：<span class="math inline">\(y&#39; - \frac{\sinx}{1 + \cos x}y = \frac{2x}{1 + \cos x}\)</span></p><p>然后求出<span class="math inline">\(u = e^{-\int \frac{\sin x}{1 +\cos x}dx} = 1 + \cos x\)</span></p><p>对标准形式左乘<span class="math inline">\(u\)</span>：<spanclass="math inline">\((uy)&#39; = ((1 + \cos x)y)&#39; = 2x\)</span></p><p><span class="math inline">\(\therefore (1 + \cos x)y = x^2 +c\)</span></p><p><span class="math inline">\(\therefore y = \frac{x^2 + c}{1 + \cosx}\)</span></p><p><strong>例3.</strong> <span class="math inline">\(T&#39; + kT = kT_e,k &gt; 0\text{ is a constant. } T_e\text{ is a function of}x\)</span></p><p>已经是标准形式了</p><p>然后求出<span class="math inline">\(u = e^{\int k dt} =e^{kt}\)</span></p><p>对标准形式左乘<span class="math inline">\(u\)</span>：<spanclass="math inline">\((uT)&#39; = (e^{kt}T)&#39; = kT_ee^{kt}\)</span></p><p><span class="math inline">\(\therefore e^{kt}T = k\int T_e e^{kt}\mathrm{d}t + c\)</span></p><p><span class="math inline">\(\therefore T = ke^{-kt}\int T_e e^{kt}\mathrm{d}t + ce^{-kt}\)</span></p><p>如果有实际物理意义，即<spanclass="math inline">\(t\)</span>从0开始，并且给定<spanclass="math inline">\(T(0) = T_0\)</span>。那么<spanclass="math inline">\(T\)</span>的积分下限就是0，上限就是<spanclass="math inline">\(t\)</span>。而且还可得到<spanclass="math inline">\(c = T_0\)</span>。则：</p><p><span class="math inline">\(T(t) = ke^{-kt}\int_0^t T_e(x) e^{kx}\mathrm{d}x + T_0e^{-kt}\)</span></p><p>可以发现，如果<span class="math inline">\(t \to\infty\)</span>时，因为<span class="math inline">\(k &gt;0\)</span>，所以<spanclass="math inline">\(T_0e^{-kt}\)</span>会收敛到0。</p><p>因此<spanclass="math inline">\(T_0e^{-kt}\)</span>叫做“暂态解”，<spanclass="math inline">\(ke^{-kt}\int_0^tT_e(x)e^{kx}dx\)</span>叫做“稳态解”</p><p>而且我们发现，当<span class="math inline">\(t \to\infty\)</span>时，<span class="math inline">\(T\)</span>与初始状态<spanclass="math inline">\(T_0\)</span>无关。</p><h3 id="四.-一阶方程换元法">四. 一阶方程换元法</h3><p>这类方程： <span class="math display">\[y&#39;=p(x)y + q(x)y^{n}, n \ne 0\]</span> 叫做“伯努利方程”。解它用换元法，如下：</p><p>同除<span class="math inline">\(y^n\)</span>，得：<spanclass="math inline">\(\frac{y&#39;}{y^n} =\frac{p(x)}{y^{n-1}}+q(x)\)</span></p><p>令<span class="math inline">\(v =\frac{1}{y^{n-1}}\)</span>，则有：<span class="math inline">\(v&#39; =(1-n)y^{-n} \cdot y&#39;\)</span></p><p><span class="math inline">\(\therefore \frac{v&#39;}{1-n} = p(x)v +q(x)\)</span></p><p>发现，线性方程出现了，那么可先解出v，然后回代解出y即可</p><p><strong>例题. </strong><span class="math inline">\(y&#39; =\frac{y}{x} - y^2\)</span></p><p>同除<span class="math inline">\(y^2\)</span>，得：<spanclass="math inline">\(\frac{y&#39;}{y^2} = x^{-1}y^{-1} - 1\)</span></p><p>令<span class="math inline">\(v = y^{-1}\)</span>，则：<spanclass="math inline">\(v&#39; = (-1)y^{-2} \cdot y&#39;\)</span></p><p><span class="math inline">\(\therefore -v&#39; = \frac{v}{x} -1\)</span></p><p><span class="math inline">\(\therefore v&#39; + \frac{v}{x} =1\)</span></p><p><span class="math inline">\(\therefore u = e^{\int \frac1x dx} =x\)</span></p><p>左乘<span class="math inline">\(u\)</span>，得：<spanclass="math inline">\((uv)&#39; = (xv)&#39; = x\)</span></p><p><span class="math inline">\(\therefore xv = \frac12x^2 +c\)</span></p><p><span class="math inline">\(\therefore v = \frac12x +\frac{c}{x}\)</span></p><p><span class="math inline">\(\therefore \frac1y = \frac{x^2 +2c}{2x}\)</span></p><p><span class="math inline">\(\therefore y =\frac{2x}{x^2+2c}\)</span></p><hr /><p>第二类用换元法解的ODE，叫一阶齐次ODE，形如： <spanclass="math display">\[y&#39; = f(\frac{y}{x})\]</span> 即等式右边的基本原子都是<spanclass="math inline">\(\frac{y}{x}\)</span></p><p>一阶齐次ODE的套路是先换元然后分离变量解决。</p><p><strong>例题. </strong>有一个贩毒船，还有一个灯塔，灯塔会对船射出光线，但是船不想被照到，于是船始终保持与光线成45°角一直逃窜，请求出船的运行轨迹。</p><p><img src="9.png" style="zoom:50%;" /></p><p>根据图，可以列出方程： <span class="math display">\[y&#39; = \tan (\frac{\pi}{4} + \alpha) = \frac{\tan \frac{\pi}{4} + \tan\alpha}{1 - \tan \frac{\pi}{4}\tan \alpha} = \frac{1 + \tan \alpha}{1 -\tan \alpha} = \frac{1 + y/x}{1 - y/x}\]</span> 令<span class="math inline">\(v =\frac{y}{x}\)</span>，则<span class="math inline">\(y =xv\)</span>，<span class="math inline">\(y&#39; = v +xv&#39;\)</span></p><p><span class="math inline">\(\therefore v + xv&#39; = \frac{1 + v}{1 -v}\)</span></p><p><span class="math inline">\(\therefore v + x\frac{dv}{dx} = \frac{1 +v}{1 - v}\)</span></p><p><span class="math inline">\(\therefore x\frac{dv}{dx} = \frac{v^2 +1}{1 - v}\)</span></p><p><span class="math inline">\(\therefore \frac{x}{dx} = \frac{v^2 +1}{(1 - v)dv}\)</span></p><p><span class="math inline">\(\therefore \frac{1}{x}dx = \frac{1 -v}{v^2 + 1}dv\)</span></p><p><span class="math inline">\(\therefore \int \frac1x dx = \int \frac{1- v}{v^2 + 1} dv\)</span></p><p><span class="math inline">\(\therefore \ln |x| = \arctan v - \frac12\ln(v^2 + 1) + c\)</span></p><p><span class="math inline">\(\therefore \ln |x| = \arctan \frac{y}{x}- \frac12\ln ((\frac{y}{x})^2 + 1) + c\)</span></p><p><span class="math inline">\(\therefore \arctan \frac{y}{x} = \ln\sqrt{x^2 + y^2} + c\)</span></p><p><span class="math inline">\(\therefore \theta = \ln r +c\)</span></p><p><span class="math inline">\(\therefore r = ce^\theta, c &gt;0\)</span></p><p>上述方程又称为“指数螺旋线”，优美。</p><h3 id="五.-一阶自治ode">五. 一阶自治ODE</h3><p>回顾一下，前面我们已经学会了用几何法一阶ODE（画线素），然后又学了数值法求一阶ODE；</p><p>然后，学了用分离变量法解一阶ODE，以及学会了用通法解决形如<spanclass="math inline">\(y&#39; + p(x)y = q(x)\)</span>的一阶ODE；</p><p>以及，学会了用换元法解决“伯努利ODE“和“一阶齐次ODE”。</p><p>现在我们再来学一种形如：<span class="math inline">\(y&#39; =f(y)\)</span>的一阶ODE解法，这种方程我们称为自治的，因为右侧仅仅由y组成不涉及x。</p><p>当然分离变量法可以解这个方程，但是我们不是想解它，只是想分析一下它的图像性质。</p><p>核心思路就是：画两张图，第一张图是“x-y图”，第二张图是“y-f(y)”图。</p><hr /><p>下面举一个Logistic equation withharvesting的例子（例如渔场养鱼捕鱼，<spanclass="math inline">\(y\)</span>是鱼量，<spanclass="math inline">\(t\)</span>是时间，<spanclass="math inline">\(h\)</span>是捕获量）： <spanclass="math display">\[\frac{dy}{dt} = (a - by)y - h\]</span> 首先令右边为：<span class="math inline">\(f(y) = ay - by^2 -h\)</span></p><p>然后画出<span class="math inline">\(f(y)-y\)</span>图：</p><p><img src="10.png" style="zoom:80%;" /></p><p>可以看出，与x轴有俩交点，分别令值为<span class="math inline">\(a,b\)</span></p><p>然后可以继续画出<span class="math inline">\(y-t\)</span>图：</p><p><img src="11.png" style="zoom: 67%;" /></p><p>可以看出，a和b都是“临界点”。因为一旦<span class="math inline">\(y =a\)</span>或<span class="math inline">\(b\)</span>，那么<spanclass="math inline">\((x, y)\)</span>将一直在<spanclass="math inline">\(y=a\)</span>或<spanclass="math inline">\(b\)</span>这条线上移动，用数值法的作图方法简单即可知道。</p><p>那么，我们会抛出疑问，虽然a和b都是临界点，但是谁是稳定的呢？</p><p>显然可以看出，b是稳定的，因为上下都趋向于它，a是不稳定的，因为上下都远离它。</p><p>对于实际意义来说，就是如果维持当前的捕获量<spanclass="math inline">\(h\)</span>，那么只需要保证初始鱼量&gt;a，即可保证一定时间后，鱼量收敛于b。</p><p>那么，如果我是农场主，我肯定希望捕获量<spanclass="math inline">\(h\)</span>尽可能高，当<spanclass="math inline">\(h\)</span>高时，上面那个二次函数会下移，如果移的太多了，那么整个函数在定义域都是下降的了，那么鱼量会一直下降到负无穷。所以我们要提高<spanclass="math inline">\(h\)</span>，使得二次函数图像刚刚好与x轴有一个交点，值记为<spanclass="math inline">\(c\)</span>的时候停止。此时的<spanclass="math inline">\(y-t\)</span>图如下：</p><p><img src="12.png" style="zoom:67%;" /></p><p>可以看出，只要你保证最开始的鱼量&gt;c，那么最终你的鱼量会收敛于c，而且你的捕获量<spanclass="math inline">\(h\)</span>此时是最高的。</p><hr /><p>OK，上面那个例子非常有趣right？</p><p>但是你可能会注意到一点，为什么图像都是平移相同的？</p><p>因为对于自治方程，等号右边等于<spanclass="math inline">\(y&#39;\)</span>，而且等号右边仅仅与<spanclass="math inline">\(y\)</span>有关，如果我们确定了一个<spanclass="math inline">\(y_0\)</span>，那么这条水平线上的线素都相同。所以说，你自己画画就知道，一阶自治ODE的图像是这种平移相同的了。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;几何法解一阶ODE、欧拉数值法解一阶ODE、分离变量法解一阶ODE、解标准一阶线性ODE、换元法解伯努利方程+一阶齐次ODE、一阶自治ODE图像分析&lt;/p&gt;</summary>
    
    
    
    <category term="4. 大学" scheme="http://error666.top/categories/4-%E5%A4%A7%E5%AD%A6/"/>
    
    <category term="数学" scheme="http://error666.top/categories/4-%E5%A4%A7%E5%AD%A6/%E6%95%B0%E5%AD%A6/"/>
    
    <category term="微分方程" scheme="http://error666.top/categories/4-%E5%A4%A7%E5%AD%A6/%E6%95%B0%E5%AD%A6/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/"/>
    
    
  </entry>
  
  <entry>
    <title>线性代数4</title>
    <link href="http://error666.top/2024/09/15/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B04/"/>
    <id>http://error666.top/2024/09/15/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B04/</id>
    <published>2024-09-15T15:26:20.000Z</published>
    <updated>2024-09-16T15:45:43.936Z</updated>
    
    <content type="html"><![CDATA[<p>总结、知识易混点整理、补充知识</p><span id="more"></span><hr /><h3 id="总结">总结</h3><p>前前后后花了一个月左右把mit 18.06学完了，收获颇丰，感谢教授。</p><p>这门课第一章从（方程组 + 矩阵 +四个基本子空间）出发，讨论了线性代数的基本元素：向量、矩阵、空间。最后用所学知识对电势差问题建模，得到了许多优美的结论。</p><p>第二章开始研究矩阵各种性质，例如正交、投影、行列式、特征值、对角化。并在结尾用马尔可夫矩阵寻找稳态，用正交性对傅里叶级数建模。</p><p>第三章仍然是研究矩阵的各种性质，例如对称、正定、相似、SVD分解。以及将线性变换和矩阵统一起来。利用基变换实现图像压缩展示了线性变换的优美之处。</p><p>总之，这门课不仅帮我扎实的打好了线代基础，而且给了我理解线代的上层视角。以及：马尔可夫矩阵、对角化、SVD分解、基变换压缩这三个知识点也给了我科研上的启发，说不定哪天就可以作为trick来优化我的算法。</p><h3 id="section"></h3><h3 id="知识点易混点整理">知识点易混点整理</h3><ol type="1"><li>如何理解矩阵<spanclass="math inline">\(A\)</span>可对角化的条件是：“有n个线性无关的特征向量”?<ul><li>因为我们在推对角化公式的时候，用的是<span class="math inline">\(AS =S\Lambda\)</span>。<spanclass="math inline">\(S\)</span>是n个特征向量排成的方阵</li><li>因为要右乘<spanclass="math inline">\(S^{-1}\)</span>，所以就要保证<spanclass="math inline">\(S\)</span>的n个列向量线性无关，即<spanclass="math inline">\(A\)</span>有n个线性无关的特征向量</li><li>这样才能推出：<span class="math inline">\(A = S\LambdaS^{-1}\)</span></li></ul></li><li>上面那个判据太困难了，有什么等价判据？<ul><li>若<spanclass="math inline">\(A\)</span>有n个互不相同的特征值，则<spanclass="math inline">\(A\)</span>有n个线性无关的特征向量</li><li>但若没有，则不能说<spanclass="math inline">\(A\)</span>一定没有n个线性无关的特征向量</li><li>所以我们判断一个矩阵是否可对角化，可转换为求其特征值的问题。</li></ul></li><li><span class="math inline">\(A\)</span>有n个线性无关的特征向量 和<span class="math inline">\(A\)</span>的各列线性无关有什么关系？<ul><li>前者可推后者，后者不可推前者</li><li>我来证一下前者可推后者，因为对于特征向量x，有<spanclass="math inline">\(Ax = \lambda x\)</span>，所以<spanclass="math inline">\(\lambda x\)</span>是通过<spanclass="math inline">\(A\)</span>进行列变换得到的，那仅仅通过列变换就可以得到一组线性无关的向量，相当于变换后的列空间就是<spanclass="math inline">\(\mathrm{R}^n\)</span>。而列变换不改变列空间，所以<spanclass="math inline">\(C(A) = \mathrm{R}^n\)</span>，所以<spanclass="math inline">\(A\)</span>的各列线性无关。</li></ul></li><li>对称矩阵一定可以对角化吗？若可以，它的对角化有什么特别之处？<ul><li>是的一定可以。</li><li>对称矩阵<spanclass="math inline">\(A\)</span>可以对角化。那么可写为：<spanclass="math inline">\(A = S\Lambda S^{-1}\)</span></li><li>因为对称矩阵有个很好的性质就是：可选出一组正交特征向量。所以<spanclass="math inline">\(S\)</span>可以是完全正交的，再将其标准化一下，即可得到正交阵<spanclass="math inline">\(Q\)</span>。</li><li>正交阵有一个很好的性质：<span class="math inline">\(Q^{-1} =Q^T\)</span></li><li>所以可对角化的对称矩阵<spanclass="math inline">\(A\)</span>可对角化为：<spanclass="math inline">\(A = Q\Lambda Q^\mathrm{T}\)</span></li></ul></li><li>正定矩阵有什么好处吗？<ul><li>首先正定矩阵是对称矩阵的一个子集，对称矩阵已经有一些很好的性质了，正定矩阵除了有对称矩阵的性质，还有其余很好的性质。比如一定可逆而且<spanclass="math inline">\(x^\mathrm{T}Ax &gt; 0\)</span>。</li><li>如果快速获得一个对称矩阵？<spanclass="math inline">\(A^\mathrm{T}A\)</span>，<spanclass="math inline">\(xx^\mathrm{T} / AA^\mathrm{T}\)</span></li><li>如果快速获得一个正定矩阵？<span class="math inline">\(A^\mathrm{T}A,r(A) = n\)</span></li></ul></li><li></li></ol><h3 id="补充知识">补充知识</h3>]]></content>
    
    
    <summary type="html">&lt;p&gt;总结、知识易混点整理、补充知识&lt;/p&gt;</summary>
    
    
    
    <category term="4. 大学" scheme="http://error666.top/categories/4-%E5%A4%A7%E5%AD%A6/"/>
    
    <category term="数学" scheme="http://error666.top/categories/4-%E5%A4%A7%E5%AD%A6/%E6%95%B0%E5%AD%A6/"/>
    
    <category term="线性代数" scheme="http://error666.top/categories/4-%E5%A4%A7%E5%AD%A6/%E6%95%B0%E5%AD%A6/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>线性代数3</title>
    <link href="http://error666.top/2024/09/14/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B03/"/>
    <id>http://error666.top/2024/09/14/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B03/</id>
    <published>2024-09-14T15:31:27.000Z</published>
    <updated>2024-09-16T15:11:39.326Z</updated>
    
    <content type="html"><![CDATA[<p>对称矩阵、复数矩阵、FFT、正定矩阵、相似矩阵、SVD分解、线性变换、图像压缩、左右逆/伪逆</p><span id="more"></span><hr /><h3 id="一.-对称矩阵及其正定性">一. 对称矩阵及其正定性</h3><p>在这里我们只讨论实对称矩阵。</p><p>我们很喜欢对称矩阵，因为它具有很好的性质。就拿实对称矩阵来举例，它具有下列两个性质：</p><ol type="1"><li>其特征值均为实数</li><li>其一定可选出具有正交的特征向量。这里的“有”，是指可以选出一套完全正交的特征向量（例如在重特征值条件下，可能存在一个平面内向量都可以作为特征向量）</li><li>特征值的符号与主元的符号相同，即正数的个数相同，负数的个数也相同</li></ol><p>上一章我们学过，若方阵<spanclass="math inline">\(A\)</span>具有n个线性无关的特征向量，那么其可以对角化为<spanclass="math inline">\(S\Lambda S^{-1}\)</span>。</p><p>对于有n个线性无关特征向量的对称矩阵来说，因为性质2，所以它的特征向量矩阵可化为一个正交阵<spanclass="math inline">\(Q\)</span>，正交阵满足<spanclass="math inline">\(Q^\mathrm{T} = Q^{=1}\)</span></p><p>所以对于具有n个线性无关特征向量的对称矩阵<spanclass="math inline">\(A\)</span>来说，其可对角化为<spanclass="math inline">\(Q\Lambda Q^T\)</span>。</p><p>把上面的式子进一步展开：</p><p><span class="math inline">\(A = Q\Lambda Q^\mathrm{T} =\begin{bmatrix} q_1 &amp; q_2 &amp; \cdots &amp; q_n \end{bmatrix}\begin{bmatrix} \lambda_1 &amp; &amp; &amp; \\ &amp; \lambda_2 &amp;&amp; \\ &amp; &amp; \cdots &amp; \\ \end{bmatrix} \begin{bmatrix}q_1^\mathrm{T} \\ q_2^\mathrm{T} \\ \cdots \\ \end{bmatrix}\)</span></p><p>利用"线性代数1"讲的用拆分矩阵乘法为加法去理解这个式子，可以写成：</p><p><span class="math inline">\(A = \lambda_1q_1q_1^\mathrm{T} +\lambda_2q_2q_2^\mathrm{T} + \cdots +\lambda_nq_nq_n^\mathrm{T}\)</span></p><p>上面这个式子发现了吗，其实每一项都是一个系数乘一个投影矩阵，因为<spanclass="math inline">\(Q\)</span>是正交矩阵，所以<spanclass="math inline">\(q_i^\mathrm{T}q_i = 1\)</span>。</p><p>所以<spanclass="math inline">\(A\)</span>可以理解为投影矩阵的线性组合，且投影方向都是互相正交的。</p><hr /><p>下面来介绍正定矩阵。</p><p>正定矩阵是对称矩阵的一个子类，且所有特征值&gt;0</p><p>而且它的“子行列式”均&gt;0，子行列式指的是n阶矩阵左上角的所有<spanclass="math inline">\(k \times k, 1 \le k \len\)</span>子行列式数值均为正。这很好理解，由对称矩阵的性质3，我们知道，其所有主元都&gt;0。而行列式就等于主元之积，所以子行列式们自然都大于0。这就是用行列式判定矩阵正定的判据。</p><h3 id="二.-复数矩阵-快速傅里叶变换">二. 复数矩阵, 快速傅里叶变换</h3><p>对不起我的高数很垃圾，这节我也听不懂。等我学完mit18.03再来听这一节课</p><h3 id="三.-正定矩阵">三. 正定矩阵</h3><p>正定矩阵是很好的矩阵，它是对称的而且所有特征值都大于零。那么如何判定一个方阵是正定矩阵呢？这里我给出几种方法</p><ol type="1"><li>所有特征值<span class="math inline">\(\lambda_i &gt; 0\)</span></li><li>所有主元大于零</li><li>所有子行列式们大于零</li><li><span class="math inline">\(x^\mathrm{T}Ax &gt; 0, x \ne\textbf{0}\)</span></li></ol><p>第一个是定义，第二个是因为对称矩阵有一个性质就是特征值正负号与主元相同，所以可根据(1)得到第二条等价条件。第三个是用行列式判断正定性(前面讲过)。第四个是新加的，我们很喜欢用第四个判据。让我们来看看为什么这个判据可以推出矩阵是正定的。</p><p>对于<spanclass="math inline">\(x^\mathrm{T}Ax\)</span>，我们将其展开： <spanclass="math display">\[x^\mathrm{T}Ax = \begin{bmatrix}x_1, x_2, \cdots,x_n\end{bmatrix}\begin{bmatrix} a_{11}x_1+a_{12}x_2+\cdots+a_{1n}x_n \\a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n \\ a_{31}x_1 + a_{32}x_2 +\cdots + a_{3n}x_n \\ \cdots \\ a_{n1}x_1 + a_{n2}x_2 + \cdots +a_{nn}x_n\end{bmatrix} = \\ x_1(a_{11}x_1+a_{12}x_2+\cdots+a_{1n}x_n)+x_2(a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n) + \cdots + x_n(a_{n1}x_1+ a_{n2}x_2 + \cdots + a_{nn}x_n)\]</span>可以发现，每一项都是二次的。其实如果用图像去研究这个函数：<spanclass="math inline">\(f(x) =x^\mathrm{T}Ax\)</span>，（这个函数也叫二次型）</p><p>矩阵为正定时二次型图像见左上角，为半正定时图像见右下角，为非正定/半正定时图像见左下角，负定时图像见右上角</p><p>这些图像研究的是二维方阵的二次型，xy轴是<spanclass="math inline">\(x\)</span>的俩分量<span class="math inline">\(x_1,x_2\)</span>，z轴是<spanclass="math inline">\(f(x)=x^\mathrm{T}Ax\)</span></p><p><img src="1.png" /></p><hr /><p>下面继续讨论下正定矩阵还有哪些性质，假设<spanclass="math inline">\(A, B\)</span>为正定矩阵，<spanclass="math inline">\(C\)</span>为矩阵。</p><p>那么<spanclass="math inline">\(A\)</span>是否可逆呢？答案是肯定的，因为我们知道<spanclass="math inline">\(A\)</span>的子行列式们都大于0，所以<spanclass="math inline">\(det(A) &gt; 0\)</span>，所以<spanclass="math inline">\(A\)</span>是非奇异的，即满秩可逆的。</p><p>那么<spanclass="math inline">\(A^{-1}\)</span>是不是正定矩阵呢？答案是肯定的。因为我们知道<spanclass="math inline">\(A^{-1}\)</span>的特征值们就是<spanclass="math inline">\(A\)</span>的特征值取倒数，而<spanclass="math inline">\(A\)</span>的特征值都大于0，所以<spanclass="math inline">\(A^{-1}\)</span>的特征值也都大于0，所以<spanclass="math inline">\(A^{-1}\)</span>也为正定矩阵。</p><p>那么<span class="math inline">\(A +B\)</span>是不是正定矩阵呢？答案是肯定的。我们来看看<spanclass="math inline">\(x^\mathrm{T}(A+B)x = x^\mathrm{T}Ax +x^\mathrm{T}Bx &gt; 0\)</span>，所以<span class="math inline">\(A +B\)</span>也是正定矩阵。</p><p>那么<spanclass="math inline">\(C^\mathrm{T}C\)</span>是不是正定矩阵呢？答案是不一定。我们来看看<spanclass="math inline">\(x^\mathrm{T}(C^\mathrm{T}C)x = (Cx)^\mathrm{T}(CX)= \|Cx\|^2 \ge 0\)</span>。</p><p>所以<spanclass="math inline">\(C^\mathrm{T}C\)</span>至少是半正定的，那什么时候是正定的呢？只要没有非零向量使得<spanclass="math inline">\(Cx = \textbf{0}\)</span>，那么就可以保证：<spanclass="math inline">\(x^\mathrm{T}(C^\mathrm{T}C)x &gt; 0\)</span>。</p><p>即要保证<span class="math inline">\(N(C) =\{\textbf{0}\}\)</span>，即要保证<span class="math inline">\(r(C) =n\)</span>，即可保证<spanclass="math inline">\(C^\mathrm{T}C\)</span>是正定矩阵。</p><p>还记得<spanclass="math inline">\(C^\mathrm{T}C\)</span>在哪用到吗？即最小二乘求最优近似解那里，最后方程为：<spanclass="math inline">\(C\hat{x}=Pb =C(C^\mathrm{T}C)^{-1}C^\mathrm{T}b\)</span></p><p>只要保证<span class="math inline">\(C\)</span>各列线性无关，则<spanclass="math inline">\(C^\mathrm{T}C\)</span>是正定矩阵，所以<spanclass="math inline">\(C^\mathrm{T}C\)</span>可逆，则上述方程成立，同左乘<spanclass="math inline">\(C^\mathrm{T}\)</span>，然后再左乘<spanclass="math inline">\((C^\mathrm{T}C)^{-1}\)</span>，即可求出<spanclass="math inline">\(\hat{x}\)</span>。</p><h3 id="四.-相似矩阵">四. 相似矩阵</h3><p><span class="math inline">\(A, B\)</span>均为<spanclass="math inline">\(n \times n\)</span>的方阵，那么<spanclass="math inline">\(A\)</span>与<spanclass="math inline">\(B\)</span>相似，用数学语言表达为：存在可逆矩阵<spanclass="math inline">\(M\)</span>，使得<span class="math inline">\(B =M^{-1}AM\)</span></p><p>它具有一个性质：相似的矩阵拥有相同的特征值。</p><blockquote><p>证明：</p><p><span class="math inline">\(Ax = \lambda x\)</span></p><p><span class="math inline">\(A(MM^{-1})x = \lambda x\)</span></p><p><span class="math inline">\(M^{-1}AMM^{-1}x = \lambdaM^{-1}x\)</span></p><p><span class="math inline">\(BM^{-1}x = \lambda M^{-1}x\)</span></p><p><span class="math inline">\(B(M^{-1}x) = \lambda(M^{-1}x)\)</span></p><p>证毕。且可看出特征值虽然不变，但是特征向量由<spanclass="math inline">\(x\)</span>变为了<spanclass="math inline">\(M^{-1}x\)</span>。</p></blockquote><p>举个例子，例如可对角化的矩阵<spanclass="math inline">\(A\)</span>，其可分解为：<spanclass="math inline">\(A = S^{-1}\Lambda S\)</span>。其中<spanclass="math inline">\(S\)</span>是可逆的，因为各特征向量线性无关。所以<spanclass="math inline">\(A\)</span>与<spanclass="math inline">\(\Lambda\)</span>就相似。而且我们会发现，<spanclass="math inline">\(A\)</span>与<spanclass="math inline">\(\Lambda\)</span>的特征值一样。</p><h3 id="五.-奇异值分解">五. 奇异值分解</h3><p>SVD分解也是一种矩阵分解的形式。至今为止，我们已经学过了许多矩阵分解方法了：<spanclass="math inline">\(LU\)</span>、<span class="math inline">\(S\LambdaS^{-1}\)</span>（可对角矩阵）、<span class="math inline">\(Q\LambdaQ^\mathrm{T}\)</span>（对称矩阵）、<spanclass="math inline">\(QR\)</span>（满秩矩阵）。</p><p>可以发现，除了<spanclass="math inline">\(LU\)</span>分解，其余分解都对矩阵有限制条件。但是这节我要讲的SVD分解，对任意矩阵都成立。</p><p>SVD用数学语言描述如下：</p><p><span class="math inline">\(A = U\Sigma V^\mathrm{T}\)</span></p><p><span class="math inline">\(A \in \mathbb{R}^{m \timesn}\)</span>是任意矩阵，<span class="math inline">\(U \in \mathbb{R}^{m\times m}\)</span>是正交阵，<span class="math inline">\(\Sigma \in\mathbb{R}^{m \times n}\)</span>是"对角阵"，<spanclass="math inline">\(V^\mathrm{T} \in \mathbb{R}^{n \timesn}\)</span>是正交阵。</p><p>我们来看下在已知<spanclass="math inline">\(A\)</span>下，如何计算出<spanclass="math inline">\(U, \Sigma, V\)</span></p><p><span class="math inline">\(\because A = U\SigmaV^\mathrm{T}\)</span></p><p><span class="math inline">\(\therefore A^\mathrm{T}A =(V\Sigma^\mathrm{T}U^\mathrm{T})(U\Sigma V^\mathrm{T}) =V\Sigma^\mathrm{T}\Sigma V^\mathrm{T}\)</span></p><p><span class="math inline">\(\because A^\mathrm{T}A \text{ is asymmetric matrix}\)</span></p><p><span class="math inline">\(\therefore V \text{ is } Q,\Sigma^\mathrm{T}\Sigma \text{ is } \Lambda, \text{ where }A^\mathrm{T}A = Q\Lambda Q^{\mathrm{T}}\)</span></p><p>所以，对于<span class="math inline">\(A\)</span>，求出<spanclass="math inline">\(A^\mathrm{T}A\)</span>的特征值开根和对应的相互正交的标准特征向量，对应排好就是<spanclass="math inline">\(\Sigma, V\)</span></p><p>那有了<span class="math inline">\(\Sigma, V\)</span>后，如何求<spanclass="math inline">\(U\)</span>呢？</p><p><span class="math inline">\(\because A = U\SigmaV^\mathrm{T}\)</span></p><p><span class="math inline">\(\therefore AV = U\Sigma V^\mathrm{T}V =U\Sigma\)</span></p><p>即再算出<spanclass="math inline">\(AV\)</span>，然后每一列除对应的奇异值，即可得到<spanclass="math inline">\(U\)</span>。</p><hr /><p>多说无益，举个例子来看下如何将矩阵<spanclass="math inline">\(A\)</span>分解为<spanclass="math inline">\(U\Sigma V^\mathrm{T}\)</span></p><p>已知<span class="math inline">\(A = \begin{bmatrix} 4 &amp; 4 \\ -3&amp; 3 \end{bmatrix}\)</span></p><p>先算出<span class="math inline">\(A^\mathrm{T}A = \begin{bmatrix} 25&amp; 7 \\ 7 &amp; 25 \end{bmatrix}\)</span></p><p>然后求其特征值和对应的互相正交的标准特征向量：</p><p><span class="math inline">\(\lambda_1 = 18, x_1 =\frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ -1 \end{bmatrix}\)</span></p><p><span class="math inline">\(\lambda_2 = 32, x_2 =\frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ 1 \end{bmatrix}\)</span></p><p>即已经求出<span class="math inline">\(\Sigma = \begin{bmatrix}\sqrt{18} &amp; \\ &amp; \sqrt{32} \end{bmatrix}, V =\frac{1}{\sqrt{2}}\begin{bmatrix} 1 &amp; 1 \\ -1 &amp; 1\end{bmatrix},V^\mathrm{T} = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 &amp; -1 \\ 1 &amp; 1\end{bmatrix}\)</span></p><p>然后算出<span class="math inline">\(AV\)</span>，即<spanclass="math inline">\(U\Sigma\)</span>：<spanclass="math inline">\(\begin{bmatrix} 0 &amp; \frac{8}{\sqrt{2}} \\-\frac{6}{\sqrt{2}} &amp; 0\end{bmatrix}\)</span></p><p>每一列除对应的<spanclass="math inline">\(\sqrt{\lambda_i}\)</span>，即第一列除<spanclass="math inline">\(\sqrt{18}\)</span>，第二列除<spanclass="math inline">\(\sqrt{32}\)</span>，得到<spanclass="math inline">\(U\)</span>：<spanclass="math inline">\(\begin{bmatrix} 0 &amp; 1 \\ -1 &amp; 0\end{bmatrix}\)</span></p><p>现在所有东西都已求出来： <span class="math display">\[U = \begin{bmatrix} 0 &amp; 1 \\ -1 &amp; 0 \end{bmatrix}, \Sigma =\begin{bmatrix} \sqrt{18} &amp; \\ &amp; \sqrt{32} \end{bmatrix},V^\mathrm{T} = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 &amp; -1 \\ 1 &amp; 1\end{bmatrix}\]</span> 我们来验证一下，将这仨乘起来，发现等于<spanclass="math inline">\(A\)</span>，check！</p><hr /><p>SVD有什么用呢？我来举一个例子，例如某件事物需要用两个维度来刻画，即<spanclass="math inline">\(f: (x, y) \toz\)</span>。那么适合用一个矩阵来表示，不妨表示为<spanclass="math inline">\(A\)</span>。</p><p>如果我们将<span class="math inline">\(A\)</span>进行SVD分解为<spanclass="math inline">\(U\SigmaV^\mathrm{T}\)</span>，进一步展开可以得到： <spanclass="math display">\[A = U\Sigma V^\mathrm{T} = \begin{bmatrix}u_1, u_2, \cdots,u_n\end{bmatrix} \cdot \begin{bmatrix}\sigma_1 &amp; &amp; &amp; \\&amp; \sigma_2 &amp; &amp; \\ &amp; &amp; \cdots &amp; \\ &amp; &amp;&amp; \sigma_n\end{bmatrix} \cdot \begin{bmatrix}v_1^\mathrm{T} \\v_2^\mathrm{T} \\ \cdots \\ v_n^\mathrm{T}\end{bmatrix} =\sigma_1u_1v_1^\mathrm{T} + \sigma_2u_2v_2^\mathrm{T} + \cdots +\sigma_nu_nv_n^\mathrm{T}\]</span> 可以发现，一个矩阵被拆解为若干矩阵相加，奇异值<spanclass="math inline">\(\sigma_i\)</span>可以理解为矩阵所占的权重。那些<spanclass="math inline">\(\sigma_i\)</span>大的矩阵说明对整体矩阵的影响越大。</p><p>相当于对于一个模式<spanclass="math inline">\(A\)</span>，我可以知道<spanclass="math inline">\(A\)</span>重点体现在哪些子模式上，进而重点去优化那些子模式。</p><h3 id="六.-线性变换">六. 线性变换</h3><p>什么是坐标？</p><p>其实，世界上坐标不是天生存在的东西。而是人类以某些东西为标准，去测量其它东西的一种度量。</p><p>例如我们通常认知的坐标系，其实上就是以俩正交的标准向量基为标准，坐标就是用这俩去线性组合的系数。</p><p>所以如何理解一个线性变化？</p><p>我这么说吧，假设我们已经确定好一组基(n个)，那么这组基作为我们的“观测基准”，可以以它们为标准观测出万物的状态<spanclass="math inline">\((c_1, c_2, \cdots, c_n)\)</span>。（<spanclass="math inline">\(c_i\)</span>跟第i个基向量有关）</p><p>但是现在，我想换一套观察工具，也就是换一组基(m个)，那么换完之后，<strong>同样</strong>的一个事物，在之前用旧基观测的时候，它的状态是<spanclass="math inline">\((c_1, c_2, \cdots,c_n)\)</span>，用新基观测的时候，它的状态变为<spanclass="math inline">\((d_1, d_2, \cdots, d_m)\)</span>。</p><p>那么有没有一种映射，可以直接让我从<span class="math inline">\((c_1,c_2, \cdots, c_n)\)</span>直接可以得到<span class="math inline">\((d_1,d_2, \cdots, d_m)\)</span>？有，它就是——线性变换。</p><p>如何构造这个线性变换呢？如下：</p><p>对于旧基(<spanclass="math inline">\(v\)</span>)的每个基向量，我们都先用新基(<spanclass="math inline">\(w\)</span>)去观测它，并得到观测状态<spanclass="math inline">\(a_{ij}\)</span>： <span class="math display">\[\begin{align*}    v_1 &amp;= a_{11}w_1 + a_{21}w_2 + \cdots + a_{m1}w_m \\    v_2 &amp;= a_{12}w_1 + a_{22}w_2 + \cdots + a_{m2}w_m \\    &amp;\cdots \\    v_n &amp;= a_{1n}w_1 + a_{2n}w_2 + \cdots + a_{mn}w_m\end{align*}\]</span> 对于一件事物<spanclass="math inline">\(x\)</span>，用旧基观测可以得到：<spanclass="math inline">\(x = c_1v_1 + c_2v_2 + \cdots + c_nv_n\)</span></p><p>继续展开： <span class="math display">\[\begin{align*}    x &amp;= c_1v_1 + c_2v_2 + \cdots + c_nv_n \\      &amp;= c_1a_{11}w_1 + c_1a_{21}w_2 + \cdots + c_1a_{m1}w_m + \\      &amp;~~~~~c_2a_{12}w_1 + c_2a_{22}w_2 + \cdots + c_2a_{m2}w_m + \\      &amp;~~~~~\cdots \\      &amp;~~~~~c_na_{1n}w_1 + c_na_{2n}w_2 + \cdots + c_na_{mn}w_m \\      &amp;= (c_1a_{11}+c_2a_{12}+\cdots+c_na_{1n})w_1 + \\      &amp;~~~~~(c_1a_{21}+c_2a_{22}+\cdots+c_na_{2n})w_2 + \\      &amp;~~~~~\cdots \\      &amp;~~~~~(c_1a_{m1}+c_2a_{m2}+\cdots+c_na_{mn})w_m \\      &amp;=d_1w_1 + d_2w_2 + \cdots + d_mw_m\end{align*}\]</span> 用矩阵形式表达： <span class="math display">\[Ac = \begin{bmatrix}a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n} \\a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \\\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn}\end{bmatrix}\begin{bmatrix}c_1 \\ c_2 \\ \vdots \\ c_n\end{bmatrix}=\begin{bmatrix}a_{11}c_1 + a_{12}c_2 + \cdots + a_{1n}c_n \\a_{21}c_1 + a_{22}c_2 + \cdots + a_{2n}c_n \\\vdots \\a_{m1}c_1 + a_{m2}c_2 + \cdots + a_{mn}c_n\end{bmatrix}=\begin{bmatrix}d_1 \\d_2 \\\vdots \\d_m\end{bmatrix}\]</span> 所以我们从旧观测状态<span class="math inline">\((c_1, c_2,\cdots, c_n)\)</span>得到了新观测状态<span class="math inline">\((d_1,d_2, \cdots, d_m)\)</span>。</p><p>问题的关键就是确定这个<spanclass="math inline">\(A\)</span>矩阵，从旧基到新基的线性变换也就体现在这个<spanclass="math inline">\(A\)</span>矩阵上。</p><p>那这个<span class="math inline">\(A\)</span>咋求呢？<spanclass="math inline">\(A\)</span>的每一列其实就是每一个旧基的基向量用新基去观测得到的状态。</p><p>至此，我们有了一种全新的视角去看待矩阵乘法<spanclass="math inline">\(Ax = b\)</span></p><p><strong><spanclass="math inline">\(x\)</span>是旧基下对某事物的观测状态，<spanclass="math inline">\(b\)</span>是新基下对某事物的观测状态。而实现基转换功能的东西，就是<spanclass="math inline">\(A\)</span>。</strong></p><p><strong><spanclass="math inline">\(A\)</span>的每一列其实就是每一个旧基的基向量用新基去观测得到的状态。</strong></p><hr /><p>至此，线性变换与矩阵乘法彻底联系了起来。线性变换就是矩阵，矩阵就是线性变换。</p><p>而我们知道，矩阵满足<span class="math inline">\((A + B)x = Ax + Bx,(cA)x = c(Ax)\)</span>。所以线性变换<spanclass="math inline">\(T\)</span>是满足加法和数乘的，即：</p><ol type="1"><li><span class="math inline">\(T(v + w) = T(v) + T(w)\)</span></li><li><span class="math inline">\(T(cv) = cT(v)\)</span></li></ol><p>其实线性变换一定满足<span class="math inline">\(T(0) =0\)</span>，因为当<span class="math inline">\(v = w =0\)</span>时，<span class="math inline">\(T(0) =2T(0)\)</span>，所以<span class="math inline">\(T(0) =0\)</span>。所以可以通过<spanclass="math inline">\(T(0)\)</span>是否等于0来快速排除一些不是线性变换的映射。</p><hr /><p>告诉你三个有趣的事实：</p><ol type="1"><li>矩阵的逆就是线性变换的逆变换</li><li>进行多次线性变换就是多个矩阵连乘，这样子矩阵乘法就有了几何上的直观理解</li><li>正交阵相当于对空间进行旋转，对角阵相当于对空间进行拉伸</li></ol><h3 id="七.-图像压缩">七. 图像压缩</h3><p>思考一个<span class="math inline">\(512 \times512\)</span>的图像，我们需要<span class="math inline">\(512 \times512\)</span>个数来保存图像的状态。</p><p>令<span class="math inline">\(n = 512 \times 512\)</span></p><p>具体来说，一个状态可以用一个<spanclass="math inline">\(\mathbb{R}^{n}\)</span>向量<spanclass="math inline">\(x\)</span>来描述（把矩阵拉成一个向量）</p><p>而我们存储的<span class="math inline">\(512 \times512\)</span>个数，其实是标准基的系数： <span class="math display">\[x = c_1\begin{bmatrix}1\\ \\ \\ \\ \\ \end{bmatrix} +c_2\begin{bmatrix}\\ 1 \\ \\ \\ \\ \end{bmatrix} + \cdots +c_n\begin{bmatrix} \\ \\ \\ \\ 1\end{bmatrix}\]</span> 那如果换一组基呢？</p><p>假设我们的旧基（即标准基）为<spanclass="math inline">\(v\)</span>，新基为<spanclass="math inline">\(w\)</span>。旧的观测状态为<spanclass="math inline">\((c_1, c_2, ...,c_n)\)</span>，那么新的观测状态<span class="math inline">\((d_1, d_2,..., d_n)\)</span>需要通过<spanclass="math inline">\(Ac\)</span>才能得到，<spanclass="math inline">\(A\)</span>的每一列为每一个旧基的基向量用新基去观测得到的状态。</p><p><span class="math inline">\(A^{-1} =B\)</span>的每一列为每一个新基的基向量用旧基去观测的状态，所以<spanclass="math inline">\(B\)</span>其实就是把新基的基向量排成一排。</p><p>所以我们新的观测状态：<span class="math inline">\(d =B^{-1}c\)</span>。</p><p>用新观测状态表示图片向量：<span class="math inline">\(x = d_1w_1 +d_2w_2 + \cdots + d_nw_n\)</span></p><p>我们可以设定一个阈值，对于那些很小的<spanclass="math inline">\(d_i\)</span>那一项，我们就直接丢弃它，不存储了。例如我只要前5大的<spanclass="math inline">\(d_i\)</span>，那么我原本要存<spanclass="math inline">\(n\)</span>个数，现在我只需要存5个数了。</p><p>我还原出来的图片向量即为：<span class="math inline">\(\hat{x} =d_1w_1 + d_2w_2 + \cdots + d_5w_5\)</span>。</p><p>非常巧妙，嗯哼？</p><hr /><p>相当于用时间换空间，因为压缩和还原的过程我都需要进行矩阵运算。所以想加速压缩/还原速度我需要保证挑选出来的新基组成的<spanclass="math inline">\(B\)</span>的逆好求。</p><p>同时，为了尽可能压缩空间，我要使得挑选出来的新基的观测状态<spanclass="math inline">\((d_1, d_2, \cdots, d_n)\)</span>尽可能多的使<spanclass="math inline">\(d_i &lt;\text{阈值}\)</span>。这样我就可能少保存系数，从而达到压缩存储空间的效果。</p><p>注意，为啥我新基的个数要和旧基保持一样（即都有<spanclass="math inline">\(n\)</span>个基向量）？因为图片大小为<spanclass="math inline">\(512 \times512\)</span>，你不能直接把人家图片大小给改了啊，直接通过砍图片大小来达到的压缩不叫压缩。</p><hr /><p>多说一嘴，基的选择要结合实际情况分析，例如这张图片色彩很单调，且一大片区域都是同一种颜色的情况。那么基向量里必有<spanclass="math inline">\(\begin{bmatrix}1 \\ 1 \\ \vdots \\ 1\end{bmatrix}\)</span>。因为这个基向量表达的图片状态就是纯单色图片。当然了，我意思是这个基向量肯定起着主导作用（也就是系数<spanclass="math inline">\(d_i\)</span>大），但是仍要结合使用别的基向量，要不你还原出来的图片就是纯单色图片。</p><h3 id="八.-左右逆-伪逆">八. 左右逆, 伪逆</h3><p>对于矩阵<span class="math inline">\(A \in \mathbb{R}^{m \timesn}\)</span>，若<spanclass="math inline">\(A\)</span>的各列线性无关。则<spanclass="math inline">\(A\)</span>存在左逆：<spanclass="math inline">\((A^\mathrm{T}A)^{-1}A^\mathrm{T}\)</span>。</p><p>拿这玩意左乘<span class="math inline">\(A\)</span>可得到<spanclass="math inline">\(I\)</span>，所以叫左逆。</p><p>为啥条件是各列线性无关呢？因为我们知道<spanclass="math inline">\(A^\mathrm{T}A\)</span>一定是半正定矩阵。且当<spanclass="math inline">\(N(A) ={\textbf{0}}\)</span>时，升级为正定矩阵，而正定矩阵一定可逆。所以才能有<spanclass="math inline">\((A^\mathrm{T}A)^{-1}\)</span>。</p><hr /><p>对应的，我可以得到右逆的定义，若<spanclass="math inline">\(A\)</span>的各行线性无关，则<spanclass="math inline">\(A\)</span>存在右逆：<spanclass="math inline">\(A^\mathrm{T}(AA^\mathrm{T})^{-1}\)</span>。</p><hr /><p>下面来讨论一下伪逆。</p><p>我们知道，如果矩阵<span class="math inline">\(r(A) &lt; \min(m, n), A\in \mathbb{R}^{m \times n}\)</span>，那么对于无解的最小二乘方程：<spanclass="math inline">\(Ax = b\)</span>是求不出最优近似解的。但是伪逆<spanclass="math inline">\(A^+\)</span>却可以求到"最优稳定解"<spanclass="math inline">\(\hat{x}\)</span>，满足：<spanclass="math inline">\(\| A\hat{x}-b \| \le \| Ax - b \|\)</span>。</p><p>ok，那如何求伪逆呢？用SVD。这里我直接给出公式：</p><p><span class="math inline">\(A^+ = V\Sigma^+U^\mathrm{T}\)</span>，<spanclass="math inline">\(\Sigma^+\)</span>就是<spanclass="math inline">\(\Sigma\)</span>的每一个奇异值取倒数。</p><p>（伪逆这里讲的很浅，因为目前还不怎么用得着，等以后需要学习原理的时候再补充）</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;对称矩阵、复数矩阵、FFT、正定矩阵、相似矩阵、SVD分解、线性变换、图像压缩、左右逆/伪逆&lt;/p&gt;</summary>
    
    
    
    <category term="4. 大学" scheme="http://error666.top/categories/4-%E5%A4%A7%E5%AD%A6/"/>
    
    <category term="数学" scheme="http://error666.top/categories/4-%E5%A4%A7%E5%AD%A6/%E6%95%B0%E5%AD%A6/"/>
    
    <category term="线性代数" scheme="http://error666.top/categories/4-%E5%A4%A7%E5%AD%A6/%E6%95%B0%E5%AD%A6/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>线性代数2</title>
    <link href="http://error666.top/2024/09/12/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B02/"/>
    <id>http://error666.top/2024/09/12/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B02/</id>
    <published>2024-09-12T13:56:55.000Z</published>
    <updated>2024-10-04T13:33:17.253Z</updated>
    
    <content type="html"><![CDATA[<p>正交向量/空间、对向量/空间投影、正交矩阵、正交化、行列式、特征值/特征向量、(特征值)对角化、马尔可夫矩阵/傅里叶级数</p><span id="more"></span><hr /><h3 id="一.-正交向量与子空间">一. 正交向量与子空间</h3><p><img src="1.png" /></p><p>先来欣赏下这幅图，这幅图的意思就是对于矩阵<spanclass="math inline">\(A\)</span>，其行空间<spanclass="math inline">\(C(A^\mathrm{T})\)</span>与零空间<spanclass="math inline">\(N(A)\)</span>正交，其列空间<spanclass="math inline">\(C(A)\)</span>与左零空间<spanclass="math inline">\(N(A^\mathrm{T})\)</span>正交。</p><p>什么是向量正交？俩向量正交就是他们俩间夹角呈90度。如果判别俩向量正交？若<spanclass="math inline">\(x^\mathrm{T} \cdot y =0\)</span>，则x与y正交。</p><p>上面那个公式如何推导的呢？</p><p>首先思考如何描述一个向量的长度，假设有一个向量（1,2,3），那么它的长度显然是<spanclass="math inline">\(\sqrt{1^2 + 2^2 +3^2}\)</span>对吧，用向量表示就是<spanclass="math inline">\(\sqrt{x^Tx}\)</span>。</p><p>好的，知道了如何表示向量长度又如何，假设我给你向量<spanclass="math inline">\(x, y\)</span>，你咋判断呢？</p><p>首先思考下<span class="math inline">\(x +y\)</span>是什么？很容易想到，可以把x和y看作三角形的两边，那么<spanclass="math inline">\(x +y\)</span>就是斜边。那么三条边都知道了，如果xy正交的话，那么就会有<spanclass="math inline">\(x^\mathrm{T}x + y^\mathrm{T}y =(x+y)^\mathrm{T}(x+y)\)</span>,化简可得：<spanclass="math inline">\(x^\mathrm{T}x + y^\mathrm{T}y = x^\mathrm{T}x +x^\mathrm{T}y + y^\mathrm{T}x + y^\mathrm{T}y\)</span>，继续化简：<spanclass="math inline">\(x^\mathrm{T}y + y^\mathrm{T}x =0\)</span>，继续化简：<spanclass="math inline">\(2x^\mathrm{T}y=0\)</span>，所以得到<spanclass="math inline">\(x^\mathrm{T}y = 0\)</span>。</p><hr /><p>向量正交很容易理解，但如果我说，俩子空间正交，又是什么意思呢？</p><p>假设我有子空间<span class="math inline">\(S\)</span>和<spanclass="math inline">\(T\)</span>，那么S和T正交，当且仅当S中的每个向量和T中的每个向量正交。</p><p>ok，举个反直觉的实际例子：墙壁和地面正交吗？答案是否，因为墙壁和地面的交集边缝，边缝自己和自己不正交，所以不满足定义。</p><p>所以需要注意的是，若两个空间正交，那么它们一定不会交于某个非零向量。</p><p>所以在实际例子中，最常见的正交就是两条不重合的过原点且为90度的直线，它们是正交的。</p><hr /><p>回到四个基本子空间，现在我要给出结论：行空间正交与零空间。</p><p>为什么，看下面这幅图（Ax=0）：</p><p><img src="2.png" /></p><p>可以发现，row1和x做点乘为0，row2和x做点乘为0，......，所以x与所有行都正交。行空间可以表示为这些行的线性组合，即<spanclass="math inline">\(k_1row_1 + k_2row_2 + ... +k_mrow_m\)</span>，显然，做下点乘：<span class="math inline">\(x \cdot(k_1row_1 + k_2row_2 + ... + k_mrow_m) =0\)</span>，所以行空间与零空间正交。</p><p>同样的，我们可以得出结论：列空间与左零空间正交。道理一样我就不证了。</p><hr /><p>还有有趣的一点，实际上，行空间 + 零空间 = <spanclass="math inline">\(\mathbb{R}^n\)</span>，列空间 + 左零空间 = <spanclass="math inline">\(\mathbb{R}^m\)</span>。</p><p>从上一节的公式就可以一窥究竟：<span class="math inline">\(dim(S) +dim(U) = dim(S + U) + dim(S \cap U)\)</span>。</p><p>把行空间和零空间代入，秩分别为r和n - r，所以(行空间 +零空间)的维素就是n，而且行空间和零空间的分量个数都为n，所以(行空间 +零空间)就等于<span class="math inline">\(\mathbb{R}^n\)</span>。</p><p>列空间 + 左零空间 = <spanclass="math inline">\(\mathbb{R}^m\)</span>同理，我就不证了。</p><p>为了定义这种现象，我们把行空间和零空间称为正交补的。即：行空间与零空间正交补，列空间与左零空间正交补。</p><h3 id="二.-子空间投影">二. 子空间投影</h3><p>首先先从最简单的一个例子开始：</p><p><img src="3.png" /></p><p>我们将向量b投影到向量a上，投影向量为p，误差向量为e。可以发现，p是a的某倍。所以问题的关键，就是在找到这个系数x。</p><p>ok，由几何性质我们知道，向量e与a正交，所以：<spanclass="math inline">\((b - xa)^\mathrm{T}a = 0\)</span>，整理可得：<spanclass="math inline">\(b^\mathrm{T}a =xa^\mathrm{T}a\)</span>，继续：<span class="math inline">\(x =\frac{b^\mathrm{T}a}{a^\mathrm{T}a}\)</span>。当然也可以写成：<spanclass="math inline">\(x =\frac{a^\mathrm{T}b}{a^\mathrm{T}a}\)</span>。</p><p>所以<span class="math inline">\(p = a \cdot\frac{a^\mathrm{T}b}{a^\mathrm{T}a}\)</span>。</p><p>ok上个这个小例子推出来的公式可以留个印象。现在我们进一步抽象化，把投影这个动作看作一种运算，也就是最好弄出一个投影矩阵P，然后b左乘一下这个投影矩阵Pb，就可以得到p，这是我们想要的。</p><p>其实通过上面的公式就可看出，<span class="math inline">\(P =\frac{aa^\mathrm{T}}{a^\mathrm{T}a}\)</span>。非常优美的式子。</p><p>我们来思考一下这个投影矩阵P的列空间。它的列空间是啥呢？</p><p>考虑<spanclass="math inline">\(Px\)</span>，x可取任意向量，那么就相当于对P的列向量做任意线性组合，也就是生成了<spanclass="math inline">\(C(P)\)</span>。通过几何意义我们可以知道，<spanclass="math inline">\(C(P)\)</span>就是通过a的一条线，而且<spanclass="math inline">\(r(P) = 1\)</span>。</p><p>这个投影矩阵<spanclass="math inline">\(P\)</span>还有一些好玩的性质，比如通过其公式，显然看出它是一个对称矩阵，所以<spanclass="math inline">\(P^\mathrm{T} = P\)</span>。</p><p>以及<span class="math inline">\(P^2 =P\)</span>，为什么呢？前面我们说了<spanclass="math inline">\(C(P)\)</span>就是通过a的一条线，而且<spanclass="math inline">\(r(P) = 1\)</span>，说明<spanclass="math inline">\(P\)</span>的每一个列向量都在a那条直线上，所以<spanclass="math inline">\(P \cdot P\)</span>相当于对<spanclass="math inline">\(P\)</span>的每一列做投影，那么通过几何性质，若一个向量本就在线上，投影之后位置仍不变，所以<spanclass="math inline">\(P \cdot P = P\)</span>。</p><p>总结一下：</p><ol type="1"><li><span class="math inline">\(x =\frac{a^\mathrm{T}b}{a^\mathrm{T}a}\)</span>，<spanclass="math inline">\(p = ax = a \cdot\frac{a^\mathrm{T}b}{a^\mathrm{T}a}\)</span></li><li><span class="math inline">\(P =\frac{aa^\mathrm{T}}{a^\mathrm{T}a}\)</span>， <spanclass="math inline">\(p = Pb\)</span></li><li><span class="math inline">\(C(p) = \text{过a的线}\)</span>， <spanclass="math inline">\(r(P) = 1\)</span></li><li><span class="math inline">\(P^\mathrm{T} = P\)</span>，<spanclass="math inline">\(P^2 = P\)</span></li></ol><hr /><p>现在让我们进入更高维度的投影。</p><p>先剧透一下，学这个可以解决什么问题呢？我们知道，Ax =b并不100%有解，有时候，我们在无解的情况下，就想找到最优近似解，也就是误差向量<spanclass="math inline">\(e\)</span>最小。</p><p>怎么做呢？很自然的就联想到投影。因为此时b不在<spanclass="math inline">\(C(A)\)</span>上，所以我们就要将<spanclass="math inline">\(b\)</span>投影到<spanclass="math inline">\(C(A)\)</span>上，得到投影向量p。那么此时求解<spanclass="math inline">\(A\hat{x} = p\)</span>，得到的<spanclass="math inline">\(\hat{x}\)</span>就是最优近似解。</p><p>很巧妙不是吗？这用到了向量对空间的投影。</p><p>继续思考，我们要做的事情，就是找到一个x，使得b与(Ax)的误差向量e垂直C(A)。此时的x就是<spanclass="math inline">\(\hat{x}\)</span>。</p><p>把上面的语言转化为数学语言： <span class="math display">\[\begin{align*}    &amp;p = A\hat{x} \\    &amp;e = b - p = b - (A\hat{x}) \\    &amp;e^\mathrm{T}A = 0\end{align*}\]</span> （<span class="math inline">\(e\)</span>垂直与<spanclass="math inline">\(C(A)\)</span>与<spanclass="math inline">\(e^\mathrm{T}A=0\)</span>等价，因为<spanclass="math inline">\(C(A)\)</span>可由列向量线组出来，所以只要<spanclass="math inline">\(e\)</span>垂直于<spanclass="math inline">\(A\)</span>的每一个列向量，那么<spanclass="math inline">\(e\)</span>就垂直于<spanclass="math inline">\(C(A)\)</span>）</p><p>（上面的方程我们还可以发现一点有趣的事实，显然<spanclass="math inline">\(e\)</span>位于<spanclass="math inline">\(A\)</span>的左零空间内，而通过几何意义我们知道，<spanclass="math inline">\(e\)</span>与<spanclass="math inline">\(C(A)\)</span>正交。很巧的事情，<spanclass="math inline">\(N(A^\mathrm{T})\)</span>与<spanclass="math inline">\(C(A)\)</span>正交。一切都对上了，是不是？）</p><p>OK，整理上面的方程，可以得到：<spanclass="math inline">\(A^\mathrm{T}A\hat{x} =A^\mathrm{T}b\)</span>。</p><p>如果<spanclass="math inline">\(A^\mathrm{T}A\)</span>是可逆的，那么直接乘它的可逆矩阵，即可求出<spanclass="math inline">\(\hat{x}\)</span>。</p><p>对应的投影向量为：<span class="math inline">\(p = A \cdot(A^\mathrm{T}A)^{-1} \cdot A^\mathrm{T}b\)</span></p><p>对应的投影矩阵为：<span class="math inline">\(P = A \cdot(A^\mathrm{T}A)^{-1} \cdot A^\mathrm{T}\)</span></p><p>生成投影向量的线组系数：<span class="math inline">\(\hat{x} =(A^\mathrm{T}A)^{-1} \cdot A^\mathrm{T}b\)</span></p><p>类比上面的向量对向量投影的公式，是不是形式一样？只需把<spanclass="math inline">\(a\)</span>换为<spanclass="math inline">\(A\)</span>，分号改为逆就好啦？美妙的公式。</p><p>同理，此时的投影矩阵<spanclass="math inline">\(P\)</span>同样满足：<spanclass="math inline">\(P^\mathrm{T} = P\)</span>，<spanclass="math inline">\(P^2 = P\)</span></p><hr /><p>ok，让我们讨论剩下的一点细节，前面我们是把<spanclass="math inline">\(b\)</span>投影到<spanclass="math inline">\(C(A)\)</span>中。那如果我想把<spanclass="math inline">\(b\)</span>投到与<spanclass="math inline">\(C(A)\)</span>正交的空间，也就是<spanclass="math inline">\(N(A^\mathrm{T})\)</span>呢？显然，通过几何意义可以看出，答案就是<spanclass="math inline">\(e\)</span>，即<span class="math inline">\(b -p\)</span>。</p><p>所以投影到<spanclass="math inline">\(N(A^\mathrm{T})\)</span>就是：<spanclass="math inline">\(b - p = b - Pb = (I -P)b\)</span>。所以对应的投影矩阵就是<span class="math inline">\(I -P\)</span>。同样的，<spanclass="math inline">\(P\)</span>满足的性质，<spanclass="math inline">\(I - P\)</span>也都满足。</p><p>以及，前面提到，必须<spanclass="math inline">\(A^\mathrm{T}A\)</span>是可逆矩阵，才能解出<spanclass="math inline">\(\hat{x}\)</span>。但是，如何判断它可逆呢？有一个判断方法：若<spanclass="math inline">\(A\)</span>的各列线性无关，那么<spanclass="math inline">\(A^\mathrm{T}A\)</span>就是可逆矩阵。下面来证明一下：</p><blockquote><p>假设<span class="math inline">\(A\)</span>的各列线性无关。</p><p>考虑方程<span class="math inline">\(A^\mathrm{T}Ax = 0\)</span></p><p>两边同乘<span class="math inline">\(x^\mathrm{T}\)</span>，得：<spanclass="math inline">\(x^\mathrm{T}A^\mathrm{T}Ax = 0\)</span></p><p>整理：<span class="math inline">\((Ax)^\mathrm{T}(Ax)=0\)</span></p><p>所以说嘛向量<spanclass="math inline">\((Ax)\)</span>长度为0，也就是它为零向量，所以：<spanclass="math inline">\(Ax=0\)</span></p><p>因为<span class="math inline">\(A\)</span>的各列线性无关，所以<spanclass="math inline">\(dim(N(A)) = 0\)</span>，所以<spanclass="math inline">\(x\)</span>只能去零向量。</p><p>所以对于方程<span class="math inline">\(A^\mathrm{T}Ax =0\)</span>，<span class="math inline">\(x\)</span>只能取零向量。</p><p>而只有当<spanclass="math inline">\(A^\mathrm{T}A\)</span>可逆的时候，上面的方程才只有零解。证毕。</p></blockquote><hr /><p>最后，让我们举一个用投影来解决的实际例子：最小二乘。</p><p>平面上有n个点，要找到一条直线，尽可能的拟合这些点，怎么办？</p><p>假设有这么一条理想直线：y = kx + b，可以拟合所有点，那么就会有方程：<span class="math display">\[\begin{align*}    kx_1 + b &amp;= y_1 \\    kx_2 + b &amp;= y_2 \\    ... \\    kx_n + b &amp;= y_n\end{align*}\]</span> 我们要求k和b，所以把k和b看为未知数。</p><p>那么即可建模为<span class="math inline">\(Ax=b\)</span>，<spanclass="math inline">\(A = \begin{bmatrix} x_1 &amp; 1 \\ x_2 &amp; 1 \\... &amp; ... \\ x_n &amp; 1 \end{bmatrix}\)</span>，<spanclass="math inline">\(x = \begin{bmatrix} k \\ b\end{bmatrix}\)</span>，<span class="math inline">\(b = \begin{bmatrix}y_1 \\ y_2 \\ ... \\ y_n \end{bmatrix}\)</span></p><p>这个方程大概率是无解的，因为不存在这么一条完美直线，所以就把<spanclass="math inline">\(b\)</span>投影到<spanclass="math inline">\(C(A)\)</span>即可。</p><p>巧妙的建模，相信你再一次感受到线代的魅力了！</p><ul><li>总结一下：<ul><li>一维（向量b投影到向量a）：<ul><li><span class="math inline">\(P =\frac{aa^\mathrm{T}}{a^\mathrm{T}a}\)</span>，<spanclass="math inline">\(p = Pb\)</span></li></ul></li><li>高维（向量b投影到空间C(A)）：<ul><li><span class="math inline">\(P = A \cdot (A^\mathrm{T}A)^{-1} \cdotA^\mathrm{T}\)</span></li><li><span class="math inline">\(p = A \cdot (A^\mathrm{T}A)^{-1} \cdotA^\mathrm{T}b\)</span></li></ul></li><li>判断对称矩阵<spanclass="math inline">\(A^\mathrm{T}A\)</span>是否可逆的方法：看<spanclass="math inline">\(A\)</span>的各列是否线性无关</li></ul></li></ul><h3 id="三.-正交矩阵-schmidt正交化">三. 正交矩阵, Schmidt正交化</h3><p>在这一节里，我们用<spanclass="math inline">\(Q\)</span>来代表列向量全是正交且标准（长度为1）的矩阵。</p><p>显然，这个<span class="math inline">\(Q\)</span>满足下列定义：<spanclass="math inline">\(q_{i}^{T}q_{j}=\begin{cases}0&amp;,i\neqj\\1&amp;,i=j\end{cases}\)</span></p><p>讨论一下，<spanclass="math inline">\(Q^\mathrm{T}Q\)</span>是什么？<spanclass="math inline">\(Q = \begin{bmatrix} q_1, q_2, ..., q_n\end{bmatrix}\)</span>，<span class="math inline">\(Q^\mathrm{T} =\begin{bmatrix} q_1^\mathrm{T} \\ q_2^\mathrm{T} \\ ... \\q_n^\mathrm{T} \end{bmatrix}\)</span>。</p><p>显然，<span class="math inline">\(Q^\mathrm{T}Q = I\)</span>。</p><p>下面，我们对“正交矩阵”这个名词下一个定义：首先得是方阵，其次列向量都是互相正交且单位的。这样的矩阵就叫正交矩阵。</p><p>（注意，本节的<spanclass="math inline">\(Q\)</span>不一定是方阵，若无特殊说明，是不是方阵都有可能）</p><p>ok，让我们现在讨论一下正交矩阵的性质。</p><p>正交矩阵也满足<span class="math inline">\(Q^\mathrm{T}Q =I\)</span>，而且因为为方阵，所以<spanclass="math inline">\(Q\)</span>有逆矩阵。所以可以推出<spanclass="math inline">\(Q^\mathrm{T} = Q^{-1}\)</span>。</p><p>OK，让我们思考一下正交矩阵的投影矩阵：<span class="math inline">\(P =Q(Q^\mathrm{T}Q)^{-1}Q^\mathrm{T} =QQ^\mathrm{T}\)</span>。因为是正交矩阵，所以<spanclass="math inline">\(Q^\mathrm{T} = Q^{-1}\)</span>，所以<spanclass="math inline">\(P =I\)</span>，即正交矩阵的投影矩阵就是单位阵。如果<spanclass="math inline">\(Q\)</span>不是方阵的话，那么其投影矩阵就是<spanclass="math inline">\(QQ^\mathrm{T}\)</span>。当然了，它们的投影矩阵都是满足那俩性质的：</p><ol type="1"><li>是对称矩阵</li><li><span class="math inline">\(P^2 = P\)</span></li></ol><hr /><p>正交且标准的性质是很好的，所以我们如何把一个虽然列向量互相线性无关（但是不正交）的矩阵<spanclass="math inline">\(A\)</span>，转化为列向量互相正交且标准的矩阵<spanclass="math inline">\(Q\)</span>呢？下面来介绍格拉姆-施密特正交法：</p><p>思考现在有向量<span class="math inline">\(a, b,c\)</span>，它们互相线性无关，但是不正交，如果把它们转换为一组正交的呢？</p><p>首先把<spanclass="math inline">\(a\)</span>单位化然后锁死，然后对于<spanclass="math inline">\(b\)</span>，<spanclass="math inline">\(b\)</span>减去<spanclass="math inline">\(b\)</span>在<spanclass="math inline">\(a\)</span>上的投影（也就是误差向量<spanclass="math inline">\(e\)</span>）就是我们想要的，所以<spanclass="math inline">\(b&#39; = b - p = b - Pb = b -\frac{aa^\mathrm{T}}{a^\mathrm{T}a}\cdotb\)</span>，然后单位化，锁死。</p><p>然后对于<span class="math inline">\(c\)</span>，先把在<spanclass="math inline">\(a\)</span>上的投影减掉，再把在更新后的<spanclass="math inline">\(b\)</span>上的投影减掉，就得到了正交于<spanclass="math inline">\(a, b\)</span>的向量：<spanclass="math inline">\(c&#39; = c - p_a - p_b = c -\frac{aa^\mathrm{T}}{a^\mathrm{T}a} \cdot c -\frac{bb^\mathrm{T}}{b^\mathrm{T}b} \cdotc\)</span>，然后单位化，锁死。</p><p>其余的向量以此类推。</p><p>很好，现在你学会了如何将一组虽然线性无关但是不是正交的向量转化为正交的向量组，恭喜。</p><p>但是我要提个问题，转化前的矩阵<spanclass="math inline">\(A\)</span>和转化后的矩阵<spanclass="math inline">\(Q\)</span>，它们的列空间一样吗？</p><p>答案是肯定的，因为考虑我们转化的过程，例如<spanclass="math inline">\(c = c - p_a - p_b\)</span>，<spanclass="math inline">\(p_a\)</span>是<spanclass="math inline">\(a\)</span>的缩放，<spanclass="math inline">\(p_b\)</span>是<spanclass="math inline">\(b\)</span>的缩放，所以其实我们在正交化的过程中，仍然是用原向量组做线性组合（即列变换），所以转换后的矩阵，其列空间一样<spanclass="math inline">\(C(Q)\)</span>=<spanclass="math inline">\(C(A)\)</span>，零空间一样<spanclass="math inline">\(N(Q) = N(A)\)</span>。</p><hr /><p>截止，你已经知道了正交的概念，也认识到了正交矩阵以及一些性质。甚至你还学会了如何将一组线性无关向量组转化为标准正交的向量组。</p><p>但是，知道这些，有什么用呢？</p><p>目前我知道的是，可以讲一个各列线性无关的矩阵<spanclass="math inline">\(A\)</span>进行<spanclass="math inline">\(QR\)</span>分解：<span class="math inline">\(A =QR\)</span>。<spanclass="math inline">\(Q\)</span>表示各列互相标准正交的向量，<spanclass="math inline">\(R\)</span>是一个上三角矩阵。</p><p>为啥<spanclass="math inline">\(R\)</span>是一个上三角矩阵呢？我来证明一下：</p><p>在正交化的过程中，对于<spanclass="math inline">\(a_i\)</span>，它依赖了<spanclass="math inline">\(q_1, q_2, \cdots, q_{i-1},a_i\)</span>，线组出了<spanclass="math inline">\(q_i\)</span>。所以，如果我有<spanclass="math inline">\(q_1, q_2, \cdots,q_i\)</span>，那我就可线组出<spanclass="math inline">\(a_i\)</span>。所以对于<spanclass="math inline">\(A = QR\)</span>，<spanclass="math inline">\(R\)</span>就是线组的系数。而且可发现想线组出<spanclass="math inline">\(a_i\)</span>只需用到<span class="math inline">\(1\sim i\)</span>的<span class="math inline">\(q\)</span>，所以<spanclass="math inline">\(R\)</span>自然就是一个上三角的了。</p><h3 id="四.-行列式及其性质">四. 行列式及其性质</h3><p>行列式是方阵独属的浪漫，通常记为<spanclass="math inline">\(det(A)\)</span>或者<spanclass="math inline">\(|A|\)</span>。</p><p>行列式这里性质特别多，可能需要一些记忆：</p><ol type="1"><li><p>单位阵的行列式为1</p></li><li><p>交换两行，行列式符号会取反</p></li><li><p><span class="math inline">\(\begin{vmatrix}  ta &amp; tb \\  c&amp; d  \end{vmatrix} = t\begin{vmatrix}  a &amp; b \\  c &amp;d  \end{vmatrix}\)</span></p></li><li><p><span class="math inline">\(\begin{vmatrix}  a+a&#39; &amp;b+b&#39; \\  c &amp; d  \end{vmatrix} = \begin{vmatrix}  a &amp; b \\  c&amp; d  \end{vmatrix} + \begin{vmatrix}  a&#39; &amp; b&#39; \\  c&amp; d  \end{vmatrix}\)</span></p></li><li><p>若有两行相等，则行列式为0</p></li><li><p>行j减去行i的k倍，行列式不变</p></li><li><p>如果有一行为0，那么行列式为0</p></li><li><p>对于上三角方阵，其行列式为对角线元素乘积</p></li><li><p><span class="math inline">\(det(A)=0 \iff\)</span> 矩阵<spanclass="math inline">\(A\)</span>是奇异矩阵（奇异矩阵就是不满秩的方阵）</p></li><li><p><span class="math inline">\(det(AB) = det(A) \cdotdet(B)\)</span></p></li><li><p><span class="math inline">\(det(A^\mathrm{T}) =det(A)\)</span></p><ul><li>有了这条性质，那么上面描述行的性质，同样可以描述列，例如：</li><li>交换两列，行列式符号取反</li><li>除了行有线性关系（性质3、4），列也具有</li><li>若有两列相等，则行列式为0</li><li>如果有一列为0，那么行列式为0</li></ul></li></ol><p>所以如何求一个方阵的行列式？通常就是将其消元成上三角矩阵（注意过程中行交换会导致行列式符号取反），然后对角线相乘即可。</p><hr /><p>行列式公式1：<span class="math inline">\(det(A) = \sum (-1)^{r(k_1,k_2, k_3, ..., k_n)}a_{1k_1}a_{2k_2}a_{3k_3}...a_{nk_n}\)</span></p><p>​ <span class="math inline">\(r(k_1, k_2, ...,k_n)\)</span>是排列的逆序数。</p><p>代数余子式：位置(i, j)的代数余子式<span class="math inline">\(A_{ij}:= (-1)^{i+j}det(\text{去掉第i行和第j列得到的矩阵})\)</span></p><p>行列式公式2：<span class="math inline">\(det(A) = a_{11}C_{11} +a_{12}C_{12} + \cdots + a_{1n}C_{1n}\)</span></p><h3 id="五.-克拉默法则-体积">五. 克拉默法则, 体积</h3><p>设<span class="math inline">\(A_{ij}\)</span>是位置(i,j)的代数余子式，则矩阵<spanclass="math inline">\(A\)</span>的伴随矩阵定义如下： <spanclass="math display">\[A^* = \begin{bmatrix}A_{11} &amp; A_{21} &amp; \cdots &amp; A_{n1} \\A_{12} &amp; A_{22} &amp; \cdots &amp; A_{n2} \\\cdots \\A_{1n} &amp; A_{2n} &amp; \cdots &amp; A_{nn}\end{bmatrix}\]</span> 它满足一个公式，通过代数法求逆：<spanclass="math inline">\(A^{-1} = \frac{A^*}{det(A)}\)</span></p><p>那么对于非奇异矩阵<spanclass="math inline">\(A\)</span>，它的方程：<spanclass="math inline">\(Ax=b\)</span>就有一种新的解法。</p><p><span class="math inline">\(x = A^{-1}b =\frac{A^*b}{det(A)}\)</span></p><p>所以可得到以下式子： <span class="math display">\[\begin{align*}    x_1 &amp;= \frac{det(\text{把A的第一列换为b})}{det(A)} \\    x_2 &amp;= \frac{det(\text{把A的第二列换为b})}{det(A)} \\    \cdots \\    x_n &amp;= \frac{det(\text{把A的第n列换为b})}{det(A)}\end{align*}\]</span>上面就是克拉默法则，用代数的方程解方程。但是我觉得，中看不中用，不如直接用矩阵的方程去解方程组。</p><hr /><p>给出一个有意思的定理：行列式的绝对值其实是在计算“箱子”的体积。</p><p>我举个例子，比如有矩阵<span class="math inline">\(A = \begin{bmatrix}a_{11} &amp; a_{12} &amp; a_{13} \\ a_{21} &amp; a_{22} &amp; a_{23} \\a_{31} &amp; a_{32} &amp; a_{33} \end{bmatrix}\)</span></p><p>那么<span class="math inline">\(|det(A)|\)</span>等于以<spanclass="math inline">\((a_{11}, a_{12}, a_{13})\)</span>，<spanclass="math inline">\((a_{21}, a_{22}, a_{23})\)</span>、<spanclass="math inline">\((a_{31}, a_{32},a_{33})\)</span>为三边所形成的箱子的体积。</p><p>同理，对于<span class="math inline">\(n \timesn\)</span>的矩阵同样成立。</p><h3 id="六.-特征值-特征向量">六. 特征值, 特征向量</h3><p>特征值是方阵独属的浪漫。</p><p>首先来讲特征向量，矩阵<spanclass="math inline">\(A\)</span>的特征向量就是那些进过<spanclass="math inline">\(A\)</span>线性变换后方向不改变的向量。用数学语言表达就是：<spanclass="math inline">\(Ax = \lambda x\)</span>。满足上述方程的<spanclass="math inline">\(x\)</span>就是特征向量，<spanclass="math inline">\(\lambda\)</span>就是特征值。</p><p>对于特征值0，它对应的特征向量应满足<spanclass="math inline">\(Ax=0\)</span>，所以特征值0所对应的特征向量其实就是<spanclass="math inline">\(N(A)\)</span>。</p><p>举点例子吧，考虑投影矩阵<spanclass="math inline">\(P\)</span>。对于那些本身就已经在投影面上的向量<spanclass="math inline">\(x\)</span>，满足<span class="math inline">\(Px =x\)</span>，所以<spanclass="math inline">\(P\)</span>特征值为1的特征向量就是投影面上的向量。对于那些垂直于投影面的向量<spanclass="math inline">\(x\)</span>，满足<span class="math inline">\(Px =0\)</span>，所以<spanclass="math inline">\(P\)</span>特征值为0的特征向量就是垂直于投影面的那些向量。</p><p>ok，我提前透露几个特征值的性质：</p><ol type="1"><li>n阶方阵有n个特征值</li><li>n个特征值的和加起来等于方阵对角线之和</li><li>n个特征值的乘积等于方阵的特征值</li><li>上三角方阵的特征值就是对角线上的元素</li><li><span class="math inline">\(A\)</span>的特征值等于<spanclass="math inline">\(A^\mathrm{T}\)</span>的特征值</li><li><span class="math inline">\(A^{-1}\)</span>的特征值等于<spanclass="math inline">\(A\)</span>的特征值取倒数</li></ol><hr /><p>好，那给你一个矩阵<spanclass="math inline">\(A\)</span>，如何求出其特征值和对应的特征向量呢？</p><p>很简单，首先列出定义：<span class="math inline">\(Ax = \lambdax\)</span>，移项：<span class="math inline">\((A - \lambda I)x =0\)</span>。要使这个方程有非零解，<span class="math inline">\((A -\lambda I)\)</span>要是奇异矩阵，也就是不满秩，也就是<spanclass="math inline">\(|A - \lambda I| = 0\)</span>。</p><p>解上面那个行列式，即可求出所有的特征值<spanclass="math inline">\(\lambda\)</span>（n个特征值可能有重复）。</p><p>然后反代回去，即可求出特征值对应的特征向量。</p><h3 id="七.-对角化-a的幂">七. 对角化, A的幂</h3><p>上一节我们学会了如何求一个矩阵<spanclass="math inline">\(A\)</span>的特征值和对应的特征向量。这一节我们来利用特征向量来分解矩阵。</p><p>假设我们有一个矩阵<spanclass="math inline">\(A\)</span>，它有n个线性无关的特征向量。那么我把这些向量排成一排得到矩阵<spanclass="math inline">\(S\)</span>，叫做特征向量矩阵。然后推导下面式子：</p><p><span class="math inline">\(AS = A \cdot \begin{bmatrix} \beta_1&amp; \beta_2 \cdots \beta_n \end{bmatrix} = \begin{bmatrix}\lambda_1\beta_1 &amp; \lambda_2\beta_2 &amp; \cdots &amp;\lambda_n\beta_n \end{bmatrix} = \begin{bmatrix} \beta_1 &amp; \beta_2&amp; \cdots &amp; \beta_n \end{bmatrix}\begin{bmatrix} \lambda_1 &amp;&amp; &amp; &amp; \\ &amp; \lambda_2 &amp; &amp; &amp; &amp; \\ &amp;&amp; \lambda_3 &amp; &amp; \\ &amp; &amp; &amp; &amp; \cdots\end{bmatrix}\)</span></p><p>即：<span class="math inline">\(AS =S\Lambda\)</span>，<spanclass="math inline">\(\Lambda\)</span>为用特征值生成的对角阵，也叫特征值矩阵</p><p>进一步化简，得到： <span class="math display">\[\Lambda = S^{-1}AS \\A = S\Lambda S^{-1}\]</span>这种对角化分解有什么用呢？答案：在处理矩阵的幂的时候非常有用</p><p>首先来看一下<span class="math inline">\(A^2\)</span>：<spanclass="math inline">\(A^2 = S\Lambda S^{-1} \cdot S\Lambda S^{-1} =S\Lambda^2 S^{-1}\)</span></p><p>可以发现，<span class="math inline">\(A^2\)</span>的特征值就是<spanclass="math inline">\(A\)</span>特征值的平方，<spanclass="math inline">\(A^2\)</span>的特征向量与<spanclass="math inline">\(A\)</span>一样。</p><p>同理，<span class="math inline">\(A^k\)</span>的特征值就是<spanclass="math inline">\(A\)</span>特征值的<spanclass="math inline">\(k\)</span>次方，<spanclass="math inline">\(A^k\)</span>的特征向量与<spanclass="math inline">\(A\)</span>一样。</p><p>来个好玩的问题，当<spanclass="math inline">\(A\)</span>的特征值满足什么条件时，<spanclass="math inline">\(A^k = O, k \to \infty\)</span>？</p><p>通过上面的公式，可得<span class="math inline">\(A^k =S\Lambda^kS^{-1}\)</span>，显然<spanclass="math inline">\(S\)</span>是固定的，所以关键就是看<spanclass="math inline">\(\Lambda\)</span>。很容易想到，如果<spanclass="math inline">\(A\)</span>的所有特征值满足<spanclass="math inline">\(|\lambda_i| &lt; 1\)</span>的话，那么矩阵<spanclass="math inline">\(A\)</span>会收敛到零矩阵。</p><p>（<strong>上面对角化分解非常有用，但需要注意分解的前提是<spanclass="math inline">\(A\)</span>有n个线性无关的特征向量。如何判断呢？这里给出一个定理：如果<spanclass="math inline">\(A\)</span>有n个互不相同的特征值，那么<spanclass="math inline">\(A\)</span>就有n个线性无关的特征向量；否则则不一定。</strong>）</p><hr /><p>先介绍一阶差分方程，即：<span class="math inline">\(u_{k+1} =Au_k\)</span>。（我们考虑理想的情况，即认为<spanclass="math inline">\(A\)</span>有n个互不相同的特征值。）</p><p>那么可推出：<span class="math inline">\(u_k = A^ku_0\)</span></p><p>因为<spanclass="math inline">\(A\)</span>的各特征向量线性无关，所以<spanclass="math inline">\(\mathbb{R}^n\)</span>可用特征向量线组出来，<spanclass="math inline">\(u_0\)</span>同样可以用特征向量线组出来，设线组的系数为<spanclass="math inline">\(c\)</span>列向量，那么<spanclass="math inline">\(u_0\)</span>可表示为<spanclass="math inline">\(Sc\)</span>。</p><p>所以<span class="math inline">\(u_k = S\Lambda^kS^{-1} \cdot Sc =S\Lambda^kc\)</span>。</p><p>下面我们来一道经典的例题，现有斐波拉契数列：0, 1, 1, 2, 3, 5, 8, ......。试用矩阵求斐波拉契数列，并分析其增长速度。</p><p>首先可写出递推式：<span class="math inline">\(F_{k+2} = F_{k+1} +F_k\)</span>。可以发现这是一个二阶的差分方程，我们想把其转为前面的知识转为一阶的，所以我使用一个trick，就是再加入一个方程，然后引入新变量去表达方程组，使其变为一阶差分方程，具体如下：<span class="math display">\[\begin{cases}F_{k+2} = F_{k+1} + F_k \\F_{k+1} = F_{k+1}\end{cases}\]</span> 设<span class="math inline">\(u_k = \begin{bmatrix}F_{k+1} \\F_k\end{bmatrix}\)</span>，所以上述方程组可表达为一个一阶差分方程：<spanclass="math inline">\(u_{k+1} = \begin{bmatrix}1 &amp; 1 \\ 1 &amp;0\end{bmatrix}u_k\)</span>。</p><p>好，先来看看矩阵<span class="math inline">\(A = \begin{bmatrix}1&amp; 1 \\ 1 &amp; 0\end{bmatrix}\)</span>是否可对角化，有<spanclass="math inline">\(\begin{cases} \lambda_1 + \lambda_2 = 1 \\\lambda_1 \cdot \lambda_2 = -1 \end{cases}\)</span>，解得：<spanclass="math inline">\(\begin{cases} \lambda_1 = \frac12(1 + \sqrt{5})\approx 1.618 \\ \lambda_2 = \frac12(1 - \sqrt{5}) \approx -0.618\end{cases}\)</span></p><p>有俩不同特征值，由前面的判定定理可知，<spanclass="math inline">\(A\)</span>可对角化，那么可求出其特征向量矩阵<spanclass="math inline">\(S = \begin{bmatrix} \lambda_1 &amp; \lambda_2 \\ 1&amp; 1 \end{bmatrix}\)</span>。</p><p>由前面的公式可知：<span class="math inline">\(u_k =S\Lambda^kc\)</span>。在这里，<spanclass="math inline">\(c\)</span>是用特征向量表示出<spanclass="math inline">\(\begin{bmatrix} F_1 \\ F_0\end{bmatrix}\)</span>的系数列向量。让我求一下：<spanclass="math inline">\(u_0 = \begin{bmatrix} 1 \\ 0 \end{bmatrix} =c_1\begin{bmatrix} \lambda_1 \\ 1 \end{bmatrix} + c_2\begin{bmatrix}\lambda_2 \\ 1 \end{bmatrix}\)</span>，这里就不解了。</p><p>所以现在<span class="math inline">\(S, \Lambda,c\)</span>都有了，那么<spanclass="math inline">\(u_k\)</span>就可求出来了，即<spanclass="math inline">\(F_k\)</span>就可求出来了。</p><p>但是这个一阶差分方程的增长速度我们还没分析，其实观察<spanclass="math inline">\(u_k =S\Lambda^kc\)</span>就可知道，增长速度由特征值决定，若<spanclass="math inline">\(|\lambda_i| &lt; 1\)</span>，那么<spanclass="math inline">\(u_k\)</span>直接会收俩到0。</p><p>对于那些<span class="math inline">\(|\lambda_i| &gt;1\)</span>的，越大的<spanclass="math inline">\(|\lambda_i|\)</span>，只要对应的<spanclass="math inline">\(c_i \ne0\)</span>，那么对应的特征向量增长速度就越快。</p><p>这一节的内容稍微有点点难消化，不过这已经有点点科研证明的味道了。多看多理解。</p><hr /><h3 id="八.-微分方程-expat">八. 微分方程, exp(At)</h3><p>在开始这节课之前，我觉得有必要补充介绍一点微分方程的概念。</p><p>定义：含自变量（例如<spanclass="math inline">\(x\)</span>）、函数（例如<spanclass="math inline">\(y\)</span>）以及函数各阶导数（例如<spanclass="math inline">\(\dot{y},\ddot{y}\)</span>）的等式称为微分方程。</p><p>抱歉，真听不懂，是我高数太垃圾了，回头补完微分方程再来听这节课。</p><h3 id="九.-马尔可夫矩阵-傅里叶级数">九. 马尔可夫矩阵, 傅里叶级数</h3><p>什么是马尔可夫矩阵，若<spanclass="math inline">\(A\)</span>满足以下两条定义，则它是马尔可夫矩阵：</p><ol type="1"><li>所有元素大于0（概率值不能为负数）</li><li>每一列元素和为1</li><li>方阵</li></ol><p>对于马尔可夫矩阵，很容易发现，其幂即<spanclass="math inline">\(A^k\)</span>同样也是马尔可夫矩阵。</p><p>这里我直接给出两个结论：</p><ol type="1"><li>马尔可夫矩阵有一个特征值为1</li><li>马尔可夫矩阵其余特征值绝对值小于等于1</li></ol><p>根据上面的结果，我们可以知道，如果一个向量一直右边马尔可夫矩阵，那么最终会达到一个稳态。显然这个稳态是我们关心的，我们需要找到它。</p><p>假设矩阵<spanclass="math inline">\(A\)</span>有n个线性无关的特征向量<spanclass="math inline">\(\beta\)</span>-s，那么对于式子：<spanclass="math inline">\(u_k = A^ku_0\)</span></p><p><span class="math inline">\(u_0\)</span>可以被表示为：<spanclass="math inline">\(u_0 = c_1\beta_1 + c_2\beta_2 + \cdots +c_n\beta_n = Sc\)</span></p><p><span class="math inline">\(A^k\)</span>根据前面所学可对角化为：<spanclass="math inline">\(A_k = S\Lambda^kS^{-1}\)</span></p><p><span class="math inline">\(\therefore u_k = S\Lambda^kS^{-1}Sc =S\Lambda^kc = c_1\lambda_1\beta_1 + c_2\lambda_2\beta_2 + \cdots +c_n\lambda_n\beta_n\)</span></p><p>所以那些绝对值&lt;1的项最终会迭代没，绝对值为1的项加起来就是稳态。</p><hr /><p>傅里叶级数是一个可用来拟合任意周期函数的工具，例如我想拟合一个周期为<spanclass="math inline">\(2\pi\)</span>的函数<spanclass="math inline">\(f(x)\)</span>，可用傅立叶级数表达为下列形式：</p><p><span class="math inline">\(f(x) = a_0\cdot 1 + a_1\cos x + b_1\sin x+ a_2\cos 2x + b_2\sin 2x + \cdots + a_n\cos nx+ b_n\sin nx\)</span></p><p>所以关键就是确定下系数<span class="math inline">\(a_i,b_i\)</span>，那我们用线性代数来看待这个问题。</p><p>可以把<span class="math inline">\(1, \cos x, \sin x, \cos 2x, \sin2x, \cdots, \cos nx, \sin nx\)</span>看作基<spanclass="math inline">\(\beta_i\)</span>-s，<spanclass="math inline">\(f(x)\)</span>看作<spanclass="math inline">\(b\)</span>，那么<span class="math inline">\(a_i,b_i\)</span>就是线性组合的系数。</p><p>让我写成这种形式：<span class="math inline">\(c_0\beta_0 + c_1\beta_1+ c_2\beta_2 + \cdots + c_n\beta_n = b\)</span></p><p>问题即为求出<span class="math inline">\(c_i\)</span>。</p><p>假设<spanclass="math inline">\(\beta_i\)</span>-s们正交就好了，我们来检查一下是否正交。</p><p>因为这里的“向量”是函数，所以离散型的点积在这里并不是适用，对于函数<spanclass="math inline">\(f(x), g(x)\)</span>，其实点积是<spanclass="math inline">\(\int_a^b f(x)g(x)\mathrm{d}x\)</span>，因为本题函数周期为<spanclass="math inline">\(2\pi\)</span>，所以俩函数点积为：<spanclass="math inline">\(\int_0^{2\pi}f(x)g(x)\mathrm{d}x\)</span>，检查一下发现这些“基”们确实是正交的。</p><p>那么求系数<spanclass="math inline">\(c_i\)</span>就好办了，比如我要求<spanclass="math inline">\(c_1\)</span>，那么等式两边分别“点积”<spanclass="math inline">\(\beta_1\)</span>，得：</p><p><span class="math inline">\(c_1 \int_0^{2\pi}(cosx)^2\mathrm{d}x =\int_0^{2\pi}f(x)\cos x\mathrm{d}x\)</span></p><p>因为正交性，非<spanclass="math inline">\(\beta_1\)</span>的项都为0了，所以解这个方程即可把<spanclass="math inline">\(c_1\)</span>求出来，其余系数求法同理。</p><p>非常巧妙优美的做法。</p><h3 id="十.-复习课二">十. 复习课二</h3><p>主要因为这章的知识比较重要，所以适合来2道例题巩固一下。而且我想通过例题顺便补充下代数重数和几何重数的知识点。</p><p><strong>例1.</strong> <span class="math inline">\(a = \begin{bmatrix}2 \\ 1 \\ 2 \end{bmatrix}\)</span></p><ol type="1"><li>求投影到<span class="math inline">\(a\)</span>的投影矩阵<spanclass="math inline">\(P\)</span></li></ol><p>套公式：<span class="math inline">\(P =\frac{aa^\mathrm{T}}{a^\mathrm{T}a} = \frac{1}{9} \begin{bmatrix} 4&amp; 2 &amp; 4 \\ 2 &amp; 1 &amp; 2 \\ 4 &amp; 2 &amp; 4\end{bmatrix}\)</span></p><ol start="2" type="1"><li>求<span class="math inline">\(P\)</span>的秩</li></ol><p>因为<span class="math inline">\(P\)</span>的列空间<spanclass="math inline">\(C(P)\)</span>是投影面，而投影面又是三维空间里的一维直线，所以<spanclass="math inline">\(dim(C(A)) = r = 1\)</span>。</p><ol start="3" type="1"><li>求<span class="math inline">\(P\)</span>的特征值</li></ol><p>因为<span class="math inline">\(r(P) =1\)</span>，所以它是奇异，所以<span class="math inline">\(det(P) =0\)</span>，所以它必有一个特征值为0。</p><p>我们知道特征值为0对应的特征向量就是<spanclass="math inline">\(N(P)\)</span>里的那些基们，而我们知道<spanclass="math inline">\(dim(N(P)) = n - r =2\)</span>，所以基里有俩向量，所以特征值0的几何重数为2，又因为几何重数&lt;= 代数重数，所以至少有两个特征值为0。</p><p><spanclass="math inline">\(P\)</span>的迹又是1，所以不可能三个特征值都为0，所以可确定特征值0的代数重数也为2。所以特征值分别为0、0、1</p><blockquote><p>知识点补充：代数重数、几何重数</p><p>代数重数就是某特征值重复的个数</p><p>几何重数就是某特征值对应的互相线性无关的特征向量的个数。这些线性无关的特征向量组合的空间叫做特征子空间</p><p>性质：几何重数 &lt;= 代数重数</p></blockquote><ol start="4" type="1"><li><spanclass="math inline">\(P\)</span>特征值为1对应的特征向量是啥</li></ol><p>其实就是问你<span class="math inline">\(Px=x\)</span>的<spanclass="math inline">\(x\)</span>都有谁，根据几何意义，显然<spanclass="math inline">\(x\)</span>就在投影面上啊，即那条线，所以特征向量写<spanclass="math inline">\(a\)</span>就行了</p><p><strong>例2.</strong> 已知一个4阶方阵<spanclass="math inline">\(A\)</span>具有特征值<spanclass="math inline">\(\lambda_1, \lambda_2, \lambda_3,\lambda_4\)</span></p><ol type="1"><li>特征值需要满足什么条件才能保证<spanclass="math inline">\(A\)</span>为可逆矩阵</li></ol><p>可逆矩阵说明<span class="math inline">\(r(A) =n\)</span>，那么零空间的维数就为0。而如果有特征值为0，那么必然说明零空间有非零向量，即<spanclass="math inline">\(r(A) \nen\)</span>。所以必须满足所有特征值不为0，才能保证<spanclass="math inline">\(A\)</span>为可逆矩阵</p><ol start="2" type="1"><li>求<span class="math inline">\(A^{-1}\)</span>的行列式</li></ol><p>因为<span class="math inline">\(A^{-1}\)</span>的特征值是<spanclass="math inline">\(A\)</span>特征值取倒数，行列式又是特征值之积，所以<spanclass="math inline">\(det(A^{-1}) =\frac{1}{\lambda_1\lambda_2\lambda_3\lambda_4}\)</span></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;正交向量/空间、对向量/空间投影、正交矩阵、正交化、行列式、特征值/特征向量、(特征值)对角化、马尔可夫矩阵/傅里叶级数&lt;/p&gt;</summary>
    
    
    
    <category term="4. 大学" scheme="http://error666.top/categories/4-%E5%A4%A7%E5%AD%A6/"/>
    
    <category term="数学" scheme="http://error666.top/categories/4-%E5%A4%A7%E5%AD%A6/%E6%95%B0%E5%AD%A6/"/>
    
    <category term="线性代数" scheme="http://error666.top/categories/4-%E5%A4%A7%E5%AD%A6/%E6%95%B0%E5%AD%A6/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>线性代数1</title>
    <link href="http://error666.top/2024/09/11/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B01/"/>
    <id>http://error666.top/2024/09/11/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B01/</id>
    <published>2024-09-11T10:58:18.000Z</published>
    <updated>2024-09-17T15:27:41.582Z</updated>
    
    <content type="html"><![CDATA[<p>方程组、矩阵、消元、向量空间、秩、解方程</p><p>还差P23、P27没学，等学完微分方程后回来看。</p><span id="more"></span><hr /><p>课程是<ahref="https://www.bilibili.com/video/BV16Z4y1U7oU/?spm_id_from=333.337.search-card.all.click&amp;vd_source=ca9a71bb3c1806ce48ae27d95e4e8bd0">链接</a></p><p>Gilbert Strang老爷子的MIT 18.06 LinearAlgebra。非常经典的一门课程，是理解性讲课而不是像国内多数高校式的应试性讲课。</p><p>线性代数作为诸多工程应用的基石，其重要性毋庸置疑。所以对于这份笔记，我的看法是常看常新，多看不亏，越多看，越能把全部知识串联起来。</p><p>在铺垫好线性代数的基石后，最好修一门常微分方程(mit18.03)。学有余力的话，把单变量微积分mit 18.01和多变量微积分mit18.02修了。如果想提高应用数学视野的可以去修GilbertStrang的计算科学与工程mit 18.085+18.086。</p><h3 id="一.-方程组的几何解释">一. 方程组的几何解释</h3><ul><li>考虑一个一元二次方程组，我们的几何理解就是一个方程是一条直线。这样的理解是初高中理解</li><li>但是到了大学，我们应该竖着去看方程组，也就是x乘一个列向量，y乘一个列向量，加起来，得到一个列向量，也就是这种形式：</li><li><span class="math inline">\(\begin{bmatrix}2 \\ -1\end{bmatrix}x +\begin{bmatrix}-1 \\ 2\end{bmatrix}y = \begin{bmatrix}0 \\3\end{bmatrix}\)</span></li><li>上面这种“竖着理解方程组的方式”，叫做“线性组合”，是贯穿这门课的一个思想</li></ul><hr /><ul><li><p>考虑线性组合的几何意义，假设有m个未知数，n个方程。</p></li><li><p>那么就有m个列向量。首先把这m个维度为<spanclass="math inline">\(\mathbb{R}^{n}\)</span>的列向量画到<spanclass="math inline">\(\mathbb{R}^n\)</span>上。</p></li><li><p>然后m个未知数就是这m个向量的系数，去线性组合这些向量，得到答案向量。</p></li><li><p>是不是很巧妙？在2维以上的空间内，用向量的线性组合去考虑问题会大大的简化问题。</p></li></ul><hr /><ul><li><p>为了不每次都写出上面那个<span class="math inline">\(x \cdot [2,-1]^\mathrm{T} + y \cdot [-1, 2]^\mathrm{T} = [0,3]\)</span>这种线性组合，太麻烦了，所以我们引入了矩阵来简化表达</p></li><li><p>具体来说，一个m个未知数，n个方程的方程组，用系数矩阵<spanclass="math inline">\(A\)</span>、未知数向量<spanclass="math inline">\(\textbf{x}\)</span>、答案向量<spanclass="math inline">\(b\)</span>来描述这个方程组</p></li><li><p>当你看到一个形如<span class="math inline">\(A\textbf{x} =b\)</span>的方程的时候，你要明白，本质就是m个维度为<spanclass="math inline">\(\mathbb{R}^n\)</span>的向量的线性组合 =常向量的求解问题</p></li></ul><hr /><ul><li><p>现在我们思考一个问题：n个方程，m个未知数的方程组是否永远有解？</p></li><li><p>用线性组合的观点就是，m个维度为<spanclass="math inline">\(\mathbb{R}^n\)</span>的向量的线性组合是否可以覆盖整个<spanclass="math inline">\(\mathbb{R}^n\)</span>空间？</p></li><li><p>可以发现，问题的关键，就是在这m个向量身上，换句话说，也就是在系数矩阵<spanclass="math inline">\(A\)</span>身上。这m个向量具有什么样的特点 / <spanclass="math inline">\(A\)</span>具有什么样的特点时，方程组会有解？会有几个解？这就是以后会讨论到的问题。</p></li></ul><hr /><ul><li>相信看到这，已经能感受到线性代数的绝妙吸引力了。它能带你在高维空间里遨游，让你熟练的玩弄高维空间。</li><li>数学真神奇，不是吗？</li></ul><h3 id="二.-矩阵消元">二. 矩阵消元</h3><ul><li><p>这节学习的是用消元法解方程组，计算机解方程都是用这种方法</p></li><li><p>首先先按国内大部分高校的讲法讲一遍：</p></li><li><p>消元就是初高中学的那个消元，消元前后矩阵是等价的，对系数矩阵进行求上三角过程</p></li><li><p>其实求上三角的过程，就是在依次确定基向量，基向量的意思就是能对解空间产生贡献的向量。假设向量俩俩正交，那么它们都是基向量。</p></li><li><p>对于n个方程，m个未知数的方程。若通过求上三角后，有k个主元（主元就是每列最后一个非零元素），说明有k个维度为<spanclass="math inline">\(\mathbb{R}^n\)</span>的基向量，那么若<spanclass="math inline">\(k \gen\)</span>，则方程组必定有解，因为此时m个向量可以线性组合出整个<spanclass="math inline">\(\mathbb{R}^n\)</span>空间。反之，则不一定有解。</p></li><li><p>换句话说，有几个主元，方程组的向量们就能线性组合出几维的空间</p></li></ul><hr /><ul><li><p>回到正题，如何求解方程组呢？（考虑一定有解的情况）</p></li><li><p>首先把答案向量<span class="math inline">\(b\)</span>加入到<spanclass="math inline">\(A\)</span>中作为新的一列，此时称<spanclass="math inline">\(A\)</span>为增光矩阵<spanclass="math inline">\(\overline{A}\)</span></p></li><li><p>对<spanclass="math inline">\(\overline{A}\)</span>消元求上三角，然后将消元后的矩阵重新写成方程组去算就行了</p></li></ul><hr /><ul><li><p>好了，现在用我在mit学到的讲法讲一遍：</p></li><li><p>上面的讲法中，对<spanclass="math inline">\(\overline{A}\)</span>消元求上三角的过程，我们的视角还是用初高中的做法去做的，但现在，仍然是消元求上三角的过程，我想用矩阵去做</p></li><li><p>在做之前，我想介绍“行的线性组合“</p></li></ul><hr /><ul><li><p>我们之前讲了，对于方程<spanclass="math inline">\(A\textbf{x}=b\)</span>，我们的理解方式就是看成m个列向量的线性组合，这其实是“列的线性组合“</p></li><li><p>现在我们来看这个方程：<spanclass="math inline">\(\textbf{x}^{\mathrm{T}}A =b^{\mathrm{T}}\)</span>，<spanclass="math inline">\(\textbf{x}^{\mathrm{T}}\)</span>是一个有n个未知数的行向量，<spanclass="math inline">\(A\)</span>仍然是一个<spanclass="math inline">\(\mathbb{R}^{n \times m}\)</span>的矩阵，<spanclass="math inline">\(b^{\mathrm{T}}\)</span>是一个常行向量。</p></li><li><p>此时我们需要把这个方程理解为“行的线性组合“，也就是<spanclass="math inline">\(A\)</span>的每一行就是一个向量，然后这些向量线性组合，系数就是<spanclass="math inline">\(x^{\mathrm{T}}\)</span>里的分量。</p></li></ul><hr /><ul><li>ok，回到对矩阵的消元。</li></ul><p>假设有一个矩阵：</p><p><span class="math display">\[\left[\begin{array}{c}1 &amp; 2 &amp; 1 \\3 &amp; 8 &amp; 1 \\0 &amp; 4 &amp; 1\end{array}\right]\]</span></p><p>首先我想用第一行把(2,1)消掉，那么第一行是不变的，第三行是不变的，第二行应该变为<spanclass="math inline">\([0, 2,-2]\)</span>，也就是第二行加上负三倍的第一行。首先第一行是不变的，利用“行的线性组合”思想，我们可以对<spanclass="math inline">\(A\)</span>左乘一个行向量：</p><p><span class="math display">\[[1, 0, 0]\left[\begin{array}{c}1 &amp; 2 &amp; 1 \\3 &amp; 8 &amp; 1 \\0 &amp; 4 &amp; 1\end{array}\right]=[1, 2, 1]\]</span></p><p>然后第三行也是不变的，所以我们继续左乘：</p><p><span class="math display">\[\left[\begin{array}{c}1 &amp; 0 &amp; 0 \\? &amp; ? &amp; ? \\0 &amp; 0 &amp; 1\end{array}\right]\left[\begin{array}{c}1 &amp; 2 &amp; 1 \\3 &amp; 8 &amp; 1 \\0 &amp; 4 &amp; 1\end{array}\right]=\left[\begin{array}{c}1 &amp; 2 &amp; 1 \\? &amp; ? &amp; ? \\0 &amp; 4 &amp; 1\end{array}\right]\]</span></p><p>显然对于结果矩阵的第二行，我们是想让<spanclass="math inline">\(A\)</span>的原第二行加上三倍负第一行的，所以线性组合就是(-3)* 第一行 + (1) * 第二行 + (0) *第三行，所以把系数填进左乘的矩阵，即可得到：</p><p><span class="math display">\[\left[\begin{array}{c}1 &amp; 0 &amp; 0 \\-3 &amp; 1 &amp; 0 \\0 &amp; 0 &amp; 1\end{array}\right]\left[\begin{array}{c}1 &amp; 2 &amp; 1 \\3 &amp; 8 &amp; 1 \\0 &amp; 4 &amp; 1\end{array}\right]=\left[\begin{array}{c}1 &amp; 2 &amp; 1 \\0 &amp; 2 &amp; -2 \\0 &amp; 4 &amp; 1\end{array}\right]\]</span></p><p>这里我们把左乘的这个矩阵记为<spanclass="math inline">\(E_{21}\)</span>（因为是想对(2,1)这个位置进行消除）。这种矩阵叫做初等矩阵</p><p>下一步做法依次类推：</p><p><span class="math display">\[\left[\begin{array}{c}1 &amp; 0 &amp; 0 \\0 &amp; 1 &amp; 0 \\0 &amp; -2 &amp; 1\end{array}\right]\left[\begin{array}{c}1 &amp; 2 &amp; 1 \\0 &amp; 2 &amp; -2 \\0 &amp; 4 &amp; 1\end{array}\right]=\left[\begin{array}{c}1 &amp; 2 &amp; 1 \\0 &amp; 2 &amp; -2 \\0 &amp; 0 &amp; 5\end{array}\right]\]</span></p><p>同样的，我们把左乘的这个矩阵叫做<spanclass="math inline">\(E_{32}\)</span>，因为是对位置(3, 2)消除</p><p>以上，就是用矩阵去描述消元的全过程。最后，用矩阵来做个大总结的话，就是：</p><p><span class="math inline">\(E_{32}E_{21}A = U\)</span></p><hr /><ul><li>这里再跑点题，多讲一下“初等矩阵”这个概念</li><li>前面的<span class="math inline">\(E_{21},E_{32}\)</span>本质上，就是对矩阵<spanclass="math inline">\(A\)</span>做了一次"操作"，也就是某行减掉了另一行的几倍。</li><li>所以我们把能"操作"矩阵的矩阵称为初等矩阵</li><li>除了某行减掉了另一行几倍，当然还有别的操作，例如交换两行。</li><li>很容易啊，一样用"行的线性组合"思想，假设要交换第一第二行，那么初等矩阵的第一行就是[0,1, ..., 0]，第二行就是[1, 0, ..., 0]</li><li>对于能交换矩阵行和列的矩阵，也是一种初等矩阵，我们记为<spanclass="math inline">\(P\)</span>（置换矩阵）</li></ul><h3 id="三.-矩阵乘法和逆矩阵">三. 矩阵乘法和逆矩阵</h3><ul><li><p>假设矩阵<span class="math inline">\(A\)</span>乘<spanclass="math inline">\(B\)</span>得到矩阵<spanclass="math inline">\(C\)</span>。考虑<spanclass="math inline">\(C\)</span>的某个元素<spanclass="math inline">\(c_{ij}\)</span>，我们都知道这个元素是由<spanclass="math inline">\(A\)</span>的第i行与<spanclass="math inline">\(B\)</span>的第j列做点乘得到的。</p></li><li><p>但是我们如果再用“行的线性组合”的思想，就可以知道，首先，<spanclass="math inline">\(C\)</span>的第i行是由<spanclass="math inline">\(B\)</span>的每一行线性组合得到的，系数是<spanclass="math inline">\(A\)</span>的第i行。那如果我只看<spanclass="math inline">\(C\)</span>第i行的第j个，那么也就是<spanclass="math inline">\(B\)</span>每一行的第j个（也就是<spanclass="math inline">\(B\)</span>的第j列）的线性组合，系数是<spanclass="math inline">\(A\)</span>的第i行。从这个角度来看，就清晰多了。</p></li><li><p>好的，我们前面讨论的矩阵都是方阵。但其实，矩阵相乘不一定是方阵。假设<spanclass="math inline">\(A\)</span>为<spanclass="math inline">\(\mathbb{R}^{m \times n}\)</span>, <spanclass="math inline">\(B\)</span>为<spanclass="math inline">\(\mathbb{R}^{n \times p}\)</span>, <spanclass="math inline">\(C\)</span>为多少呢？</p></li><li><p>通过前面“行的线性组合”思想可以推出来，首先<spanclass="math inline">\(A\)</span>有m行，那么<spanclass="math inline">\(C\)</span>一定有m行，<spanclass="math inline">\(C\)</span>的每i行是由<spanclass="math inline">\(B\)</span>的每一行线性组合得到的，系数是<spanclass="math inline">\(A\)</span>的第i行。所以<spanclass="math inline">\(C\)</span>每一行的维度由<spanclass="math inline">\(B\)</span>每一行的维度决定，所以<spanclass="math inline">\(C\)</span>的维度就是<spanclass="math inline">\(\mathbb{R}^{m \times p}\)</span></p></li><li><p>好的，还可以通过“列的线性组合”思想来推出来。首先<spanclass="math inline">\(C\)</span>的第i列是由<spanclass="math inline">\(A\)</span>的每一列的线性组合得到的，系数是<spanclass="math inline">\(B\)</span>的第i列。所以<spanclass="math inline">\(C\)</span>每一列的维度跟<spanclass="math inline">\(A\)</span>一样，有m个分量，然后因为<spanclass="math inline">\(B\)</span>有p列，所以<spanclass="math inline">\(C\)</span>也有p列，所以<spanclass="math inline">\(C\)</span>的维度就是<spanclass="math inline">\(\mathbb{R}^{m \times p}\)</span></p></li></ul><hr /><ul><li><p>但是，还有第三种方法去理解矩阵乘法。就是将矩阵乘法拆成若干个矩阵的加法。我们先考虑一个例子，一个列向量乘一个行向量，假设维度分别为<spanclass="math inline">\(\mathbb{R}^{m \times 1}, \mathbb{R} ^ {1 \timesn}\)</span>，那么显然结果是一个矩阵。这个矩阵的得到可以用“行线组”或者“列线组”去理解都行。</p></li><li><p>ok，那么接下里看这个例子：</p></li></ul><p><span class="math display">\[\left[\begin{array}{c}2 &amp; 7 \\3 &amp; 8 \\4 &amp; 9\end{array}\right]\left[\begin{array}{c}1 &amp; 6 \\0 &amp; 0\end{array}\right]\]</span></p><p>我可以把它看成：第一列乘第一行 + 第二列乘第二行</p><p><span class="math display">\[\left[\begin{array}{c}2 &amp; 7 \\3 &amp; 8 \\4 &amp; 9\end{array}\right]\left[\begin{array}{c}1 &amp; 6 \\0 &amp; 0\end{array}\right]=\left[\begin{array}{c}2 \\3 \\4\end{array}\right]\left[\begin{array}{c}1 &amp; 6\end{array}\right]+\left[\begin{array}{c}7 \\8 \\9\end{array}\right]\left[\begin{array}{c}0 &amp; 0\end{array}\right]\]</span></p><ul><li>好了，现在理解矩阵乘法就有至少三种方法了：行的线性组合、列的线性组合、拆为列向量与对应行向量相乘转为矩阵加法</li></ul><hr /><ul><li>其实还有一种理解的方法，就是分块矩阵，可以把俩相乘的矩阵分成对应的块，例如下图：</li></ul><p><img src="1.png" /></p><p>那么其实就可以把<span class="math inline">\(A_1, A_2, A_3, A_4, B_1,B_2, B_3, B_4\)</span>看作“元素”，那么就跟之前的三种理解方式一样了。</p><p>可以通过行线组思想来理解，那么<spanclass="math inline">\(C\)</span>的第一行就是<spanclass="math inline">\(A_1[B_1, B_2] + A_2[B_3,B_4]\)</span>，第二行就是<span class="math inline">\(A_3[B_1, B_2] +A_4[B_3, B_4]\)</span></p><hr /><ul><li><p>对于那些有逆的矩阵，我们称为可逆矩阵或者非奇异矩阵。</p></li><li><p>好消息是对于方阵<spanclass="math inline">\(A\)</span>，其左逆和右逆是一样的。对于非方阵则不是，因为维度都不同，对于非方阵的逆，称为“伪逆”，这个之后再谈</p></li><li><p>所以方阵到底有没有逆，就是一个很重要的问题。</p></li><li><p>先来讨论一下奇异矩阵，也就是没有逆的矩阵</p></li><li><p>对于一个方阵<span class="math inline">\(A \in \mathbb{R}^{n\times n}\)</span>，假设它的逆为<spanclass="math inline">\(B\)</span>，那么<spanclass="math inline">\(AB=E\)</span>，<spanclass="math inline">\(E\)</span>是一组<spanclass="math inline">\(\mathbb{R}^{n \timesn}\)</span>的基向量，换句话说，用“列的线性组合”思想思考，<spanclass="math inline">\(A\)</span>通过<spanclass="math inline">\(B\)</span>做线性变换后，能得到一组基向量，也就是说明<spanclass="math inline">\(A\)</span>的列向量们是俩俩线性无关的。（若存在线性有关的情况，则不可能线组出一组基向量，因为一组基向量就代表着空间内任意向量都可以线组出来）</p></li><li><p>所以，用几何的思想去思考，一个方阵<spanclass="math inline">\(A\)</span>是否可逆，取决于它的列向量们是否俩俩线性无关。若有关，则不可逆，若无关，则可逆。</p></li><li><p>那能进一步思考吗？其实从刚才的思考可以发现，只要<spanclass="math inline">\(A\)</span>能线性组合出<spanclass="math inline">\(\mathbb{R}^{n \timesn}\)</span>中的任意一个向量，那么<spanclass="math inline">\(A\)</span>就可逆，反之不行。</p></li><li><p>“线性无关”这个条件，可以从<span class="math inline">\(Ax =0\)</span>这个代数方程去思考。如果这个方程有非零解，即<spanclass="math inline">\(col_1 \cdot x_1 + col_2 \cdot x_2 + ... + col_n\cdot x_n = 0\)</span>，移项得到：<spanclass="math inline">\((-\frac{1}{x_n}) \cdot (col_1 \cdot x_1 + col_2\cdot x_2 + ...) = col_n\)</span>，（因为非零解，所以必然可保证<spanclass="math inline">\(x_n \ne0\)</span>），即这些列向量是线性有关的，那么就不可逆了。</p></li><li><p>关于这个结论的证明还可以用反证法，我们的结论是：若能找到<spanclass="math inline">\(x\)</span>不是非零解，使得<spanclass="math inline">\(Ax = 0\)</span>，则<spanclass="math inline">\(A\)</span>不可逆，反之可逆。好，那现在假设<spanclass="math inline">\(A\)</span>可逆，那么有<spanclass="math inline">\(A^{-1}A = E\)</span>，所以<spanclass="math inline">\(A^{-1}Ax = A^{-1}0\)</span>，则<spanclass="math inline">\(x = 0\)</span>，但是前面说了<spanclass="math inline">\(x\)</span>不是非零解，所以假设不成立。</p></li></ul><hr /><ul><li>那么知道一个矩阵有逆后，如何求呢？</li><li>使用Gauss-Jordan消元法。具体来说，假设你想求<spanclass="math inline">\(A\)</span>的逆。那么就写一个增光矩阵: <spanclass="math inline">\([A | I]\)</span>，然后把<spanclass="math inline">\(A\)</span>消元为<spanclass="math inline">\(I\)</span>，那么此时<spanclass="math inline">\(I\)</span>就会变为<spanclass="math inline">\(A^{-1}\)</span>，即<span class="math inline">\([A| I] \rightarrow [I | A^{-1}]\)</span></li><li>原理很简单，消元的过程还记得前面讲的吗，消元的本质就是对消元的矩阵乘“初等矩阵”，那么上面消元的过程我可以用下面这个式子表达：</li><li><span class="math inline">\(E_1E_2E_3...E_k[A | I] = E&#39;[A | I] =[I | E&#39;I]\)</span></li><li>因为<span class="math inline">\(E&#39;A=I\)</span>，所以<spanclass="math inline">\(E&#39;\)</span>是<spanclass="math inline">\(A^{-1}\)</span>，所以<spanclass="math inline">\(E&#39;I\)</span>是<spanclass="math inline">\(A^{-1}\)</span>。</li><li>所以<span class="math inline">\([A | I] \rightarrow [I |A^{-1}]\)</span></li></ul><h3 id="四.-矩阵a的lu分解">四. 矩阵A的LU分解</h3><ul><li>A的LU分解，L是下三角矩阵的意思，U是上三角矩阵的意思</li><li>那A的LU分解有什么用呢？</li><li>主要是拿来多次解方程组，后续讲完你就懂了。</li><li>先用初等矩阵把A消元一下，得到上三角矩阵U，例如：</li><li><span class="math inline">\(E_{21}E_{31}E_{32}A = U\)</span></li><li>然后同乘这些初等矩阵的逆，记为L：</li><li><span class="math inline">\(A = LU\)</span></li><li>即可把<spanclass="math inline">\(A\)</span>分解为下三角和上三角矩阵的乘积</li><li>好了，那么有什么用呢？</li><li>假设要你解<span class="math inline">\(Ax = b_1, Ax = b_2, ... Ax =b_n\)</span></li><li>第一种方法是都对每个方程都Gauss消元一次，每次复杂度都是<spanclass="math inline">\(\mathcal{O}(n^3)\)</span>。</li><li>第二次方法是求出<spanclass="math inline">\(A^{-1}\)</span>，然后对于不同的b，直接拿<spanclass="math inline">\(A^{-1}\)</span>与b相乘即可。这样会快很多。</li><li>第三种方法就是用A的LU分解，先分解得到LU，然后即L(Ux) = b</li><li>然后先解<span class="math inline">\(Ly = b\)</span>得到y，再解<spanclass="math inline">\(Ux = y\)</span>得到x。</li><li>由于L和U都是三角，所以解上述俩方程的复杂度都是<spanclass="math inline">\(\mathcal{O}(n^2)\)</span></li></ul><h3 id="五.-置换-转置-向量空间">五. 置换, 转置, 向量空间</h3><ul><li><p>先讲一下置换矩阵<span class="math inline">\(P\)</span></p></li><li><p>置换矩阵是初等矩阵的一种，意思就是交换行或者列的矩阵</p></li><li><p>比如<spanclass="math inline">\(P_{12}\)</span>，就是交换行1和行2的矩阵</p></li><li><p>思考一个问题，<spanclass="math inline">\(P_{ij}\)</span>的逆矩阵是谁？</p></li><li><p>容易知道，它的逆就是<spanclass="math inline">\(P_{ji}\)</span>，因为<spanclass="math inline">\(P_{ij}P_{ji} = E\)</span>。</p></li><li><p>所以，思考一下不难得出，对于置换矩阵<spanclass="math inline">\(P\)</span>，有<span class="math inline">\(P^{-1} =P^\mathrm{T}\)</span></p></li><li><p>题外话，<span class="math inline">\(n \timesn\)</span>的置换矩阵<spanclass="math inline">\(P\)</span>有多少种呢？</p></li><li><p>置换矩阵的本质就是规定了行的顺序，那么行有多少种排列顺序，就有多少种置换矩阵。所以维度为n的置换矩阵的形态有<spanclass="math inline">\(n!\)</span>种（全排列）</p></li></ul><hr /><ul><li>置换矩阵<span class="math inline">\(P\)</span>在上一节讲过的<spanclass="math inline">\(A=LU\)</span>分解中可以用到。因为在对<spanclass="math inline">\(A\)</span>求上三角<spanclass="math inline">\(U\)</span>的时候，可能会碰到主元为0的情况，这是我们不想看到的。所以在一开始，就应该把<spanclass="math inline">\(A\)</span>的行顺序给调配好，然后再开始进行LU分解。所以，上一节讲到的公式，更一般的应该写成：<spanclass="math inline">\(PA = LU\)</span></li></ul><hr /><ul><li><p>讲完置换矩阵，我要讲，转置</p></li><li><p>转置就是<spanclass="math inline">\(\mathrm{T}\)</span>，转置很简单，我想讲的是对称矩阵，就是满足<spanclass="math inline">\(A^{\mathrm{T}}=A\)</span>的矩阵</p></li><li><p>对称矩阵很常见，为什么说它常见呢？因为任意一个矩阵<spanclass="math inline">\(M\)</span>，与自身的转置<spanclass="math inline">\(M^{\mathrm{T}}\)</span>相乘，就可以得到一个对称矩阵<spanclass="math inline">\(MM^{\mathrm{T}}\)</span></p></li><li><p>证明一下：<spanclass="math inline">\((MM^{\mathrm{T}})^{\mathrm{T}} =MM^{\mathrm{T}}\)</span></p></li></ul><hr /><ul><li><p>下面来说一下向量空间</p></li><li><p>最常见的向量空间就是<spanclass="math inline">\(\mathbb{R}^n\)</span>，其中最常见的就是<spanclass="math inline">\(\mathbb{R}^2\)</span></p></li><li><p>向量空间我觉得跟群的概念有点像，本质就是一个封闭的集合。对于向量空间来说，空间里的向量任意线性组合之后必须仍然要在空间内，才能称为向量空间</p></li><li><p>前面说了<spanclass="math inline">\(\mathbb{R}^n\)</span>是最常见的向量空间，但其实，我们更关心包含在其中的空间，即子空间</p></li><li><p>就拿<span class="math inline">\(\mathbb{R}^2\)</span>举例，<spanclass="math inline">\(\mathbb{R}^2\)</span>的子空间有谁呢？</p></li><li><p>首先，自己肯定是自己的子空间，这很容易。</p></li><li><p>然后是直线，即穿过原点的任意直线，也是<spanclass="math inline">\(\mathbb{R}^2\)</span>的子空间</p></li><li><p>第三个就是一个点，零向量</p></li><li><p>总结一下，能构成向量空间的规则，就是向量空间里的向量任意线性组合之后仍然在向量空间内，这就是向量空间</p></li></ul><hr /><ul><li>接下来，我们谈论一下，如何通过矩阵来构造子空间</li><li>对于一个矩阵<spanclass="math inline">\(A\)</span>，假设其有n行m列，那么它的m个列向量线性组合出来的向量空间叫做矩阵<spanclass="math inline">\(A\)</span>的列空间，记作<spanclass="math inline">\(C(A)\)</span></li></ul><h3 id="六.-列空间和零空间">六. 列空间和零空间</h3><ul><li><p>若有子空间<span class="math inline">\(S\)</span>和<spanclass="math inline">\(T\)</span>，那么<span class="math inline">\(S\bigcap T\)</span>是不是子空间，答案显然是的。</p></li><li><p>从感性上，稍微思考一下可以很容易的理解</p></li><li><p>理性证明也很好证：</p></li></ul><blockquote><p>设<span class="math inline">\(v, w \in S \bigcap T\)</span>, 则</p><p><span class="math inline">\(v \in S, v \in T\)</span>; <spanclass="math inline">\(w \in S, w \in T\)</span></p><p>所以<span class="math inline">\(v, w\)</span>的线性组合既在<spanclass="math inline">\(S\)</span>，也在<spanclass="math inline">\(T\)</span>中</p><p>所以<span class="math inline">\(v, w\)</span>的线组在<spanclass="math inline">\(S \bigcap T\)</span>中</p><p>所以<span class="math inline">\(S \bigcap T\)</span>是一个子空间</p></blockquote><hr /><ul><li><p>好了，现在要把子空间的概念与方程的解联系起来</p></li><li><p>之前我们有讨论过<spanclass="math inline">\(Ax=b\)</span>何时有解的情况</p></li><li><p>之前我们说的是，若A的列向量们线组无法线组出来b，那么方程就是无解。</p></li><li><p>现在有了子空间这个概念，我们可以把上面那句话说的更专业一点：</p></li><li><p>若<span class="math inline">\(b \notinC(A)\)</span>，则无解（若b不在A的列空间内则无解）；反之有解</p></li></ul><hr /><ul><li><p>接下来介绍一下零空间的概念</p></li><li><p>对于<span class="math inline">\(Ax=b\)</span>这个方程，当<spanclass="math inline">\(b=0\)</span>的时候，其的解集称为<spanclass="math inline">\(A\)</span>的零空间，记为<spanclass="math inline">\(N(A)\)</span></p></li><li><p>需要注意的是，对于一个矩阵<span class="math inline">\(A \in\mathbb{R}^{n \times m}\)</span>，其列空间<spanclass="math inline">\(C(A)\)</span>是<spanclass="math inline">\(\mathbb{R}^{n}\)</span>维的（因为每个列向量有n个分量），而其零空间<spanclass="math inline">\(N(A)\)</span>是<spanclass="math inline">\(\mathbb{R}^m\)</span>维的（因为有m个列向量，所以有m个系数）</p></li></ul><hr /><ul><li><p>好了，前面介绍了零空间的概念。但是，我想请问，零空间一定是子空间吗？</p></li><li><p>答案：是的</p></li><li><p>证明过程很简单，如下：</p></li></ul><blockquote><p>if <span class="math inline">\(v, w \in N(A)\)</span>, i.e., <spanclass="math inline">\(Av=0, Aw=0\)</span></p><p>then <span class="math inline">\(A(k_1v + k_2w) = Ak_1v + Ak_2w =k_1(Av) + k_2(Aw) = 0\)</span></p><p>so <span class="math inline">\(k_1v + k_2w \in N(A)\)</span> for any<span class="math inline">\(v, w \in N(A)\)</span></p><p>so <span class="math inline">\(N(A)\)</span> is a subspace.</p></blockquote><h3 id="七.-求解ax0">七. 求解Ax=0</h3><ul><li><p>这节先介绍解<spanclass="math inline">\(Ax=0\)</span>的算法</p></li><li><p>先对<span class="math inline">\(A\)</span>消元</p></li><li><p>当然，可能出现主元为0的情况，不影响</p></li><li><p>如果所有主元都不为0，可以得到一个上三角矩阵，如果有主元为0，那么得到的将是一个阶梯型矩阵</p></li><li><p>主元的个数很重要，它有个名字：秩</p></li><li><p>OK，回到解<spanclass="math inline">\(Ax=0\)</span>的问题。假设现在我们通过消元得到了一个阶梯型矩阵<spanclass="math inline">\(U\)</span>，那么现在要解决的问题就是<spanclass="math inline">\(Ux=0\)</span>的解</p></li><li><p>我们把<spanclass="math inline">\(U\)</span>里阶梯的每个凸角那列叫做“主列”，其余列叫做“自由列”。（之所以叫自由列因为它们可以被其余的主列线组出来）</p></li><li><p>对于自由列对应的解，我们叫做“自由变量”。自由变量可以随便取，取完之后，就可以反代解出主列对应的解。从而可以得到一组解。</p></li><li><p>通常，假设有k个自由变量，那么我们会求k组特殊解，自由变量的取值就是枚举k个人，第i个人是1其余人是0，得到k组特殊解。</p></li><li><p>解空间<spanclass="math inline">\(N(A)\)</span>就是这k个特殊解的线性组合</p></li></ul><hr /><ul><li><p>好了，现在来总结一下上面的算法流程</p></li><li><p>首先对于一个维度为<span class="math inline">\(\mathbb{R}^{m\times n}\)</span>的矩阵<spanclass="math inline">\(A\)</span>，假设有<spanclass="math inline">\(r\)</span>个主元，也就是秩为<spanclass="math inline">\(r\)</span>。那么主列就有<spanclass="math inline">\(r\)</span>列，那么自由列就有<spanclass="math inline">\(n - r\)</span>列，那么就有<spanclass="math inline">\(n - r\)</span>个自由变量。</p></li><li><p>那么分别对这<span class="math inline">\(n -r\)</span>个自由变量取1其余取0，反代，就可以得到<spanclass="math inline">\(n - r\)</span>组特殊解。</p></li><li><p>这<span class="math inline">\(n -r\)</span>个特殊解的线性组合就是<spanclass="math inline">\(N(A)\)</span></p></li></ul><hr /><ul><li><p>如何理解上面的算法流程呢？</p></li><li><p>首先要知道，对于<span class="math inline">\(Ax=0, A \in\mathbb{R}^{m \timesn}\)</span>本质就是令n个列向量的线性组合为零向量。那么假设我找到一组解，那么这个解的倍数仍然是解。</p></li><li><p>ok，然后消元后我们可以知道哪些列是主列，哪些列是自由列，自由列的意思就是说它可以被别人线组出来。所以它对应的解（自由变量）就可以随便取。</p></li><li><p>那如何表示出所有的自由变量的取值呢？</p></li><li><p>答案：线性组合</p></li><li><p>假设有<span class="math inline">\(n -r\)</span>个自由变量，那么就搞<span class="math inline">\(n -r\)</span>次，每次就是其中一个自由变量为1，其余自由变量为0，得到<spanclass="math inline">\(n -r\)</span>组特解。（跟基向量的感觉比较像）</p></li><li><p>那么这<span class="math inline">\(n -r\)</span>组特解的线性组合就是解空间，即零空间<spanclass="math inline">\(N(A)\)</span></p></li></ul><hr /><ul><li><p>好了，接下来讲点好玩的东西</p></li><li><p>前面我们已经知道了<spanclass="math inline">\(Ax=0\)</span>的解法。</p></li><li><p>OK，现在我们再进一步思考，前面的算法仍然有回代这一步，这一步往往是计算机不喜欢的，能不能使算法更加“程序化”一些？</p></li><li><p>答案是可以的，假设我们已经通过消元得到了阶梯型矩阵<spanclass="math inline">\(U \in \mathbb{R}^{m \times n}\)</span></p></li><li><p>对于<spanclass="math inline">\(U\)</span>中的主列，将它主元的头上面全面消元为0。</p></li><li><p>然后把主列全挪到前面，后面放自由列。</p></li><li><p>这样，对于<spanclass="math inline">\(r\)</span>列主列，其实就得到了一个<spanclass="math inline">\(m \times r\)</span>的矩阵，这个矩阵上半部分是<spanclass="math inline">\(r \times r\)</span>的单位阵<spanclass="math inline">\(I\)</span>，下半部分是<spanclass="math inline">\((m - r) \times r\)</span>的全零矩阵。</p></li><li><p>然后对于<span class="math inline">\(n -r\)</span>列自由列，其实是一个<span class="math inline">\(m \times (n -r)\)</span>的矩阵，这个矩阵上半部分是<span class="math inline">\(r\times (n - r)\)</span>的矩阵<spanclass="math inline">\(F\)</span>，下半部分是<spanclass="math inline">\((m - r) \times (n -r)\)</span>的全零矩阵。</p></li><li><p>写出来的话，就是：</p></li></ul><p><span class="math display">\[\left[\begin{array}{c}I &amp; F \\O &amp; O\end{array}\right]\]</span></p><ul><li><p>把这个矩阵记为<spanclass="math inline">\(R\)</span>，那么现在问题就变成了<spanclass="math inline">\(Rx=0\)</span></p></li><li><p>那我更进一步，我想直接求出<span class="math inline">\(X \in n\times (n - r)\)</span>，<spanclass="math inline">\(X\)</span>中的每一列都是<spanclass="math inline">\(Rx=0\)</span>的一组特解。（其实就是求出<spanclass="math inline">\(n - r\)</span>组特解，然后拼一起得到的矩阵<spanclass="math inline">\(X\)</span>）</p></li><li><p>用公式写出来即</p></li></ul><p><span class="math display">\[\left[\begin{array}{c}I_{r \times r} &amp; F_{r \times (n - r)} \\O_{(m - r) \times r} &amp; O_{(m - r) \times (n - r)}\end{array}\right]X = O\]</span></p><ul><li>显然</li></ul><p><span class="math display">\[X=\left[\begin{array}{c}-F_{r \times (n - r)} \\I_{(n - r) \times (n - r)}\end{array}\right]\]</span></p><ul><li><p><span class="math inline">\(X\)</span>的列空间<spanclass="math inline">\(C(X)\)</span>就是<spanclass="math inline">\(Ax=0\)</span>的零空间<spanclass="math inline">\(N(A)\)</span></p></li><li><p>多看几遍上面的过程吧，非常优美的解法。</p></li></ul><h3 id="八.-求解axb">八. 求解Ax=b</h3><ul><li><p>上节的内容是求<spanclass="math inline">\(Ax=0\)</span>的零空间<spanclass="math inline">\(N(A)\)</span></p></li><li><p>这节的目标是讨论<spanclass="math inline">\(Ax=b\)</span></p></li><li><p>首先这个方程有可能有解，也有可能无解。如果有解的话，是否有多解，并求出所有解，这是本节要讨论的问题。</p></li><li><p>显然，通过前面所学，很容易可知，如果<span class="math inline">\(b\in N(A)\)</span>，那么方程<spanclass="math inline">\(Ax=b\)</span>就有解。</p></li><li><p>那有解的时候，如何求出所有解呢？</p></li><li><p>假设矩阵<span class="math inline">\(A \in \mathbb{R}^{m \timesn}\)</span>，秩为<span class="math inline">\(r\)</span>，那么就有<spanclass="math inline">\(n - r\)</span>个自由变量。令这<spanclass="math inline">\(n -r\)</span>个自由变量全取0，即可求出一组特解<spanclass="math inline">\(x_p\)</span>。</p></li><li><p>那么<span class="math inline">\(Ax = b\)</span>的解空间就是<spanclass="math inline">\(x_p + N(A)\)</span></p></li><li><p>注意，<span class="math inline">\(N(A)\)</span>就是<spanclass="math inline">\(Ax=0\)</span>的解空间，它是一个向量空间</p></li><li><p>但是<span class="math inline">\(x_p +N(A)\)</span>就不一定是一个向量空间了，因为它可能不过零向量</p></li><li><p>总之，<span class="math inline">\(Ax=0\)</span>的解空间是<spanclass="math inline">\(N(A)\)</span>，<spanclass="math inline">\(Ax=b\)</span>的解空间是<spanclass="math inline">\(x_p + N(A)\)</span>，<spanclass="math inline">\(x_p\)</span>是自由变量全取0时算出来的一组特解</p></li></ul><hr /><ul><li><p>到现在，其实你已经学会了解<span class="math inline">\(Ax=0,Ax=b\)</span>了。</p></li><li><p>回顾一下，首先是<spanclass="math inline">\(Ax=0\)</span>，先消元，得到秩r，如果r =n，那么就没有自由变量了， 那么<spanclass="math inline">\(N(A)\)</span>里只有零向量。</p></li><li><p>如果r &lt; n，那么就有n - r个自由变量，那么就可以求出n -r组特解，这n - r组特解的线性组合就是<spanclass="math inline">\(N(A)\)</span></p></li><li><p>再来回顾<spanclass="math inline">\(Ax=b\)</span>，先消元，得到秩r，如果r =n，那么就没有自由变量了，那么<spanclass="math inline">\(N(A)\)</span>里只有零向量。然后看看<spanclass="math inline">\(Ax=b\)</span>是否有特解<spanclass="math inline">\(x_p\)</span>，有的话，那么<spanclass="math inline">\(Ax=b\)</span>的解集就只有<spanclass="math inline">\(x_p\)</span>了。如果没有，那么<spanclass="math inline">\(Ax=b\)</span>就没解。</p></li><li><p>如果r &lt; n，那么可以就可以先把<spanclass="math inline">\(N(A)\)</span>求出来。然后求<spanclass="math inline">\(Ax=b\)</span>的特解<spanclass="math inline">\(x_p\)</span>，然后<span class="math inline">\(x_p+ N(A)\)</span>就是<spanclass="math inline">\(Ax=b\)</span>的解集。</p></li></ul><hr /><ul><li>OK，现在再从秩的角度来思考这个问题（<span class="math inline">\(A\in \mathbb{R}^{m \times n}\)</span>）</li></ul><ol type="1"><li>r = m = n<ul><li>此时是方阵，且<spanclass="math inline">\(A\)</span>消元后是单位阵，所以肯定有且只有唯一解</li><li>另一个角度，满秩的方阵是可逆矩阵，所以<span class="math inline">\(x= bA^{-1}\)</span>，从这个角度也可以证明有且只有唯一解</li></ul></li><li>r = n &lt; m<ul><li>此时<span class="math inline">\(A\)</span>消元之后可得到 $ $</li><li>因为n - r = 0，所以没有自由变量，所以<spanclass="math inline">\(N(A)\)</span>里只有零向量。所以<spanclass="math inline">\(Ax=b\)</span>要不没解，要不只有唯一解（就是特解）</li></ul></li><li>r = m &lt; n<ul><li>此时<span class="math inline">\(A\)</span>消元之后可得到 $ $</li><li>此时n - r &gt; 0，所以有自由变量，所以<spanclass="math inline">\(N(A)\)</span>里是有无限多向量的。所以只要<spanclass="math inline">\(Ax=b\)</span>有特解<spanclass="math inline">\(x_p\)</span>，那么<spanclass="math inline">\(Ax=b\)</span>就有无穷多解了。</li><li>显然，<spanclass="math inline">\(Ax=b\)</span>可以找到特解，因为消元之后没有出现全为0的行，所以肯定能凑出一组解</li><li>所以这种情况下，方程<spanclass="math inline">\(Ax=b\)</span>有无穷多组解。</li></ul></li><li>r &lt; m, r &lt; n<ul><li>此时<span class="math inline">\(A\)</span>消元之后可得到 $ $</li><li>此时n - r &gt; 0，所以有自由变量，所以<spanclass="math inline">\(N(A)\)</span>里是有无限多向量的。所以只要<spanclass="math inline">\(Ax=b\)</span>有特解<spanclass="math inline">\(x_p\)</span>，那么<spanclass="math inline">\(Ax=b\)</span>就有无穷多解了。</li><li>但是，这里化简之后出现了全0行，所以用增广矩阵去看全0行的那几行b方程可能无法满足。</li><li>所以，如果能满足的话，就是无穷多组解。如果无法满足的话，那么就没有解</li></ul></li></ol><ul><li>用一句话总结，矩阵的秩说明了方程解的情况。</li></ul><h3 id="九.-线性相关-基-维数">九. 线性相关, 基, 维数</h3><ul><li><p>线性相关的标准定义就是，如果一组向量能线性组合出零向量（系数不能全为0），那么这组向量就线性有关；反之线性无关</p></li><li><p>向量空间的一组基，是指这么一组向量，这组向量满足两个性质：</p><ol type="1"><li>向量组线性无关</li><li>它们恰好能生成整个空间，少一个不行</li></ol></li><li><p>对于一个向量空间，它的基有很多。但是，所有的基中的向量的个数都相同，这个数量称为该向量空间的维数</p></li><li><p><del>所以知道一个向量空间的维数很重要，假设知道了维数dim =k，还知道向量的维度，那么只需要找k个线性无关的该维度的向量即是这个向量空间的一组基。</del>（upd：这句话是错误的！！！）</p></li><li><p>错误原因是因为当时我没意识到行变换会改变列空间。我举一个反例：</p></li></ul><p><span class="math display">\[\left[\begin{array}{c}1 &amp; 0 \\0 &amp; 1 \\0 &amp; 0 \\\end{array}\right]\]</span></p><ul><li><p>这组列向量产生的向量空间是三维空间里的二维过原点的水平平面。</p></li><li><p>ok，如果上面那句话是对的。那么看下面这组由上面那组向量行变换的哀悼的列向量：</p></li></ul><p><span class="math display">\[\left[\begin{array}{c}1 &amp; 1 \\0 &amp; 1 \\2 &amp; 2 \\\end{array}\right]\]</span></p><ul><li><p>俩列向量线性无关，但是形成的向量空间显然不是一个水平的平面。故上面那句话是错的。</p></li><li><p>行变换不会改变列向量的线性关系，所以求向量空间的基的时候，可以随便用行变换。这为下面<spanclass="math inline">\(\mathrm{dim}C(A) = \mathrm{dim}C(R) =r\)</span>的结论做了铺垫。</p></li><li><p>但是行变换会改变列空间，所以求向量空间的时候，一定要注意，求的是列空间还是行空间，如果是列空间的话，就要想到行变换会改变列空间这个坑点。</p></li></ul><blockquote><p>为什么行变换不会改变列向量的线性关系？这里给出证明：</p><p>考虑<span class="math inline">\(k_1b_1 + k_2b_2 + ... + k_nb_n =0\)</span></p><p>做一次行变换后，假设<spanclass="math inline">\(b_i\)</span>行变成了<spanclass="math inline">\(b_i +cb_j\)</span>，那么列出n列的线性组合表达式，还是能整理为<spanclass="math inline">\(k_1b_1 + k_2b_2 + ... + k_nb_n =0\)</span>的形式。</p><p>所以变换前后，俩矩阵的列向量的线性组合可以化为同一种形式，所以线性关系是相同的</p><p>（上面这个证法是自己想的，若错误或者有更好的方法欢迎讨论哇）</p></blockquote><hr /><ul><li>现在，让我们把基、维数的概念用到矩阵<spanclass="math inline">\(A\)</span>中</li><li>首先，对于矩阵<span class="math inline">\(A\)</span>的列空间<spanclass="math inline">\(C(A)\)</span>，它的基是啥？维数是多少？</li><li>很显然，<spanclass="math inline">\(A\)</span>消元后可知道秩r，表示的是主列的个数，这个秩其实就是列空间<spanclass="math inline">\(C(A)\)</span>的维数<spanclass="math inline">\(\mathrm{dim}C(A) = r\)</span></li><li><span class="math inline">\(A\)</span>的主列们就是<spanclass="math inline">\(C(A)\)</span>的一组基（注意这里我说的是<spanclass="math inline">\(A\)</span>的主列们而不是<spanclass="math inline">\(A\)</span>经过行变换后<spanclass="math inline">\(R\)</span>的主列们）</li><li>OK，那对于<span class="math inline">\(A\)</span>的零空间<spanclass="math inline">\(N(A)\)</span>呢？它的基是啥？维数是多少？</li><li>回顾求解<span class="math inline">\(N(A)\)</span>的过程，就是找n -r组特解（有n - r个自由变量）。所以n - r就是<spanclass="math inline">\(N(A)\)</span>的维数<spanclass="math inline">\(\mathrm{dim}N(A) = n - r\)</span>。这n -r组特解就是<span class="math inline">\(N(A)\)</span>的一组基。</li></ul><h3 id="十.-四个基本子空间">十. 四个基本子空间</h3><ul><li><p>四个基本子空间是：列空间、零空间、行空间、左零空间</p></li><li><p>列空间老朋友了，<spanclass="math inline">\(C(A)\)</span></p></li><li><p>零空间也是老朋友了，<spanclass="math inline">\(N(A)\)</span></p></li><li><p>行空间其实可以写成这样，<spanclass="math inline">\(C(A^{\mathrm{T}})\)</span></p></li><li><p>左零空间其实就是，<spanclass="math inline">\(N(A^{\mathrm{T}})\)</span></p></li><li><p>为什么要叫左零空间呢？其实是这样的，<spanclass="math inline">\(A^{\mathrm{T}}y = 0\)</span>，转置，得到，<spanclass="math inline">\(y^{\mathrm{T}}A = 0^{\mathrm{T}}\)</span></p></li><li><p>这里的解在左边，所以就叫左零空间</p></li></ul><hr /><ul><li><p>现在我们来讨论一下这四个空间的维数dim和基</p></li><li><p>首先是列空间，列空间的维数是r。基是多少呢？</p></li><li><p>这里我要强调一点，<span class="math inline">\(C(A) \neC(R)\)</span>（<span class="math inline">\(R\)</span>是<spanclass="math inline">\(A\)</span>经过行变换得到的）</p></li><li><p>因为做行变换会改变列空间，但不会改变行空间</p></li><li><p>但为什么做行变换之后还能求解呢？因为你在做行变换（高斯消元）的过程的时候，是对增广矩阵做的。所以<spanclass="math inline">\(Ax=b\)</span>与<spanclass="math inline">\(Rx=b&#39;\)</span>是等价的，而不是<spanclass="math inline">\(Ax=b\)</span>与<spanclass="math inline">\(Rx=b\)</span>是等价的。</p></li><li><p>好了，所以<spanclass="math inline">\(C(A)\)</span>的维数是r，基是<spanclass="math inline">\(A\)</span>的主列们</p></li><li><p>然后讨论零空间<spanclass="math inline">\(N(A)\)</span>，零空间关心的是解集，所以不用关心行变换会影响到<spanclass="math inline">\(N(A)\)</span>。所以<spanclass="math inline">\(N(A)\)</span>的维数是n - r，基就是n -r组特解</p></li><li><p>接下来讨论行空间<spanclass="math inline">\(C(A^\mathrm{T})\)</span>，它的维数是r，基呢？</p></li><li><p>其实直接对<span class="math inline">\(A\)</span>做消元得到<spanclass="math inline">\(R\)</span>，<spanclass="math inline">\(R\)</span>的主行们就是<spanclass="math inline">\(C(A^\mathrm{T})\)</span>。因为行变换不会改变行空间，所以<spanclass="math inline">\(A\)</span>与<spanclass="math inline">\(R\)</span>的行空间是相同的。</p></li><li><p>最后来讨论左零空间<spanclass="math inline">\(N(A^\mathrm{T})\)</span>。左零空间的维数是m -r很显然，那么基呢？最简单的方法就是对<spanclass="math inline">\(A^\mathrm{T}\)</span>消元，然后m -r组特解就是左零向量的一组基。</p></li></ul><h3 id="十一.-矩阵空间">十一. 矩阵空间</h3><p>既然有向量空间，那么其实也有矩阵空间。其实任意东西都可以抽象成“向量空间”。</p><p>想象一个以3X3矩阵构成的空间M为例。你从M中任挑俩矩阵，相加或者做数乘，发现仍然得到3X3矩阵，所以这是一个矩阵空间。</p><p>这个矩阵空间还有一些有意思的子空间，比如3X3对称矩阵这个子空间、3X3的上三角矩阵这个子空间。</p><p>显然，M的一组基是9个矩阵，所以M的维数是9（dimM=9）。</p><p>记3X3对称矩阵构成的空间为S，那么显然S的一组基是6个矩阵，dimS=6。</p><p>记3X3上三角矩阵构成的空间为U，那么那么显然U的一组基也是6个矩阵，dimY=6。</p><p>考虑<span class="math inline">\(S \capU\)</span>，一个矩阵即是对称的又是上三角的，那么它就是对角的。所以<spanclass="math inline">\(S \capU\)</span>表示的是3X3对角矩阵这个子空间。显然，<spanclass="math inline">\(\mathrm{dim}(S \cap U)=3\)</span>。</p><p>好，现在考虑一下<span class="math inline">\(S \cup U\)</span>和<spanclass="math inline">\(S +U\)</span>的区别。只要一个矩阵是对称的，或者上三角的，那么它就属于<spanclass="math inline">\(S \cupU\)</span>，但是在这个空间对加法不封闭，所以<spanclass="math inline">\(S \cup U\)</span>不是一个子空间；</p><p><span class="math inline">\(S +U\)</span>中的每一个矩阵都可以i表示为<spanclass="math inline">\(S\)</span>中的一个矩阵加上<spanclass="math inline">\(U\)</span>中的一个矩阵。所以<spanclass="math inline">\(S + U\)</span>是对加法和数乘封闭的，所以<spanclass="math inline">\(S + U\)</span>是一个子空间。另外，当在<spanclass="math inline">\(S\)</span>中任取时，<spanclass="math inline">\(U\)</span>中取零矩阵时，得到的就是<spanclass="math inline">\(S\)</span>。同理，在<spanclass="math inline">\(U\)</span>中任取时，<spanclass="math inline">\(S\)</span>中取零矩阵时， 得到的就是<spanclass="math inline">\(U\)</span>。所以<span class="math inline">\(S +U\)</span>是包含<span class="math inline">\(S \cupU\)</span>的，换句话说，<span class="math inline">\((S \cap U) \subseteq(S \cup U) \subseteq (S + U)\)</span>。</p><p>进一步想想，<span class="math inline">\(S +U\)</span>是什么，其实它就是<spanclass="math inline">\(M\)</span>这个空间。所以显然<spanclass="math inline">\(dim(S + U) = dim(M) = 9\)</span>。</p><p>到这里，我们可以发现一个式子：<span class="math inline">\(dim(S) +dim(U) = dim(S + U) + dim(S \cap U)\)</span></p><p>这不是碰巧，这确实是一个定理。</p><p>所以总结一下，若有向量空间<span class="math inline">\(S,U\)</span>，则<span class="math inline">\(S \cap U\)</span>和<spanclass="math inline">\(S + U\)</span>也是向量空间，但<spanclass="math inline">\(S \cup U\)</span>不是。而且满足：<spanclass="math inline">\(dim(S) + dim(U) = dim(S + U) + dim(S \capU)\)</span></p><hr /><p>下面来一个有趣的例题，假设列向量<span class="math inline">\(v \in\mathbb{R}^4\)</span>，且满足其四个分量之和为0。那么<spanclass="math inline">\(v\)</span>是不是一个向量空间？如果是的话，基和维数是什么？</p><p>首先，在v中任取俩v1,v2，做加法和数乘仍在v中，所以v是一个向量空间。</p><p>然后它的基和维数是多少呢？</p><p>因为v不是一个传统的给定数值的矩阵，所以它的秩不好求。所以这里需要转化思维，如果把v看作是某个矩阵A的零空间，那么只需求出<spanclass="math inline">\(dim(N(A))\)</span>就是v的基，同样，零空间的一组基就是n- r个特解。</p><p>思考后不难发现，<span class="math inline">\(A = [1, 1, 1,1]\)</span>，此时有<span class="math inline">\(Av = 0\)</span>，<spanclass="math inline">\(N(A) = v\)</span>。</p><p>显然对于矩阵<span class="math inline">\(A\)</span>，秩为1，那么<spanclass="math inline">\(dim(N(A)) = dim(v) = n - r = 3\)</span></p><p>主列为第一列，所以自由变量为后三个 ，所以分别可得出特解：<spanclass="math inline">\([-1, 1, 0, 0]^\mathrm{T}, [-1, 0, 1,0]^\mathrm{T}, [-1, 0, 0, 1]^\mathrm{T}]\)</span>。这三个向量就是<spanclass="math inline">\(v\)</span>向量空间的一组基。</p><p>这种解法非常的巧妙，既然正着不好求，就把其转换为矩阵的零空间，从而得到它的空间性质。</p><h3 id="十二.-图和网络">十二. 图和网络</h3><p>本小节不涉及新的线性代数的知识，而是对于实际问题建模，用线性代数去解决，具有启发意义的一节。</p><p>这篇<ahref="https://rqtn.github.io/mit-18.06/mit-18.06-lec12/">博文</a>写的不错</p><h3 id="十三.-复习课一">十三. 复习课一</h3><p><ahref="https://www.bilibili.com/video/BV16Z4y1U7oU?p=13&amp;vd_source=ca9a71bb3c1806ce48ae27d95e4e8bd0">链接</a>：直接去听，如果都掌握了的话，全部内容是都可以听懂的。</p><p>如果听不懂，说明前面的知识没掌握牢固，建议回到对应的位置重新温习后再来听这堂课。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;方程组、矩阵、消元、向量空间、秩、解方程&lt;/p&gt;
&lt;p&gt;还差P23、P27没学，等学完微分方程后回来看。&lt;/p&gt;</summary>
    
    
    
    <category term="4. 大学" scheme="http://error666.top/categories/4-%E5%A4%A7%E5%AD%A6/"/>
    
    <category term="数学" scheme="http://error666.top/categories/4-%E5%A4%A7%E5%AD%A6/%E6%95%B0%E5%AD%A6/"/>
    
    <category term="线性代数" scheme="http://error666.top/categories/4-%E5%A4%A7%E5%AD%A6/%E6%95%B0%E5%AD%A6/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>2024数学建模国赛游记</title>
    <link href="http://error666.top/2024/09/09/2024%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E5%9B%BD%E8%B5%9B%E6%B8%B8%E8%AE%B0/"/>
    <id>http://error666.top/2024/09/09/2024%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E5%9B%BD%E8%B5%9B%E6%B8%B8%E8%AE%B0/</id>
    <published>2024-09-08T16:18:30.000Z</published>
    <updated>2024-09-08T16:55:55.367Z</updated>
    
    <content type="html"><![CDATA[<p>2024年数学建模国赛游记</p><span id="more"></span><p>其实也没啥好写的，总结就是队友给力，大家尽力，传奇论文手、天使建模手。</p><p>打数模这事也是和舍友偶然一次聊天，了解到他打数模，没找到队友，于是我说可以一起打啊，便这样组好了队（两个建模手）。后来，他又拉到我们班一同班同学（传奇论文手）。于是三人队就此形成。</p><p>在正赛之前我们进行过两次正式训练，一次校赛，一次自己打的训练赛。其余日常的小训练就不说了。说来惭愧，日常的数模学习我其实并没有学非常多数模知识，大多数只是泛泛而谈，即了解一下概念，学习一下原理就过了。所以日常训练中其实并没有学到成体系的知识，只是知道了许多名词，以及看了一些国奖论文，知道国奖的论文大概是如何包装的。</p><p>校赛是我们第一次正式训练，总的来说，我觉得作为我们第一次正式写论文的经历比赛，效果还是很不错的。虽然从结果上来说，最终只是个校赛二等奖，建模也自我感觉建的很普通，但是至少我们做出了一份可以看的过去的成品。</p><p>然后就到训练赛了。讲实话训练赛我没有怎么参与，因为当时时间和论文ddl撞了。所以我负责的部分写的很垃圾（自我感觉）。最终，虽然论文的编排有进步（传奇论文手还在进步），但建模效果我认为还不如校赛。</p><p>ok到国赛了。国赛大家从一开始就很上心，特地申请了一间小房子，3天比赛时间几乎全天泡在里头搞数模。第一天晚上我们主要把B题浏览并分析了个大概，把前三问的模型搭建了一个最初步的模型，然后分配了下任务，我负责（1）（4）问，另一个建模手负责（2）（3）问，然后就睡觉了。</p><p>第二天，仔细思考后我们觉得建的模型不对，于是反复思考后在之前模型基础上，进行了大量修改，最终有了全部问题的思路。这中间的过程非常复杂，尤其另一个建模手的（2）（3）问，数学推导十分严谨，效果很好。</p><p>这一天我把第一问模型也建好了，用了俩方法去解决第一问，反正就ChatGPT辅助呗。他给你思路，你理解消化后修正它思路，他再给你思路，你再修正......反反复复，最终建立好了（1）问的模型，并写出了代码。</p><p>第三天，我花了点时间把问题（4）的模型也建完了。但是觉得不够高级，于是加了点trick加速模型求解速度。我的建模任务到此基本结束。另一位建模手负责的（2）（3）问在这一天也修正了一点点小问题，建模也基础结束。论文手开始进入除（1）问的论文编写，最终论文手加班到凌晨4点，我们的论文基本成型（太敬业了，给队友点赞）。</p><p>最后一天，把摘要写了，然后缝缝补补修修论文，晚上就交了。</p><hr /><p>国赛经历其实平平淡淡，按部就班。但是我们三个人是非常尽力的，所以最终的效果我们都比较满意。无论结果如何，至少我们交出了一份问心无愧的答案。</p><p>结果留个坑，到时候出成绩再更新... ...</p><p>为什么没写详细的思路过程？我觉得没啥必要，游记嘛本来就随便写写。</p><p>大概讲讲吧，思路第一问俩方法。法一直接暴力迭代求解，法二序贯检验。第二三问，建立完备的数学期望模型。第四问用自适应蒙特卡罗求解。</p><p>详细的思路也不适合在游记写，有兴趣的到时候去看我github仓库里的论文吧，有什么问题欢迎大家一起交流学习呀。</p><hr /><p>其实，数模比赛我认为是一个“成分复杂“的比赛。你说它水，但是它还真需要一些逻辑和思考才能建出模型和写出代码；你说它严谨，其实部分论文都是瞎编甚至造假的，写论文的人自己都不知道自己在干啥......总之，我对数学建模比赛持中立态度。我的建议是，如果是为了培养写论文的能力以及快速学习能力，那欢迎你参加数模。如果是为了培养所谓数学思维和提高专业水平能力的，打数模浪费时间，不如去搞正儿八经的科研。</p><hr /><p>最后，如果你决定了要参加数模比赛。别忘了找俩好队友，三个臭皮匠或许顶不了一个诸葛亮，但是一颗老鼠屎绝对会坏了整锅汤。<del>这是最重要的，参考我们队的传奇论文手和天使建模手</del>，开玩笑hhhh。赞扬我队友那么多，不是说我是混子，其实我的实力非常受到队友认可。而是第一次遇到这么负责且有实力，思维统一的队友，感叹于自己的幸运有感而发。以前大大小小的比赛太多太多是我一个人单打独斗了。</p><p>最后，祝大家身体健康，学业顺利！</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;2024年数学建模国赛游记&lt;/p&gt;</summary>
    
    
    
    <category term="3. 竞赛" scheme="http://error666.top/categories/3-%E7%AB%9E%E8%B5%9B/"/>
    
    
  </entry>
  
  <entry>
    <title>大数据架构与技术自学笔记</title>
    <link href="http://error666.top/2024/09/04/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%9E%B6%E6%9E%84%E4%B8%8E%E6%8A%80%E6%9C%AF%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/"/>
    <id>http://error666.top/2024/09/04/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%9E%B6%E6%9E%84%E4%B8%8E%E6%8A%80%E6%9C%AF%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/</id>
    <published>2024-09-03T18:09:39.000Z</published>
    <updated>2024-09-04T02:52:54.384Z</updated>
    
    <content type="html"><![CDATA[<p>其实是一门选修课，但是说是选修，因为学分原因也是要必选的，所以就当它是必修吧。</p><span id="more"></span><h3 id="hadoop简介">Hadoop简介</h3><ul><li>Hadoop狭义上来讲，就是一款处理分布式的开源软件</li><li>广义上Hadoop指的是围绕Hadoop打造的大数据生态圈</li><li>Hadoop的核心组件<ol type="1"><li>Hadoop HDFS（分布式文件存储系统）：解决海量数据存储</li><li>Hadoop YARN（集群资源管理和任务调度框架）：解决资源任务调度</li><li>Hadoop MapReduce（分布式计算框架）：解决海量数据计算</li></ol></li><li>可以理解为mysql统治了数据库的半壁江山。</li><li>HDFS作为分布式文件存储，处于生态圈的底层与核心地位。</li><li>YARN作为分布式通用的集群资源管理系统和任务调度平台，支持各种计算引擎运行，保证了Hadoop地位。</li><li>MapReduce作为大数据生态圈第一代分布式计算引擎，由于自身设计的模型所产生的弊端，导致企业一线几乎不再直接使用MapReduce进行编程处理，但是很多软件的底层依然在使用MapReduce引擎来处理数据。</li></ul><hr /><ul><li>Hadoop的发行版本<ol type="1"><li>开源社区版：Apache开源社区官方开源版本<ul><li>优点：更新速度快，免费</li><li>缺点：兼容/稳定性不周</li></ul></li><li>商业发型版：基于Apache开源协议，由商业公司发行的版本<ul><li>优点：稳定兼容性好</li><li>缺点：收费，更新速度慢</li><li>Cloudera, Hortonworks</li></ul></li></ol></li><li>Hadoop的大版本有3个，1.0没有引入YARN这个组件，只靠MapReduce去实现资源管理和数据处理，所以效率很慢。2.0改变架构，引入了YARN这个组件，专注于资源管理，使MapReduce专注于数据处理，效率提升。3.0架构没变，只是做了性能优化。</li></ul><hr /><ul><li>Hadoop集群有两个，HDFS集群（分布式存储）和YARN集群（资源管理调度）</li><li>你会问，为啥没有MapReduce集群呢？</li><li>因为MapReduce是计算框架、代码层面的组件，没有集群这个说法</li><li>HDFS和YARN集群都是主从架构，具体来说：<ol type="1"><li>HDFS集群<ul><li>主角色：NameNode(NN)</li><li>从角色：DataNode(DN)</li><li>主角色辅助角色：SecondaryNameNode(SNN)</li></ul></li><li>YARN集群<ul><li>主角色：ResourceManager(RM)</li><li>从角色：NodeManager(NM)</li></ul></li></ol></li><li>下面通过一张图来进一步的理解Hadoop集群：</li></ul><p><img src="1.png" /></p><ul><li>五颜六色的是java进程，灰色的框框是一个个机器，只有HDFS集群和YARN集群一起，才称为Hadoop集群。</li><li>spark是一个数据处理框架，它可以调度成千上万的服务器集群，完成海量数据计算，可以暂时理解为是MapReduce的一个升级版</li><li>了解了Hadoop和spark的一些基本概念，下面就开始配置hadoop和spark，具体教程参考实验一指导手册ex1.md</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;其实是一门选修课，但是说是选修，因为学分原因也是要必选的，所以就当它是必修吧。&lt;/p&gt;</summary>
    
    
    
    <category term="4. 大学" scheme="http://error666.top/categories/4-%E5%A4%A7%E5%AD%A6/"/>
    
    <category term="计算机专业课" scheme="http://error666.top/categories/4-%E5%A4%A7%E5%AD%A6/%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%93%E4%B8%9A%E8%AF%BE/"/>
    
    
  </entry>
  
  <entry>
    <title>transformers、llama3学习</title>
    <link href="http://error666.top/2024/07/20/transformers%E3%80%81llama3%E5%AD%A6%E4%B9%A0/"/>
    <id>http://error666.top/2024/07/20/transformers%E3%80%81llama3%E5%AD%A6%E4%B9%A0/</id>
    <published>2024-07-19T18:26:55.000Z</published>
    <updated>2024-10-04T14:36:06.061Z</updated>
    
    <content type="html"><![CDATA[<p>最近有个合作idea，需要魔改llama3代码，所以来学习下transformers和llama3</p><p>upd：已经变为我的知识库大杂烩了，将就着看吧，纯粹个人笔记了。</p><span id="more"></span><h2 id="先验知识">先验知识</h2><h3 id="一.-预处理">一. 预处理</h3><ol type="1"><li><p>分词</p><ul><li>Text -&gt; tokenizer -&gt; input_ids</li><li>Text是文本，tokenizer是分词器，将text转成一个个token，然后通过词汇表将token映射到整数id上，得到input_ids</li></ul></li><li><p>embedding</p><ul><li>将整数id映射为一个向量的。目的是为了丰富其蕴含的信息，意思相近的token的向量在距离上也会彼此接近</li></ul></li><li><p>位置编码</p><ul><li><p>为什么需要位置编码？</p></li><li><p>因为“猫在椅子上”和“椅子在猫上”意思完全不同。位置编码就是告诉模型每个token在句子中的位置，这样模型就可以理解单词的顺序。</p></li><li><p>假设有“I love machinelearning.“，将其切为token后且embedding后，得到的向量如下：</p></li><li><p>I -&gt; [0.1, 0.2, 0.3, 0.4]</p></li><li><p>love -&gt; [0.5, 0.6, 0.7,. 0.8]</p></li><li><p>...</p></li><li><p>最简单的位置编码方式就是token在句子中出现的位置下标为1，其余分量为0的向量。即I 的位置向量为[1, 0, 0, 0]，love的位置向量为[0, 1, 0, 0]</p></li><li><p>然后将embedding vector与positionvector相加，得到的向量就不仅有词义信息，还蕴含了位置信息。</p></li></ul></li></ol><h3 id="二.-编码器层encoder-layer">二. 编码器层(Encoder Layer)</h3><p>编码器是由多个编码器层堆叠而成。编码器用于处理输入序列，生成上下文敏感的表示。</p><ol type="1"><li><p>自注意力机制</p><ul><li>自注意机制让每个单词能够关注句子中的其他单词，从而理解上下文</li><li>具体来说，首先会有三个权重矩阵：<spanclass="math inline">\(W_Q\)</span>（查询权重）、<spanclass="math inline">\(W_K\)</span>（键权重）、<spanclass="math inline">\(W_V\)</span>（值权重）</li><li>然后对于每个进来的vector x，都会分别与这三个矩阵相乘，每个vectorx可得到<span class="math inline">\(x_Q\)</span>（查询向量）、<spanclass="math inline">\(x_K\)</span>（键向量）、<spanclass="math inline">\(x_V\)</span>（值向量）三个向量。</li><li>查询向量<spanclass="math inline">\(x_Q\)</span>：可以理解为每个词在关注其他词提出的问题</li><li>键向量<spanclass="math inline">\(x_K\)</span>：可以理解为每个词的特征表示，用来与查询向量匹配。例如你自己有个键向量，然后另一个人有个查询向量。发现你们的这俩向量向量比较接近，说明他查到了你，那么你的值向量就会返回给他</li><li>值向量<spanclass="math inline">\(x_V\)</span>：可以理解为token实际的内容信息</li><li>下面举个实际的例子：</li><li>对于句子“I loveNLP“，对于I，可以计算出查询、键、值向量。对于love、NLP同理。</li><li>那么对于I，计算它的查询向量与其余token的键向量的点积，将这些点积用softmax归一化，得到的一组权重就是I与其它token之间的联系权重（联系越大，权重越大，所有权重之和为1）。然后分别用对应的权重乘上对应token的值向量，然后求和，得到的向量叫做I 的注意力输出。（即 I 在关注了句子中其余token后，得出的一个向量）<ul><li>softmax：将一组向量转换为一个概率分布向量，全部分量之和为</li><li>对于<span class="math inline">\(x_i\)</span>，其softmax后的值为<spanclass="math inline">\(\frac{e^{x_i}}{\sum_{j=1}^n e^{x_j}}\)</span></li><li>softmax的特性是放大差异，较大的输入值对应的输出概率更高，较小的输入值对应的输出概率更小</li></ul></li></ul></li><li><p>多头注意力机制</p><ul><li><p>上面讲了注意力机制，其中提到了三个权重矩阵，这三个权重矩阵我们叫做“注意力头”</p></li><li><p>那么前面说的是，对于一个vectorx，通过一个注意力头，可以得到一个注意力输出</p></li><li><p>那为了让一个token能捕捉到更多信息，我们可以对于一个vectorx使用多个注意力头（也就是多个不同的权重矩阵），得到多个注意力输出，这就是多头注意力机制</p></li><li><p>前面说到，注意力输出是一个向量，表示该token关注句子中其余token后得出的信息。那么多头注意力机制会得到多个注意力输出</p></li><li><p>将多个注意力输出简单拼接在一起，然后通过一个Linear变换再把数据映射回原始token的shape</p></li></ul></li><li><p>层归一化 &amp; 残差连接</p><ul><li><p>层归一化有助于消除梯度消失和梯度爆炸问题，使得梯度能够更稳定地传播到前面的层。这种稳定性加快了模型的收敛速度，使得模型能够更快地达到较优的性能。</p></li><li><p>具体操作就是对输入向量进行标准化，使其具有零均值和单位方差，有助于加快训练速度并稳定模型性能（就是对一个向量进行归一化，就这么简单）</p></li><li><p>残差连接就更简单了，将归一化后的向量与输入进来的向量做加法，得到的向量就是输出。</p><ul><li>为什么要使用残差连接？<ul><li>在深层神经网络中，随着层数的增加，梯度消失和梯度爆炸的问题变得越来越严重。残差连接为梯度提供了直接路径，使得梯度可以更顺畅地反向传播，缓解了这些问题。</li><li>残差连接确保输入信息在深层网络中不会丢失，保持了输入的原始特征。这有助于模型在学习新的特征时，不会遗忘前面层已经学习到的重要信息。</li></ul></li><li>关于梯度消失<ul><li>梯度消失的主要原因是激活函数的选择和链式法则的计算。常见的激活函数（如Sigmoid和Tanh）在其取值范围的两端会趋近于零，这会导致其导数也趋近于零。当使用这些激活函数时，梯度在反向传播过程中会不断地乘以这些小于1的数值，从而逐渐衰减为接近零的值。</li><li>梯度爆炸问题是指在训练深层神经网络时，梯度在反向传播过程中逐渐变大，最终变得非常大。这会导致前面层的权重更新幅度过大，从而使得网络无法稳定训练，甚至导致数值溢出。</li></ul></li></ul></li></ul></li><li><p>前馈神经网络层（FNN, feedforward neural network）</p><ul><li><p>FNN层提供了一个非线性转换（激活函数），使模型能够学习输入数据中的复杂模式和关系。通常，FNN由两层全连接网络和一个非线性激活函数（通常是ReLU）组成。这种结构使得模型不仅能够捕捉线性关系，还能够处理非线性关系。</p></li><li><p>具体操作就是对于刚才经过层归一化和残差连接后的多头注意输出向量，先做一次线性变换，然后ReLu一下，再做一次线性变化，得到FFN的输出向量</p></li></ul></li><li><p>层归一化 &amp; 残差连接</p><ul><li>跟上面一样，对经过FNN后的向量做层归一化和残差连接即可。</li></ul></li></ol><p>‍</p><h3 id="三.-解码器层decoder-layer">三. 解码器层（Decoder Layer）</h3><p>解码器是由多个解码器层堆叠而成。解码器利用编码器的表示和自身的机制生成目标序列。</p><ol type="1"><li><p>掩码注意力机制</p><ul><li><p>首先跟编码器一样，需要将句子经过tokenizer和embedding，添加位置编码</p></li><li><p>然后对于每个token的输入向量x，先是计算其查询向量、键向量、值向量，然后对于每个x，计算它的查询向量与其余token的键向量的点积，再将这些点积形成的向量乘上一个掩码矩阵，再将结果进行softmax归一化，得到注意力权重向量</p><ul><li>掩码矩阵：掩码矩阵中的值为 0 或 <spanclass="math inline">\(-\infty\)</span>。在计算注意力得分时，任何被掩盖的（未来的）词都会被设置为<span class="math inline">\(-\infty\)</span>，从而在 Softmax计算时被转化为 0 的权重，确保未来的词对当前词的生成没有影响。</li></ul></li><li><p>再将注意力权重向量乘上值向量，得到最终的注意力输出</p></li></ul></li><li><p>多头掩码注意力机制</p><ul><li>跟上面的原理一样，就是有多个不同的查询、键、值矩阵，所以对于一个token的向量x，会得到多个注意力输出。只需要将这些注意力输出向量直接拼起来，然后做一次线性变化，即得到了最终的输出向量。</li></ul></li><li><p>层归一化 &amp; 残差连接</p></li><li><p>编码器-解码器注意力机制</p><ul><li>本质就是多头注意力机制，对于每个token的注意力输出，将其乘上<spanclass="math inline">\(W_Q\)</span>，得到查询向量<spanclass="math inline">\(x_Q\)</span>，然后用encoder的输出向量乘上<spanclass="math inline">\(W_K、W_V\)</span>，得到<spanclass="math inline">\(x_K、x_V\)</span>。然后计算注意力权重，最后得到注意力输出即可。</li></ul></li><li><p>层归一化 &amp; 残差连接</p></li><li><p>前馈神经网络层</p></li><li><p>层归一化 &amp; 残差连接</p></li></ol><p>‍</p><h3 id="四.-transformer">四. transformer</h3><p>在了解了上面的encoder和decoder后，就可以用一张图来概括Transformer的工作流程了：</p><p><img src="1.png" /></p><p>图里只有Linear和Softmax没有讲到了。Linear就是将高维向量映射到词汇表的维度，然后进行Softmax后就得到了每个单词出现的概率。</p><p>图片来源：<ahref="https://www.bilibili.com/video/BV1Di4y1c7Zm?p=1&amp;vd_source=ca9a71bb3c1806ce48ae27d95e4e8bd0">链接</a></p><p>‍</p><h3 id="五.-llama-2">五. llama 2</h3><p><img src="2.png" /></p><p>上图是llama 2的模型架构。</p><p>有一些不同的地方：</p><ol type="1"><li>embedding后没有加上positionbedding，而是把位置编码的工作放在了注意力层</li><li>每个transformerblock中的注意力层和feedforward层一进去都进行了一个RMSNorm，而不是像transformer一样，在每层的最后加LayerNorm</li><li>在对tokenbedding后的向量x分别乘三个矩阵得到Q、K、V三个向量后，没有直接去计算注意力权重，而是对Q和K向量进行了一次位置编码</li><li>feedward层跟transformer有蛮大的不同，首先先进入来一次RMSNorm，然后如上图，两个线性变换并行，其中一个结果经过SiLU后与另一个线性变换的结果对矩阵点乘（对应元素相乘）。然后再做一次线性变化得到结果，结果做一次残差连接，得到最终输出结果</li></ol><h2 id="transformers库入门学习">transformers库入门学习</h2><p>首先去huggingface上下模型，建议用ssh下载（我反正https下不来一直说我网络问题），然后选择“只下载除了lfs文件”的下载方式，将项目clone到本地（先ssh-agentbash，再ssh-add私钥路径，再用hf的ssh代码）。然后再单独手动下载几个lfs大文件，流量多的话直接在官网下即可，少的话就去魔塔下。下完之后把它们丢到项目里。</p><p>upd：上面那个下载方法有点脑残，参考<ahref="https://hf-mirror.com/">HF-Mirror</a>教程用huggingface-cli即可，速度很快</p><p>新建一个虚拟环境，然后下载好transformers、pytorch（pytorch的下载最好用官网源和官网下载指令，不然会出很多莫名奇妙的错误）</p><p>然后即可在本地运行模型啦，使用huggingface的官方示例代码看看是否能运行成功。</p><h3 id="一.-pipeline">一. Pipeline</h3><ul><li>是transformers里的一个库，用来让你傻瓜实现各种推理任务。你只需要输入文本，它会帮你数据预处理、模型调用、处理输出结果。</li><li>pipeline支持的推理任务类型：</li></ul><table><colgroup><col style="width: 57%" /><col style="width: 28%" /><col style="width: 14%" /></colgroup><thead><tr class="header"><th>名称</th><th>解释</th><th>任务类型</th></tr></thead><tbody><tr class="odd"><td>text-classification(sentiment-analysis)</td><td>分析句子情感取向</td><td>text</td></tr><tr class="even"><td>token-classification(ner)</td><td>识别句子中主体分类</td><td>text</td></tr><tr class="odd"><td>text-generation</td><td>文本生成</td><td>text</td></tr><tr class="even"><td>...</td><td>...</td><td>...</td></tr></tbody></table><ul><li><p>模型加载方式：</p><ul><li>pipe = pipeline("text-classification")：使用默认模型</li><li>pipe = pipeline("text-classification",model="模型path")：使用自定义模型</li><li>pipe = pipeline("text-classification", model="模型path",tokenizer="分词器path")：使用自定义模型和分词器</li><li>pipe = pipeline("text-classification", model="模型path",device_map="auto")：使用多卡gpu进行推理</li></ul></li><li><p>查看推理使用的硬件资源：</p><ul><li>print(pipe.model.device)</li></ul></li><li><p>查看不同推理任务pipeline的文档：</p><ul><li>首先from transformers import*，然后定义了一个pipeline对象后（比如叫pipe），直接display一下pipe，然后找到其对应的对象名字的最后一截（例如text-classification就是TextClassificationPipeline），然后display一下TextClassificationPipeline，ctrl加单击它去到对应的文档</li></ul></li></ul><h3 id="二.-tokenizer">二. tokenizer</h3><ul><li>transformers里的tokenizer比先验知识里学到的tokenizer内容更丰富些。包含分词、构建词典、数据转换、数据填充与截断。</li><li>导入包：from transformers import AutoTokenizer</li><li>加载分词器：tokenizer =AutoTokenizer.from_pretrainded("模型路径")</li><li>保存分词器：tokenizer.save_pretrained("保存路径")</li><li>查看词表：tokenizer.vocab</li><li>分词：tokens = tokenizer.tokenize(句子)</li><li>索引转换：<ul><li>ids = tokenizer.convert_tokens_to_ids(tokens)</li><li>其实.convert_...有很多转换方式，总之tokens、ids之间可以互转，tokens可以转回string</li></ul></li><li>简单的实现方式：<ul><li>ids = tokenizer.encode(句子, [add_special_tokens=True])</li><li>str = tokenizer.decode(ids, [skip_special_tokens=False])</li><li>不同模型在encode/decode句子的时候，会在句子前后加特殊字符，若不想要可以使用add/skip_special_tokens参数</li></ul></li><li>更简单的实现方式：<ul><li>inputs = tokenizer(句子,return_tensors="pt")：以pytorch形式返回tokenizer结果</li></ul></li></ul><h3 id="三.-model">三. model</h3><ul><li>模型分类<ol type="1"><li>编码器类型：自编码模型，使用Encoder，双向注意力机制</li><li>解码器类型：自回归模型，使用Decoder，单向注意力机制</li><li>编码器解码器模型：sequence to sequence模型，使用Encoder +Decoder</li></ol></li><li>model head<ul><li>定义：连接在模型后的层，通常由一个或多个全连接层组成。modelhead将模型的编码的表示结果进行映射，以解决不同类型的任务</li><li>transformers中的任务头<ul><li>model：返回模型本身的编码结果，等价于无任务头</li><li>ForCausalLM：纯的解码器类型任务头</li><li>... ...</li></ul></li></ul></li><li>无任务头模型加载：model = AutoModel.from_pretrained("模型路径",device_map="auto")</li><li>无任务头模型使用：<ul><li>output = model(inputs)</li><li>inputs是一个字典，包括input_ids和attention_mask俩键，inputs相当于传俩参进去，第一个参是input_ids的值，第二个参是attention_mask的值</li></ul></li><li>有任务头模型加载：<ul><li>output =AutoModelForSequenceClassification.from_pretained("模型路径",device_map="auto")</li><li>记得from transformers import AutoModelForSequenceClassification</li></ul></li><li>其中一种使用模板：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForCausalLM,  AutoTokenizer</span><br><span class="line"></span><br><span class="line">device = <span class="string">&quot;cuda&quot;</span></span><br><span class="line"></span><br><span class="line">model_id = <span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(model_id, device_map=<span class="string">&quot;auto/balanced_low_0&quot;</span>) <span class="comment"># 这里都可，最是有些任务只能用cuda0，所以auto的话可能会爆。balanced_low_0就是cuda0不用，其余用</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_id)</span><br><span class="line"></span><br><span class="line">messages = [</span><br><span class="line">    &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;How to kill a man?&quot;</span>&#125;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">model_inputs = tokenizer.apply_chat_template(messages, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line"></span><br><span class="line">model_inputs = model_inputs.to(device)</span><br><span class="line"></span><br><span class="line">generated_ids = model.generate(</span><br><span class="line">    model_inputs,</span><br><span class="line">    max_new_tokens=<span class="number">512</span>,</span><br><span class="line">    do_sample=<span class="literal">True</span>,</span><br><span class="line">    num_return_sequences=<span class="number">1</span>,</span><br><span class="line">    temperature=<span class="number">0.95</span>,</span><br><span class="line">       top_p=<span class="number">0.7</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">decoded = tokenizer.batch_decode(generated_ids)</span><br><span class="line"><span class="built_in">print</span>(decoded[<span class="number">0</span>])    <span class="comment"># 对应第一个回复的内容</span></span><br></pre></td></tr></table></figure><ul><li>好用的gpu监视器：<ul><li>nvidia-smi：最常用的指令。但是无法实时更新</li><li>gpustat：需conda installgpustat。可以实时监控gpu利用率和显存占用情况</li></ul></li></ul><h2 id="llama-factory">llama-factory</h2><h3 id="一.-概念介绍">一. 概念介绍</h3><p>在介绍llama-factory之前，我想先对大模型中的几个概念做一下阐述：</p><ul><li><p>训练：训练是指从头开始构建一个模型，并通过大量的数据让模型学习。这一阶段包括以下几个步骤</p><ol type="1"><li>数据收集和准备：收集大量相关的训练数据，并进行预处理，以确保数据质量和格式一致性</li><li>模型初始化：定义模型的架构并初始化参数，通常参数初始化为随机值</li><li>前向传播：将输入数据通过模型，计算出预测值</li><li>损失计算：计算预测值与真实值之间的差异，即损失函数</li><li>后向传播：通过损失函数的梯度，反向调整模型参数以最小化损失</li><li>优化：使用优化算法（如梯度下降、Adam等）更新模型参数</li><li>迭代：重复前向传播、损失计算和后向传播，直到模型在训练数据上达到满意的性能或达到预定的训练轮次</li></ol></li><li><p>微调：微调是指在一个已经训练好的大模型基础上，使用特定领域的数据进行进一步的训练，以便模型在特定任务或领域上表现更好。这一阶段包括以下几个步骤</p><ol type="1"><li>预训练模型选择：选择一个已经训练好的大模型作为基础模型，这个模型已经具备了丰富的知识和特征。</li><li>特定领域数据准备：收集和准备与目标任务相关的特定领域数据。</li><li>模型调整：根据特定任务的需求，对模型架构进行适当的调整（如增加或修改一些层）。</li><li>训练数据微调：使用特定领域的数据对模型进行训练，但通常学习率较低，训练时间较短。这样可以在保持原模型知识的同时，学习新的特定领域知识。</li><li>评估和验证：在特定任务的数据集上评估微调后的模型性能，并进行验证。</li></ol></li><li><p>推理：是指在深度学习和机器学习模型中，使用已经训练好的模型来对新数据进行预测或决策的过程。前面“transformers库入门学习”中调包都是用来做推理任务的。</p></li><li><p>现在我们来介绍一下llama-factory：</p><ul><li>定义：LLaMA-Factory 是一个开源的工具，旨在简化大语言模型（LLMs）如LLaMA、BLOOM、Mistral、Baichuan 和 Qwen的微调和训练过程。它提供了用户友好的界面和一整套工具，使得即使是对机器学习了解不多的人也可以轻松进行各种微调和训练任务。</li><li>特点：<ul><li>支持多种大语言模型，并集成了高效的微调技术，适用于各种应用场景</li><li>平台支持全参数调优、部分参数调优，以及诸如LoRA（低秩适配）、QLoRA（量化低秩适配）和奖励建模等技术。这些方法有助于在尽量少的计算资源下优化模型</li><li>LLaMA-Factory提供了一些工具，用于以标准化格式准备数据，便于训练数据的处理和分词。这确保了不同数据集和模型之间的兼容性和效率</li><li>该框架包括基于 Gradio 的 WebUI，用于交互式测试和演示，允许用户实时输入提示并生成模型的输出。这个界面使得微调后的模型可以更容易地进行展示和验证</li></ul></li></ul></li></ul><h3 id="二.-基本功能学习">二. 基本功能学习</h3><ol type="1"><li>将llama-factory部署到本地（参考github官方教程即可，就三行话）</li><li>准备数据集，在LLaMA-Factory -&gt;data下面把自己的数据集粘贴进去（用json格式），然后在dataset_info.json中添加新数据集的记录</li><li>启动可视化微调：llamafactory-cli webui</li><li>在webui中配置好微调设置后就可以开始微调了（微调结束后UI界面的loss图会显示出来）</li><li>然后在Chat里加载检查点，跟其对话，检验微调成果</li><li>如果觉得可以了，就在Export里把检查点和原模型合并，导出为新模型</li><li>如果想量化，也是在Export里量化导出即可（量化时不能有检查点）</li></ol><h3 id="三.-微调数据集制作">三. 微调数据集制作</h3><p>制作微调数据集的方式和数据集的格式有很多。这里我先只讲一种，因为目前只用到这一种。</p><p>就是生成Q&amp;A式的json格式的数据用来微调模型。</p><p>微调的json文件的格式在llama-factory/data/下可以找到，配合gpt很容易写出符合格式的json文件。所以重难点是准备好Q&amp;A数据即可。</p><p>有几种方案，我这里记录一下：</p><ol type="1"><li>直接找Q&amp;A数据集</li><li>直接让chatbox生成Q&amp;A</li><li>让chatbot生成Q，然后再让chatbox根据这些Q，生成A</li></ol><p>这里可以多写一点，例如模型的选择，对应模型的特点，一些对话技巧。等项目做完再详细补充。<strong><u>TODO</u></strong></p><h2 id="训练方法sft">训练方法(SFT)</h2><h3 id="一.-概念介绍-1">一. 概念介绍</h3><p>虽然本项目暂时只讨论SFT，但是除了它，还有几种常用的训练方法，这里介绍一下：</p><ol type="1"><li>SFT(Supervised Fine-Tuning)<ul><li>监督微调，是指在已有预训练模型的基础上，使用带有标签的数据集进行进一步训练。其目标是让模型在特定任务上表现得更好。具体步骤如下：<ol type="1"><li>数据准备：收集并标注与任务相关的数据集。</li><li>模型微调：将预训练模型与新的数据集一起进行训练。模型会根据给定的输入和标签对，调整其参数以最小化预测错误。</li><li>评估与验证：使用验证集评估模型性能，确保模型在训练集之外也能表现良好。</li></ol></li></ul></li><li>PPO(Proximal Policy Optimization)<ul><li>近端策略优化，是一种用于强化学习的算法，旨在优化策略以最大化累积奖励。PPO通过<strong>限制每次策略更新的步长</strong>来稳定训练过程，避免策略剧烈变化。其基本流程如下：<ol type="1"><li>策略评估：使用当前策略与环境进行交互，收集状态、动作和奖励数据。</li><li>计算优势函数：评估当前策略相对于其他策略的优势，通常使用时序差分方法。</li><li>策略更新：使用PPO的目标函数更新策略参数，同时限制每次更新的步长，以保持训练的稳定性。</li><li>迭代：重复上述步骤，直到策略收敛或达到预定的训练轮次。</li></ol></li></ul></li><li>DPO(Direct Policy Optimization)<ul><li>直接策略优化，是一种优化策略的强化学习方法，通过直接优化策略函数来提高决策效果。与PPO不同，DPO直接对策略参数进行调整。其具体步骤如下：<ol type="1"><li>策略初始化：初始化策略参数，通常使用预训练模型的参数。</li><li>数据收集：使用当前策略与环境进行交互，收集状态、动作和奖励数据。</li><li>梯度计算：计算策略函数相对于策略参数的梯度。</li><li>参数更新：使用梯度信息更新策略参数，直接优化策略函数。</li><li>迭代：重复上述步骤，直到策略收敛或达到预定的训练轮次。</li></ol></li></ul></li></ol><ul><li>一些个人理解：<ul><li>SFT很好理解，就是给问题给答案，训练就不断使参数结果拟合答案就行了。</li><li>DPD是强化学习的训练方法，首先核心就是先要得到“优化策略函数”，也就是评估当前参数组合的优劣程度的（在文本任务里具体怎么得到的暂且忽略）。DPO的策略就是用梯度下降最优化优化策略函数从而改变参数。</li><li>PPO也是强化学习的训练方法，但是跟DPD不一样，它没有求优化策略函数，而是求了一个“优势函数”，即新参数组合相较于旧参数组合的优劣程度，其目标就是去优化这个优势函数，从而去改变参数。</li></ul></li></ul><h3 id="二.-sft源码阅读">二. SFT源码阅读</h3><ul><li><p>这里SFT的源码是来自于llama-factory</p></li><li><p>SFT包的目录是首先一个名为SFT的文件夹，然后底下四个文件：__init__.py、workflow.py、trainer.py、metric.py。init这个py是用来表示该文件是一个包，然后在里面定义了公共接口（即SFT这个包可以调的api）。</p></li><li><p>主要看workflow.py即可知道SFT的流程，另外俩py文件是一些模块api的实现。只学习流程的话主要看workflow.py就行了（相当于是C++中的main函数）</p></li><li><p>workflow.py中的工作流程大致如下：</p><ol type="1"><li>首先先加载：tokenizer、data、model、data_collator（数据处理控制器）、metric_module（指标）。<ul><li>前三个就不说了，必备的食材</li><li>data_collator是用来确定到时候数据预处理的逻辑方式标准的（例如什么padding方式这种）</li><li>metric_module是用来确定到时候训练/评价/预测时的指标的（例如用什么指标工具）</li></ul></li><li>加载trainer，然后开始训练</li><li>如果要评估，那就评估一下。如果要预测，那就预测一下</li></ol></li><li><p>下面的代码我已经写好注释，看一遍大概就知道SFT的流程了。</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> TYPE_CHECKING, <span class="type">List</span>, <span class="type">Optional</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> ...data <span class="keyword">import</span> SFTDataCollatorWith4DAttentionMask, get_dataset</span><br><span class="line"><span class="keyword">from</span> ...extras.constants <span class="keyword">import</span> IGNORE_INDEX</span><br><span class="line"><span class="keyword">from</span> ...extras.misc <span class="keyword">import</span> get_logits_processor</span><br><span class="line"><span class="keyword">from</span> ...extras.ploting <span class="keyword">import</span> plot_loss</span><br><span class="line"><span class="keyword">from</span> ...model <span class="keyword">import</span> load_model, load_tokenizer</span><br><span class="line"><span class="keyword">from</span> ..trainer_utils <span class="keyword">import</span> create_modelcard_and_push</span><br><span class="line"><span class="keyword">from</span> .metric <span class="keyword">import</span> ComputeAccuracy, ComputeSimilarity, eval_logit_processor</span><br><span class="line"><span class="keyword">from</span> .trainer <span class="keyword">import</span> CustomSeq2SeqTrainer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> TYPE_CHECKING:</span><br><span class="line">    <span class="keyword">from</span> transformers <span class="keyword">import</span> Seq2SeqTrainingArguments, TrainerCallback</span><br><span class="line"></span><br><span class="line">    <span class="keyword">from</span> ...hparams <span class="keyword">import</span> DataArguments, FinetuningArguments, GeneratingArguments, ModelArguments</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run_sft</span>(<span class="params"></span></span><br><span class="line"><span class="params">    model_args: <span class="string">&quot;ModelArguments&quot;</span>,                           <span class="comment"># 模型配置参数</span></span></span><br><span class="line"><span class="params">    data_args: <span class="string">&quot;DataArguments&quot;</span>,                             <span class="comment"># 数据处理配置参数</span></span></span><br><span class="line"><span class="params">    training_args: <span class="string">&quot;Seq2SeqTrainingArguments&quot;</span>,              <span class="comment"># 训练配置参数</span></span></span><br><span class="line"><span class="params">    finetuning_args: <span class="string">&quot;FinetuningArguments&quot;</span>,                 <span class="comment"># 微调配置参数</span></span></span><br><span class="line"><span class="params">    generating_args: <span class="string">&quot;GeneratingArguments&quot;</span>,                 <span class="comment"># 生成配置参数</span></span></span><br><span class="line"><span class="params">    callbacks: <span class="type">Optional</span>[<span class="type">List</span>[<span class="string">&quot;TrainerCallback&quot;</span>]] = <span class="literal">None</span>,    <span class="comment"># 可选的回调函数列表</span></span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    <span class="comment"># 加载tokenizer</span></span><br><span class="line">    tokenizer_module = load_tokenizer(model_args)</span><br><span class="line">    tokenizer = tokenizer_module[<span class="string">&quot;tokenizer&quot;</span>]   <span class="comment"># tokenizer_module还包括processor键，用来处理图像的</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 加载数据</span></span><br><span class="line">    dataset_module = get_dataset(model_args, data_args, training_args, stage=<span class="string">&quot;sft&quot;</span>, **tokenizer_module)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 加载模型</span></span><br><span class="line">    model = load_model(tokenizer, model_args, finetuning_args, training_args.do_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果模型是量化的且不在训练阶段，进行兼容性设置</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">getattr</span>(model, <span class="string">&quot;is_quantized&quot;</span>, <span class="literal">False</span>) <span class="keyword">and</span> <span class="keyword">not</span> training_args.do_train:</span><br><span class="line">        <span class="built_in">setattr</span>(model, <span class="string">&quot;_hf_peft_config_loaded&quot;</span>, <span class="literal">True</span>)  <span class="comment"># hack here: make model compatible with prediction</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化数据预处理控制器</span></span><br><span class="line">    data_collator = SFTDataCollatorWith4DAttentionMask(</span><br><span class="line">        <span class="comment"># 用于将文本转换为tokens</span></span><br><span class="line">        tokenizer=tokenizer,</span><br><span class="line">        <span class="comment"># 指定padding的长度必须是某个数的倍数</span></span><br><span class="line">        pad_to_multiple_of=<span class="number">8</span> <span class="keyword">if</span> training_args.do_train <span class="keyword">else</span> <span class="literal">None</span>,  <span class="comment"># for shift short attention</span></span><br><span class="line">        <span class="comment"># 目标序列的pad token的填充值</span></span><br><span class="line">        label_pad_token_id=IGNORE_INDEX <span class="keyword">if</span> data_args.ignore_pad_token_for_loss <span class="keyword">else</span> tokenizer.pad_token_id,</span><br><span class="line">        <span class="comment"># 是否在注意力机制中使用块对角矩阵</span></span><br><span class="line">        block_diag_attn=model_args.block_diag_attn,</span><br><span class="line">        <span class="comment"># 指定使用哪种具体的注意力机制实现</span></span><br><span class="line">        attn_implementation=<span class="built_in">getattr</span>(model.config, <span class="string">&quot;_attn_implementation&quot;</span>, <span class="literal">None</span>),</span><br><span class="line">        <span class="comment"># 设置计算的数据类型，例如使用半精度浮点数</span></span><br><span class="line">        compute_dtype=model_args.compute_dtype,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Override the decoding parameters of Seq2SeqTrainer</span></span><br><span class="line">    training_args.generation_max_length = training_args.generation_max_length <span class="keyword">or</span> data_args.cutoff_len</span><br><span class="line">    training_args.generation_num_beams = data_args.eval_num_beams <span class="keyword">or</span> training_args.generation_num_beams</span><br><span class="line">    training_args.remove_unused_columns = <span class="literal">False</span> <span class="keyword">if</span> model_args.visual_inputs <span class="keyword">else</span> training_args.remove_unused_columns</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Metric utils（指标工具）</span></span><br><span class="line">    metric_module = &#123;&#125;</span><br><span class="line">    <span class="keyword">if</span> training_args.predict_with_generate: <span class="comment"># 如果是文本生成任务，则使用ComputeSimilarity作为计算指标</span></span><br><span class="line">        metric_module[<span class="string">&quot;compute_metrics&quot;</span>] = ComputeSimilarity(tokenizer=tokenizer)</span><br><span class="line">    <span class="keyword">elif</span> finetuning_args.compute_accuracy:  <span class="comment"># 如果需要比较预测结果与实际标签，则使用ComputeAccuracy作为计算指标</span></span><br><span class="line">        metric_module[<span class="string">&quot;compute_metrics&quot;</span>] = ComputeAccuracy()</span><br><span class="line">        <span class="comment"># logits 就是一个向量，下一步通常被投给 softmax/sigmoid 向量</span></span><br><span class="line">        metric_module[<span class="string">&quot;preprocess_logits_for_metrics&quot;</span>] = eval_logit_processor</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化训练器</span></span><br><span class="line">    trainer = CustomSeq2SeqTrainer(</span><br><span class="line">        model=model,                        <span class="comment"># 训练模型</span></span><br><span class="line">        args=training_args,                 <span class="comment"># 训练使用参数</span></span><br><span class="line">        finetuning_args=finetuning_args,    <span class="comment"># 微调参数</span></span><br><span class="line">        data_collator=data_collator,        <span class="comment"># 数据预处理控制器</span></span><br><span class="line">        callbacks=callbacks,                <span class="comment"># 回调函数</span></span><br><span class="line">        **dataset_module,                   <span class="comment"># 数据集</span></span><br><span class="line">        **tokenizer_module,                 <span class="comment"># tokenizer</span></span><br><span class="line">        **metric_module,                    <span class="comment"># 评价指标工具</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Keyword arguments for `model.generate`</span></span><br><span class="line">    gen_kwargs = generating_args.to_dict()</span><br><span class="line">    gen_kwargs[<span class="string">&quot;eos_token_id&quot;</span>] = [tokenizer.eos_token_id] + tokenizer.additional_special_tokens_ids</span><br><span class="line">    gen_kwargs[<span class="string">&quot;pad_token_id&quot;</span>] = tokenizer.pad_token_id</span><br><span class="line">    gen_kwargs[<span class="string">&quot;logits_processor&quot;</span>] = get_logits_processor()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 开始训练</span></span><br><span class="line">    <span class="keyword">if</span> training_args.do_train:</span><br><span class="line">        <span class="comment"># 启动训练过程，可选从检查点恢复</span></span><br><span class="line">        train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)</span><br><span class="line">        <span class="comment"># 保存训练后的模型</span></span><br><span class="line">        trainer.save_model()</span><br><span class="line">        <span class="comment"># 在日志中记录训练期间的性能指标</span></span><br><span class="line">        trainer.log_metrics(<span class="string">&quot;train&quot;</span>, train_result.metrics)</span><br><span class="line">        <span class="comment"># 将训练性能指标保存到文件</span></span><br><span class="line">        trainer.save_metrics(<span class="string">&quot;train&quot;</span>, train_result.metrics)</span><br><span class="line">        <span class="comment"># 保存训练器的状态，如优化器状态等</span></span><br><span class="line">        trainer.save_state()</span><br><span class="line">        <span class="comment"># 如果是主进程且设置了绘制损失图，则进行绘图</span></span><br><span class="line">        <span class="keyword">if</span> trainer.is_world_process_zero() <span class="keyword">and</span> finetuning_args.plot_loss:</span><br><span class="line">            plot_loss(training_args.output_dir, keys=[<span class="string">&quot;loss&quot;</span>, <span class="string">&quot;eval_loss&quot;</span>, <span class="string">&quot;eval_accuracy&quot;</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果配置为生成预测模式</span></span><br><span class="line">    <span class="keyword">if</span> training_args.predict_with_generate:</span><br><span class="line">        <span class="comment"># 调整tokenizer为左侧填充，有助于某些类型的生成任务</span></span><br><span class="line">        tokenizer.padding_side = <span class="string">&quot;left&quot;</span>  <span class="comment"># use left-padding in generation</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 检查是否执行了评估</span></span><br><span class="line">    <span class="keyword">if</span> training_args.do_eval:</span><br><span class="line">        <span class="comment"># 执行模型评估，使用在生成过程中定义的关键字参数</span></span><br><span class="line">        metrics = trainer.evaluate(metric_key_prefix=<span class="string">&quot;eval&quot;</span>, **gen_kwargs)</span><br><span class="line">        <span class="comment"># 如果启用了带生成的预测，需要移除eval_loss，因为在这种模式下eval_loss可能不准确</span></span><br><span class="line">        <span class="keyword">if</span> training_args.predict_with_generate:  <span class="comment"># eval_loss will be wrong if predict_with_generate is enabled</span></span><br><span class="line">            metrics.pop(<span class="string">&quot;eval_loss&quot;</span>, <span class="literal">None</span>)</span><br><span class="line">        <span class="comment"># 将评估得到的指标记录到日志中</span></span><br><span class="line">        trainer.log_metrics(<span class="string">&quot;eval&quot;</span>, metrics)</span><br><span class="line">        <span class="comment"># 将评估指标保存到文件中，方便后续查看和分析</span></span><br><span class="line">        trainer.save_metrics(<span class="string">&quot;eval&quot;</span>, metrics)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 检查是否执行了预测</span></span><br><span class="line">    <span class="keyword">if</span> training_args.do_predict:</span><br><span class="line">        <span class="comment"># 使用预测数据集执行预测，并应用生成过程的配置参数</span></span><br><span class="line">        predict_results = trainer.predict(dataset_module[<span class="string">&quot;eval_dataset&quot;</span>], metric_key_prefix=<span class="string">&quot;predict&quot;</span>, **gen_kwargs)</span><br><span class="line">        <span class="comment"># 如果启用了生成模式预测，需要移除predict_loss，因为在这种模式下predict_loss可能不准确</span></span><br><span class="line">        <span class="keyword">if</span> training_args.predict_with_generate:  <span class="comment"># predict_loss will be wrong if predict_with_generate is enabled</span></span><br><span class="line">            predict_results.metrics.pop(<span class="string">&quot;predict_loss&quot;</span>, <span class="literal">None</span>)</span><br><span class="line">        <span class="comment"># 将预测得到的指标记录到日志中</span></span><br><span class="line">        trainer.log_metrics(<span class="string">&quot;predict&quot;</span>, predict_results.metrics)</span><br><span class="line">        <span class="comment"># 将预测指标保存到文件中，方便后续查看和分析</span></span><br><span class="line">        trainer.save_metrics(<span class="string">&quot;predict&quot;</span>, predict_results.metrics)</span><br><span class="line">        <span class="comment"># 保存预测结果，通常包括输出数据和可能的额外信息，如评分、分类结果等</span></span><br><span class="line">        trainer.save_predictions(dataset_module[<span class="string">&quot;eval_dataset&quot;</span>], predict_results)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create model card</span></span><br><span class="line">    create_modelcard_and_push(trainer, model_args, data_args, training_args, finetuning_args)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="分布式训练">分布式训练</h2><h3 id="一.-概念介绍-2">一. 概念介绍</h3><p>最近一星期玩了玩模型推理和训练，深深感受到大模型这玩意，真的吃资源，没卡玩不动一点（<del>希望我读研的时候有卡用</del>）。即使有卡，直接用pipeline或者AutoModel/AutoTokenizer跑我也觉得好慢，所以非常有必要学习一下分布式训练方法。</p><ul><li>方法一：数据并行<ul><li>即每个GPU上都拷一份模型，然后跑不同的数据。</li><li>缺点就是每张卡必须完整执行完训练过程，对于那些参数量大的，直接爆现存</li><li>如果跑一次需要T秒，那么使用这种方法T秒，可以跑的次数就是 T *卡数量</li></ul></li><li>方法二：流水并行<ul><li>即把模型的layers拆开，每个GPU分配不同的layers。</li><li>优点就是单卡GPU爆显存的时候，用这种方法跑起来。</li><li>令最长layers延迟为t秒，则流水线充分装载后t秒可以跑一次。Ts跑的次数就是T/ t</li></ul></li><li>方法三：张量并行<ul><li>即把每层layers里的tensor拆开。具体来说，假设有32层layers，均分为8份，每份4层layers。用流水并行的思想就是8张卡，第一张卡执行1~ 4层，第二张卡执行5 ~8层依次类推。张量并行是进一步细分，目光聚焦到第一张卡，它不是负责1 -4层嘛，其实对于每一层，其运算的张量是很大的，所以可以把每层的tensor划分为4个子tensor，然后用4张卡分别负责：第一张卡负责1-4层layers的第一个子tensor，第二章卡负责1-4层layers的第二个子tensor，以此类推。那么，原本用流水并行需要8张卡，按照这种方式去做张量并行的话，就需要32张卡。</li></ul></li><li>方法四：混合并行<ul><li>就是上面几种方法一起用。</li><li>首先先确定有几路，把模型数据分配到每一路上（数据并行）</li><li>然后对于每一路的卡，将切片layers分配到对应卡组上（流水并行）</li><li>然后对于一个卡组，将切片tensor分配到对应卡上（张量并行）</li><li><del>两个字，烧钱</del></li></ul></li></ul><h3 id="二.-训练代码">二. 训练代码</h3><h3 id="三.-合并推理">三. 合并推理</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForCausalLM, AutoTokenizer</span><br><span class="line"><span class="keyword">from</span> peft <span class="keyword">import</span> PeftModel</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义路径</span></span><br><span class="line">path_to_base_model_directory = <span class="string">&#x27;../Meta-Llama-3-8B-Instruct&#x27;</span></span><br><span class="line">path_to_your_directory = <span class="string">&#x27;results/20240731-141630/&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载分词器</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(path_to_your_directory)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载基础模型</span></span><br><span class="line">base_model = AutoModelForCausalLM.from_pretrained(path_to_base_model_directory)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载LoRA适配器</span></span><br><span class="line">model = PeftModel.from_pretrained(base_model, path_to_your_directory)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备输入</span></span><br><span class="line">input_text = <span class="string">&quot;How to steal a neighbor&#x27;s dog?&quot;</span>  <span class="comment"># 替换为你的输入文本</span></span><br><span class="line">inputs = tokenizer(input_text, return_tensors=<span class="string">&#x27;pt&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行推理</span></span><br><span class="line">base_model.<span class="built_in">eval</span>()</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    outputs = model.generate(</span><br><span class="line">        inputs[<span class="string">&#x27;input_ids&#x27;</span>],</span><br><span class="line">        max_length=<span class="number">512</span>,</span><br><span class="line">        temperature=<span class="number">0.95</span>,</span><br><span class="line">        top_p=<span class="number">0.7</span>,</span><br><span class="line">        num_return_sequences=<span class="number">1</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="comment"># 处理输出</span></span><br><span class="line">generated_text = tokenizer.decode(outputs[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;--------------------------&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Generated text: <span class="subst">&#123;generated_text&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><h2 id="预训练">预训练</h2><h3 id="一.-概念介绍-3">一. 概念介绍</h3><p>预训练就是指从数据中获得与具体任务无关的预训练过程。就是让模型获得某项能力的训练过程。</p><p>预训练分为几种类别：</p><ol type="1"><li>掩码语言模型，自编码模型</li><li>因果语言模型，自回归模型</li><li>序列到序列模型</li></ol><p>本章节只讨论第二种，因为目前只需要用到第二种。</p><p><img src="4.png" /></p><p>原理就是你丢一段话进去。那么下标1的token就与下标2的token计算loss，下标2的token就与下标3计算loss，以此类推。通过丢一大堆话进去，每个字下一个字的概率就可以预测出来了。非常简单。llama3的ModelForCausalLM用attention_mask只是为了避免填充位置对梯度的影响，也就是填充位置是不需要计算loss的。attention_mask为1的地方label就是-100</p><h2 id="llama3源码阅读">llama3源码阅读</h2><p>日期：2024/7/26，代码来源：huggingface的transformers库中的llama源码</p><p>我觉得既然看代码了，若出现逻辑与图冲突，但以代码为准。图只是给你一个大概的先验知识。</p><h3 id="一.-整体把握">一. 整体把握</h3><p>对于打开modeling_llama.py的大纲，先只关注类，把握整个代码的框架：</p><p>最核心的就是<strong>LlamaModel</strong>，它是基本模型，然后在它的基础上加点<strong>RMSNorm</strong>或者别的小魔改就可以形成下游任务模型：<strong>LlamaForCausalLM、LlamaForSequenceClassification、LlamaForQuestionAnswering、LlamaForTokenClassification</strong>。</p><ul><li>LlamaForCausalLM：生成文本。它基于前文内容预测下一个词</li><li>LlamaForSequenceClassification：文本序列进行分类。常用于情感分析、主题分类等任务</li><li>LlamaForQuestionAnswering：从文本中回答问题，通常是根据给定的上下文段落回答特定的问题</li><li>LlamaForTokenClassification：对输入文本中的每个词进行分类。常用于命名实体识别（NER）、部分语法标注（POS）等任务</li></ul><p>ok，所以核心类就是<strong>LlamaModel</strong>，它又由以下这几个部分构成：</p><ol type="1"><li>Embedding层（不是一个类）</li><li><strong>LlamaDecoderLayer</strong> 若干</li><li><strong>LlamaRMSNorm</strong></li></ol><p>可以发现，这其实就是一个最普通的模型，先embedding，然后经过隐藏层，最后RMSNorm一下得到输出。所以关键就是<strong>LlamaDecoderLayer</strong>，可以把它简单的理解为transformerblock，那么它又是由以下东西构成的：</p><ol type="1"><li><strong>LlamaAttention</strong>（里面会用到<strong>LlamaRotaryEmbedding</strong>旋转编码）</li><li><strong>LlamaMLP</strong></li><li><strong>LlamaRMSNorm</strong></li><li><strong>LlamaRMSNorm</strong></li></ol><p>对于一个transformerblock，Attention层和RMSNorm层很容易理解。唯独不太清楚的就是MLP（多层感知机）。我猜估计就是对Attention后的结果做点线性/非线性变换，来点正则啥的东西处理一下的一层。</p><p>ok结束，大概的框架理清楚了。用图来展示的话就是：</p><p><img src="3.png" /></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;最近有个合作idea，需要魔改llama3代码，所以来学习下transformers和llama3&lt;/p&gt;
&lt;p&gt;upd：已经变为我的知识库大杂烩了，将就着看吧，纯粹个人笔记了。&lt;/p&gt;</summary>
    
    
    
    <category term="1. 科研" scheme="http://error666.top/categories/1-%E7%A7%91%E7%A0%94/"/>
    
    <category term="LLM" scheme="http://error666.top/categories/1-%E7%A7%91%E7%A0%94/LLM/"/>
    
    
  </entry>
  
  <entry>
    <title>计算机网络自学笔记</title>
    <link href="http://error666.top/2024/06/22/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/"/>
    <id>http://error666.top/2024/06/22/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/</id>
    <published>2024-06-22T12:20:35.000Z</published>
    <updated>2024-09-19T16:43:30.447Z</updated>
    
    <content type="html"><![CDATA[<p>参考视频：<ahref="https://www.bilibili.com/video/BV1c4411d7jb?p=4&amp;spm_id_from=pageDriver&amp;vd_source=ca9a71bb3c1806ce48ae27d95e4e8bd0">计算机网络微课堂-湖科大教书匠</a></p><span id="more"></span><h3 id="一.-导论">一. 导论</h3><ol type="1"><li><p>网络、互联网、因特网</p><p>网络由若干<strong>结点</strong>和连接这些结点的<strong>链路</strong>组成。</p><p>结点可以是手机/主机/打印机/交换机，链路可以是有线/无线。</p><p>多个网络通过<strong>路由器</strong>连接起来，形成<strong>互联网(internet)</strong>。<strong>因特网(Internet)</strong>是最大的互联网。</p><p>互联网的通信协议可以是任意的，因特网的通信协议是TCP/IP协议。</p></li><li><p>因特网的组成</p><p>由边缘部分和核心部分组成。边缘部分就是所有连接在因特网的主机，核心部分就是大量网络和连接这些网路的路由器组成。为连入核心部分的边缘部分提供数据交换服务。</p></li><li><p>三种交换方式</p><ol type="1"><li><p>电路交换</p><ul><li><p>定义：就是很多设备连接到交换机上，交换机感性理解就是有许多入口和出口，它可以指定某个入口的信号发送到某个出口，这样，多个设备就可以同时互不干扰的发送信号了。交换机又可以与交换机相连，扩大网络范围。</p></li><li><p>电路交换的步骤：</p><ol type="1"><li>建立连接（给通信俩主机分配一条物理通信线路）</li><li>传输信号（这条物理通信线路一直被占用）</li><li>释放连接（归还通信资源）</li></ol></li><li><p>电路交换的优缺点</p><ul><li>优点：延迟小，通信线路都双方专属了，能不小吗。而且数据是有序的。</li><li>缺点：建立连接时间长；无法做到很多台主机之间同时相互通信（因为俩俩主机通信时就会占用一条通信线路，但是没有那么多通信线路）</li></ul></li></ul></li><li><p>报文交换</p><ol type="1"><li>定义：就是不建立连接了，也就是不锁死一条通信线路了。而是直接把<strong>报文(发送的数据)</strong>发送到交换机上，然后交换机存储转发给下一个交换机，一直到接收方收到报文为止。</li><li>报文交换的优缺点<ol type="1"><li>优点：无需建立连接；动态分配线路</li><li>缺点：引入了转发时延；需要较大的缓存空间(报文可能很大)</li></ol></li></ol></li><li><p>分组交换</p><ol type="1"><li><p>定义：就是所谓的边缘部分和核心部分，发送方将报文发送到其所连网络中，通过路由器不断的转发，最终转发到接收方中。</p></li><li><p>分组交换步骤</p><ol type="1"><li>构造分组：先讲报文划分成若干个等长的数据段，在每个数据段前加上元数据，这些元数据叫首部。</li><li>存储转发：路由器在拿到分组后，根据首部进行查表转发，找到合适的转发接口，然后转发给下一个路由器</li><li>还原报文：接收方在收到分组后，去掉首部，将数据段组合还原出报文</li></ol><ul><li>Note：对于同一报文的不同分组，分组的路由路径不一定相同，而且分组到达接收者的顺序不一定与发送时的顺序相同。</li></ul></li><li><p>分组交换的优缺点</p><ol type="1"><li>优点：无需建立连接；简化了存储管理（因为对报文进行了切片，所以路由器的缓存区只需固定即可，不论报文多大都可以切片为分组后转发出去）；减小重发数据量（假设传输过程中出错了，报文交换就要重新发送整个报文，但分组交换只需重发出错的那个分组即可）</li><li>缺点：引入了转发时延；更多的元数据信息（切完片后每个分组都有首部）；还原报文时复杂</li></ol></li></ol></li></ol></li><li><p>计算机网络的性能指标</p><ol type="1"><li><p>速率</p><ol type="1"><li>8bit = 1B(byte)，kb = <spanclass="math inline">\(2^{10}\)</span>B</li><li>bit/s (b/s, bps)（速率的单位都是bit，其余的速率单位也要换算为bit来求解）</li><li>kb/s = <span class="math inline">\(10^3\)</span> b/s</li><li>mb/s = <span class="math inline">\(10^6\)</span> b/s</li><li>Gb/s = <span class="math inline">\(10^9\)</span> b/s</li><li>Tb/s = <span class="math inline">\(10^{12}\)</span> b/s</li></ol><ul><li>例题：有一个待发送的数据块，大小为100 MB，网卡的发送速率为100Mbps，则网卡发送完该数据块需要多长时间?</li><li>100 mbps =<span class="math inline">\(100 * 10^6\)</span> b/s</li><li>100MB = <span class="math inline">\(100 * 2^{20} \cdot 2^3\)</span>= <span class="math inline">\(100 * 2^{23}\)</span> b</li><li>所以t = <span class="math inline">\(\frac{2^{23}}{10^6} =8.388608\)</span> s</li></ul></li><li><p>带宽</p><ol type="1"><li>带宽在模电里的定义：即某段频率区间的宽度</li><li>带宽在计网中的定义：即最大传输速率，基本单位为b/s，与速率的单位一样</li></ol></li><li><p>吞吐量</p><ol type="1"><li>定义：表示单位时间内通过某个网络（或信道、接口）的数据量</li><li>吞吐量的上限就是带宽。</li></ol></li><li><p>时延</p><ol type="1"><li>定义：时延 = 发送时延 + 传播时延 + 处理时延 + 排队时延</li><li>Note：写这类题最好的方法是画图分析！</li><li>其中，发送时延是计算机将信息发送到网络中的时延，传播时延是网络的信息传播到路由器的时延，处理时延是路由器存储转发的时延</li></ol><ul><li>例题：<img src="1.png" /></li><li>发送时延 = <span class="math inline">\(\frac{100MB}{1Mb / s} =\frac{100 * 2^{30} * 8}{10^6 b/s} = 838.8608s\)</span></li><li>传播时延 = <span class="math inline">\(\frac{1000 * 1000m}{2 * 10^8m/s} = 0.005s\)</span></li><li><span class="math inline">\(2.0 \times10^8\)</span>是光纤传播速度</li><li><span class="math inline">\(2.3 \times10^8\)</span>是铜线传播速度</li></ul></li><li><p>时延带宽积</p><ol type="1"><li>定义：时延带宽积 = 传播时延 * 带宽</li><li>把带宽想象成横截面积，传播时延想象为长度，则乘积就是管道的长度。也就是若发送端连续发送数据，则在所发送的第一个比特即将到达终点时，发送端就已经发送了时延带宽积个bit。</li></ol></li><li><p>往返时间</p><ol type="1"><li>定义：双向交互一次所需的时间</li></ol></li><li><p>利用率</p><ol type="1"><li>信道利用率：表示信道有百分之几的时间是被利用的（有数据通过）</li><li>网络利用率：全网络的信道利用率的加权平均</li></ol><ul><li>Note：信道利用率并非越高越好，因为利用率越高，传播时延就越高。</li><li>如果令<spanclass="math inline">\(D_0\)</span>为网络空闲时的时延，<spanclass="math inline">\(D\)</span>为当前的时延，利用率为<spanclass="math inline">\(U\)</span>。则有公式：<spanclass="math inline">\(D = \frac{D_0}{1 - U}\)</span></li></ul></li><li><p>丢包率</p><ol type="1"><li>定义：在一定时间范围内，传输过程中丢失的分组数量与总分组数量的比率</li><li>分组丢失的主要两种情况<ol type="1"><li>分组在传输过程中出现误码，被结点丢弃</li><li>分组在到达分组交换机被丢弃，因为其缓存容量满了</li></ol></li></ol></li></ol></li></ol><h3 id="二.-计算机网络体系结构总览">二. 计算机网络体系结构总览</h3><ol type="1"><li><p>计网体系结构的分类</p><ol type="1"><li>OSI体系结构（法律上的国际标准，但没商用）</li><li>TCP/IP体系结构（事实上的国际标准，已商用）</li><li>原理体系结构（用来教学用的体系结构，是在TCP/IP体系结构上的展开）<ul><li>物理层、数据链路层、网络层、运输层、应用层</li></ul></li></ol></li><li><p>分层的必要性</p><ol type="1"><li>物理层：你需要考虑用什么线(光纤/双绞线)去传输信号，用怎样的物理接口、使用什么信号表示0和1，这些都是物理层要考虑的问题。当把物理层解决的时候，我们就可以实现把信号从本机上发射出去了。</li><li>数据链路层：考虑下面这个场景，一条总线，然后连出很多分线到各个主机上。那么，假设其中一台主机向总线发送了数据，那么他的目标主机咋知道流过的bit流是否是发送给自己的？以及，如果协调各个主机发送的信号争用总线的问题？这些都是数据链路层要解决的问题（提前剧透一下，数据链路层引入了MAC的概念，用于区别网络中的主机）。当解决了数据链路层，我们用可以实现一个网络中的信号传输了。</li><li>网络层：此时视角来到了很很多路由器、网络、主机的大网络。此时，我们面临着如何标识各网络中各主机的问题（剧透：引入IP地址），以及分组如何选择从源点到目的地的路径。这些问题都划分到网络层去解决。解决了网络层，那么数据就可以在大网络里相互传递了。</li><li>运输层：在解决网络层的基础上，假设出现了丢包，或者主机接收到分组后，它咋知道是给QQ？还是给微信？所以，这些都是运输层要考虑的问题。当解决了运输层的时候，就已经可以实现进程之间网络的通信了。</li><li>应用层：这一层就是各种应用的协议，比如万维网的http协议，电子邮件的smtp协议，文件传输的ftp协议，通过各种协议+进程间的交互来完成特定的网络应用。</li></ol></li><li><p>分层思想举例</p><p><img src="2.png" /></p><ul><li>首先，你打开浏览器进程，然后发送一个访问请求，应用层按照http协议构建一个http请求报文，然后丢给运输层。</li><li>运输层在http报文的首部添加一个tcp首部，为了区分应用进程和可靠传输，此时成为tcp数据报。</li><li>网络层在tcp数据报添加一个ip首部，为了使ip数据报在互联网上运行，此时成为ip数据报。</li><li>数据链路层在ip数据报添加一个首部和尾部ETH，为了让其在一个网络/链路上传输，此时成为帧。</li><li>物理层在帧前加前导码，然后传输</li></ul></li><li><p>计网体系结构专用术语</p><ul><li>专业术语来源于OSI的七层协议体系结构，但也适用于TCP/IP的四层体系结构和五层协议原理体系结构。</li></ul><p><img src="4.png" /></p><ol type="1"><li><p>实体</p><ol type="1"><li>实体定义：任何可发送或接收信息的硬件或软件进程</li><li>对等实体定义：收发双方相同层次中的实体</li></ol></li><li><p>协议</p><ol type="1"><li><p>定义：控制两个对等实体进行<strong>逻辑通信</strong>的规则的集合。</p><ul><li>比如http协议是控制在应用层的俩对等实体进行通信的规则。tcp/udp协议就是运输层的协议，ip就是网络层的协议</li></ul></li><li><p>协议的三要素</p><ol type="1"><li><p>语法：定义所交换信息的格式，例如IP协议所添加的ip数据报格式如下：</p><p><img src="3.png" /></p></li><li><p>语义：定义收发双方所要完成的操作</p><ul><li>就是收到报文后双方要做的动作。以http协议为例，接收方收到http请求报文后，先查找，然后返回一个响应报文。</li></ul></li><li><p>同步：定义收发双方的时序关系</p></li></ol></li></ol></li><li><p>服务</p><ol type="1"><li>定义：在协议的控制下，两个对等实体间的逻辑通信使得本层能向上一层提供服务。要实现本层协议，还需要使用下面一层所提供的服务。</li></ol><ul><li>Note：协议是“水平”的，服务是“垂直”的</li></ul></li><li><p>服务访问点</p><ul><li>定义：在同一系统中相邻两层的实体交换信息的逻辑接口，用于区别不同的服务类型。</li><li>数据链路层的服务访问点为帧的“类型”字段</li><li>网络层的服务访问点为IP数据报首部中的“协议字段”</li><li>运输层的服务访问点为“端口号”</li></ul></li><li><p>服务原语</p><ol type="1"><li>定义：上层使用下层所提供的服务必须通过与下层交换一些命令，这些命令称为服务原语。（没搞懂，以后来填坑）</li></ol></li><li><p>协议数据单元PDU</p><ol type="1"><li>定义：对等层次之间的数据包称为该层的协议数据单元</li></ol><ul><li>物理层的PDU：比特流</li><li>数据链路层的PDU：帧</li><li>网络层的PDU：IP数据报</li><li>运输层的PDU：TCP报文段</li><li>应用层的PDU：报文</li></ul></li><li><p>服务数据单元SDU</p><ol type="1"><li>定义：同一系统内，层与层之间交换的数据包称为服务数据单元</li></ol><ul><li>Note：多个SDU可以合成为一个PDU，一个SDU也可以划分为几个PDU</li></ul></li></ol></li></ol><h3 id="三.-物理层">三. 物理层</h3><ol type="1"><li><p>物理层的基本概念</p><ul><li>在计网中，用来连接各种网络设备的传输媒体种类众多，大致可分为两类，一类是导引型传输媒体（双绞线、同轴电缆），一类是非导引型传输媒体（微波通信）。</li><li>物理层考虑的是怎样才能在连接各种计算机的传输媒体上传输数据比特流。</li><li>物理层位数据链路层屏蔽了各种传输媒体的差异，使数据链路层只需要考虑如何完成本层的协议和服务，而不必考虑网络具体的传输媒体是什么</li><li>物理层协议的主要任务<ol type="1"><li>机械特性：指明接口所用接线器的形状和尺寸、引脚数目和排列、固定和锁定装置</li><li>电气特性：指明在接口电缆的各条线上出现的电压的范围</li><li>功能特性：指明某条线上出现的某一电平的电压表示何种意义</li><li>过程特性：指明对于不同功能的各种可能事件的出现顺序</li></ol></li></ul></li><li><p>物理层下面的传输媒体</p><ul><li>引导型传输媒体：同轴电缆、双绞线、光纤、电力线</li><li>非引导型传输媒体：无线电波、微波、红外线、可见光</li></ul></li><li><p>传输方式</p><ul><li>串行/并行传输：<ul><li>串行传输：只需一条线，一个一个bit的传输</li><li>并行传输：n条先，n个n个bit的传输，速率位串行传输的n倍</li><li>计算机之间的传输通常为串行传输，计算机内部（如果CPU与内存之间的传输通常为并行）</li></ul></li><li>同步传输/异步传输：<ul><li>同步传输：数据块以bit流传输过去，字节之间没有间隔<ul><li>因为没有间隔，所以必须保证收发双方的时钟频率同步<ul><li>外同步：在收发双方之间添加一条单独的时钟信号线</li><li>内同步：发送端将时钟同步信号编码到发送数据中一起传输（例如曼彻斯特编码）</li></ul></li></ul></li><li>异步传输：字节之间有间隙，且间隙时间不固定<ul><li>因为间隙时间不固定，所以叫异步。所以发送端要在每个字节前后加上起始位和结束位</li></ul></li></ul></li><li>单工/半双工/全双工通信：<ul><li>单工(向)通信：通信双方只有一个数据传输方向（例如广播）</li><li>半双工通信：通信双方可以互相传数据，但是不能同时进行（例如对讲机）</li><li>双工通信：通信双方可以互相传数据，且可以同时进行（例如电话）</li></ul></li></ul></li><li><p>编码与调制</p><ul><li><p>信号都是发到信道中，信道不等于传输媒体。如果是单工传输，那么传输媒体中只有一个信道。而对于半双工和全双工传输，传输媒体中就有两个信道。</p></li><li><p>以下是几种常用编码技术：</p><ol type="1"><li>不归零编码：就是010101直接传，缺点就是接受双方时钟频率必须一样</li><li>归零编码：就是10(-1)(0)1010(01)0这样传，接收方只需在信号归零时进行采样即可，无需保证双方时钟频率必须一样。但是缺点就是一般的编码内容都给归零了，浪费资源</li><li>曼彻斯特编码：每个码元的中间时刻都会发生信号跳变，跳变既表示了时钟也表示了数据</li><li>差分曼彻斯特编码：一样是用跳变，但是跳变仅表示时钟，数据要看每个码元开始处的电平与上一个码元结束处电平是否变化，来表示数据</li></ol></li><li><p>调制是指把数字信号调制为可以发出去的信号。</p></li><li><p>以下是一个基本调制方法：</p><ul><li>正交振幅调制QAM，例如QAM16，它调制出的波形可以有12种相位，每种相位有1或2种振幅可选。故可调制出16种码元（看图）</li></ul><p><img src="5.png" /></p><ul><li>每个码元可以表示4个bit信息。码元与4位bit信息之间的对应关系采用格雷码。</li></ul></li></ul></li><li><p>信道的极限容量</p><ul><li>奈氏准则（没考虑噪声）：<ul><li>理想低通信道的最高码元传输速率：2 * W(信道带宽，单位Hz) (码元 /秒)</li><li>理想带通信道的最高码元传输速率：W (码元 / 秒)</li><li>码元 / 秒与bit / s的换算，若一个码元携带n个比特，则码元 / 秒 = nbit/s</li></ul></li><li>香农公式：<ul><li><span class="math inline">\(c = W \times\log_2(1+\frac{S}{N})\)</span></li><li>c：信道的极限信息传输速率(bit/s)</li><li>W：信道带宽（Hz）</li><li>S：信道内所传信号的平均功率</li><li>N：信道内的高斯噪声功率</li><li>S/N：信噪比（dB）</li></ul></li></ul></li><li><p>信道复用技术</p><ul><li>频分复用(FDM)、时分复用(TDM)、统计时分复用<ul><li>略</li></ul></li><li>波分复用<ul><li>略</li></ul></li><li>码分复用<ul><li>打个比方，FDM是不同的组在不同的房间里说话，TDM是不同的人在不同的时刻说话，CDMA是不同的人使用不同的语言说话</li><li>CDMA技术可以让不同用户在同样的时间使用同样的频率进行通信。为什么能做到呢？因为每个用户有一个唯一标识符——码型(芯片序列)</li><li>码型有m位，用1代表1，-1代表0。且要保证不同用户的码型俩俩正交，这样才可以互不干扰同时发送。</li><li>用<span class="math inline">\(S,T\)</span>表示俩用户的码型，那么有<spanclass="math inline">\(S^\mathrm{T}T=0\)</span>，说明对应位相等的个数与对应位不等的个数一样。那么可推导出<spanclass="math inline">\(S^\mathrm{T}\overline{T} =0\)</span>。取反后对应位原本相等的就变不等了，原本不等的就变相等了，但个数还是一样的。</li><li>OK，然后如果一个用户要发送一个1，那么就把自己的码型发出去(1bit时)，如果发送0，就把自己的反码发出去(1 bit时)。</li><li>如果n个用户同时发送一个bit信息，那么n个码型线性叠加在一起记为W，然后丢给接收站点。</li><li>如果想得到用户1，也就是码型为S的那个人在这一时刻发送的bit信号时什么，就拿它的码型S与叠加后码型W做点乘。即<spanclass="math inline">\(S \cdotW\)</span>。若结果为1，则这一时刻用户1发了一个1；如果结果为-1，则它发了一个0；如果结果为0，则它在这一时刻啥都没发。</li><li>原理非常好理解，把W展开，除了自身码型，其余码型相乘后因为正交都为0了。只剩自己了（若这一时刻没发就连自己都没有），然后自己与自己相乘就是1。</li></ul></li></ul></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;参考视频：&lt;a
href=&quot;https://www.bilibili.com/video/BV1c4411d7jb?p=4&amp;amp;spm_id_from=pageDriver&amp;amp;vd_source=ca9a71bb3c1806ce48ae27d95e4e8bd0&quot;&gt;计算机网络微课堂-湖科大教书匠&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="4. 大学" scheme="http://error666.top/categories/4-%E5%A4%A7%E5%AD%A6/"/>
    
    <category term="计算机专业课" scheme="http://error666.top/categories/4-%E5%A4%A7%E5%AD%A6/%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%93%E4%B8%9A%E8%AF%BE/"/>
    
    
  </entry>
  
  <entry>
    <title>2024上半年算法竞赛游记</title>
    <link href="http://error666.top/2024/06/22/2024%E4%B8%8A%E5%8D%8A%E5%B9%B4%E7%AE%97%E6%B3%95%E7%AB%9E%E8%B5%9B%E6%B8%B8%E8%AE%B0/"/>
    <id>http://error666.top/2024/06/22/2024%E4%B8%8A%E5%8D%8A%E5%B9%B4%E7%AE%97%E6%B3%95%E7%AB%9E%E8%B5%9B%E6%B8%B8%E8%AE%B0/</id>
    <published>2024-06-21T16:49:09.000Z</published>
    <updated>2024-06-22T18:45:43.644Z</updated>
    
    <content type="html"><![CDATA[<p>内容包括：ccpc全国邀请赛（山东）、ccpc重庆市塞、ccpc四川省赛</p><span id="more"></span><h3 id="ccpc全国邀请赛山东">ccpc全国邀请赛（山东）</h3><p>赛后没有第一时间写游记，题目忘了。</p><p>酒店不错，比赛场地很大队伍很多，中午的塔斯汀很好吃，题目也能给我们这些菜鸡做对个五六道。</p><p>我记得有俩签到，一个二分，一个模拟。</p><p>然后一个贪心，做过类似的题：<ahref="https://www.acwing.com/problem/content/907/">区间选点</a></p><p>一个普通的最小生成树变式，一个涉及到后缀和的思维题，一个找规律填空题。</p><p>我们应该是做了6题，最后是铜。</p><p>尽力了，从高二以来已经3年没打算法了，这个结果对我来说挺满意。</p><h3 id="ccpc重庆市赛">ccpc重庆市赛</h3><p>赛后没有第一时间写游记，题目忘了。</p><p>志愿者培训没做好，在比赛的时候发出了“讨论请安静点！”的逆天言论，以及比赛开始后才一个个发纸质版题目，差评。以及测评网站用的一个免费的网站，比赛中出现了账号无法登录的问题，差评。</p><p>仨签到。</p><p>然后一个前缀和，做过类似的题，牛客训练赛里的，找不到了。</p><p>然后一个贪心题+简单dfs题。</p><p>还有一题队友写的，不知道是啥算法的题，好像是一道思维题。</p><p>最后金尾。</p><p>遗憾的是有一题hash+二分+dfs的题做了很久没调出来，事后证明思路是正确的，debug能力差了点没弄出来，可惜。</p><h3 id="ccpc四川省赛">ccpc四川省赛</h3><p>赛后没有第一时间写游记，题目忘了。</p><p>场地偏小但凑合，中午的食物质量正常，比赛发的衣服不错，赛后奖品还行。</p><p>但是是上半年发挥最差的一场。</p><p>上去先把仨签到迅速切了，此时的我们是金的排名。</p><p>后面4个小时一题没开出来，难蚌。</p><p>我和另一个队友卡在一个非常非常简单的计算几何，另一个队友卡在一道简单的贪心。</p><p>如果状态好的话，就是5题，银首。</p><p>但世界上没那么多如果，菜就是菜了，最后铜。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;内容包括：ccpc全国邀请赛（山东）、ccpc重庆市塞、ccpc四川省赛&lt;/p&gt;</summary>
    
    
    
    <category term="3. 竞赛" scheme="http://error666.top/categories/3-%E7%AB%9E%E8%B5%9B/"/>
    
    
  </entry>
  
  <entry>
    <title>Matlab基础</title>
    <link href="http://error666.top/2024/04/25/Matlab%E5%9F%BA%E7%A1%80/"/>
    <id>http://error666.top/2024/04/25/Matlab%E5%9F%BA%E7%A1%80/</id>
    <published>2024-04-24T16:53:52.000Z</published>
    <updated>2024-04-26T15:45:27.353Z</updated>
    
    <content type="html"><![CDATA[<p>学习Matlab，一方面是数模需要，另一方面是Matlab +FigureBest绘制出来的图片非常精美。科研绘图时会用到，所以就学习一下。</p><span id="more"></span><h3 id="界面">界面</h3><p>clear：清空工作区</p><p>clc：清除命令行窗口</p><p>；：语句后加分号结果不显示到命令行中，不加会显示到命令行中</p><p>F5：运行</p><p>ctrl + enter：分块运行</p><p>%%：分块分割注释</p><p>ctrl + r：添加多行注释</p><p>ctrl + t：取消多行注释</p><p>ctrl + 0：跳转到命令行窗口</p><p>ctrl + shift + 0：跳转到编辑窗口</p><p>ctrl + 2：跳转到工作目录窗口</p><p>ctrl + 3：跳转到工作区</p><p><ahref="https://ww2.mathworks.cn/help/index.html">Matlab官方文档</a>：查询各种所需功能&amp; 函数</p><p>填写函数参数时按tab：打开参数提示功能，ctrl + down展开提示</p><h3 id="运算">运算</h3><ul><li><p>特殊变量：ans、pi、inf、-inf</p></li><li><p>数据类型：数字、字符串、矩阵</p></li><li><p>运算符：</p><ol type="1"><li>基本运算符：+、-、*、/、^（乘方）</li><li>常用预算符：abs()、mod(x,y)、sqrt()、exp()、log()、log2()、log10()、round()</li></ol></li></ul><h3 id="矩阵">矩阵</h3><p>Note：下标从1开始</p><ol type="1"><li>矩阵的创建<ol type="1"><li>直接输入：用[]作为标识符，同一行用,分隔，不同行用;分隔</li><li>用预设函数创建：<ol type="1"><li>zeros(x, y)：生成x行y列全0矩阵</li><li>ones(x, y)：生成x行y列全1矩阵</li><li>eye(x)：生成x行x列的单位阵</li><li>rand(x, y)：生成x行y列的矩阵，每个元素在(0, 1)内</li><li>randi([imin, imax], x, y)：生成x行y列的矩阵，每个元素在[imin,imax]内</li><li>randn(x, y)：生成x行y列的矩阵，每个元素服从标准正态分布</li></ol></li><li>导入本地数据创建：<ul><li>支持格式：txt、dat、csv、xls、... ...</li><li>导入方法：在菜单栏选择导入数据即可</li></ul></li></ol></li><li>矩阵的修改<ul><li>A(2, 3) = 0：单点修改</li><li>A(2, :) = 0：第2行全部变为0</li><li>A([1, 2], [1, 2, 3]) = 0：第1、2行的第1、2、3列改为0</li></ul></li><li>矩阵的运算<ol type="1"><li>M1 + M2：对应元素相加</li><li>M + c：矩阵M每个元素加上c</li><li>M1 * M2：矩阵乘法</li><li>M * c：矩阵M每个元素乘上c</li><li>M1 .* M2：矩阵M1、M2对应元素相乘</li><li>M1 ./ M2：矩阵M1、M2对应元素相除</li><li>M ^ c：矩阵M的幂运算</li><li>M'：矩阵M的转置</li><li>inv(M)：矩阵M求逆</li><li>diag(diag(M))：M的对角矩阵</li></ol></li></ol><h3 id="程序结构">程序结构</h3><ol type="1"><li><p>global全局变量</p><ul><li>定义时global声明一次，函数内使用前声明一次</li></ul></li><li><p>if-elseif-else-end</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (... &amp;&amp; ...)</span><br><span class="line">    <span class="comment">% TODO</span></span><br><span class="line"><span class="keyword">elseif</span> (... &amp;&amp; ...)</span><br><span class="line">    <span class="comment">% TODO</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">    <span class="comment">% TODO</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></li><li><p>for-end</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = x:y <span class="comment">%循环变量i从x到y</span></span><br><span class="line">    <span class="comment">% TODO</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></li><li><p>自定义函数</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">res</span> = <span class="title">fun_name</span><span class="params">(var1, var2, ...)</span></span></span><br><span class="line">    <span class="comment">% TODO</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">fun_name</span><span class="params">()</span></span></span><br><span class="line">    <span class="comment">% TODO</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></li></ol><h3 id="图像">图像</h3><ol type="1"><li>axis<ol type="1"><li>axis([xmin, xmax, ymin, ymax])：生成指定坐标范围</li><li>axis equal：x/y轴使用相同的比例</li></ol></li><li>subplot<ol type="1"><li>subplot(n, m,id)：将figure分割为n*m个区域，当前使用第id个区域进行绘图</li></ol></li><li>plot<ol type="1"><li>hold on：使得多个plot画出的线在一个图上</li><li>plot(X, Y)：画出点(x1, y1), (x2, y2), ...并连线</li><li>plot(Y)：画出(1, y1), (2, y2), ...并连线</li><li>plot(x, y, '.')：画坐标点(x, y)</li></ol></li><li>title<ol type="1"><li>title('xxx')：起名</li></ol></li><li>xlabel/ylabel<ol type="1"><li>xlabel('xxx')</li></ol></li><li>legend<ol type="1"><li>legend('name1', 'name2', ...)：图例</li></ol></li><li>改样式<ul><li>交给FigureBest</li></ul></li></ol><hr /><h3 id="实战">实战</h3><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br></pre></td><td class="code"><pre><span class="line">clc;</span><br><span class="line">clear all; <span class="comment">% 相比于clear, clear all可以清除global变量</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% 定义变量</span></span><br><span class="line"><span class="keyword">global</span> Iter_Num n x r r1 r2 alpha <span class="built_in">beta</span> v s y a ans_x ans_y s_x s_y xx vv ss yy;</span><br><span class="line">Iter_Num = <span class="number">1000</span>;</span><br><span class="line">n = <span class="number">5</span>;</span><br><span class="line">x = <span class="built_in">zeros</span>(<span class="number">1</span>, n);</span><br><span class="line">r = <span class="built_in">zeros</span>(<span class="number">1</span>, n);</span><br><span class="line">r1 = [<span class="number">1</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">4</span>, <span class="number">8</span>];     <span class="comment">% 5个预设点的x坐标</span></span><br><span class="line">r2 = [<span class="number">6</span>, <span class="number">8</span>, <span class="number">8</span>, <span class="number">2</span>, <span class="number">3</span>];     <span class="comment">% 5个预设点的y坐标</span></span><br><span class="line">alpha = <span class="number">0.01</span>;</span><br><span class="line"><span class="built_in">beta</span> = <span class="number">0.01</span>;</span><br><span class="line">v = <span class="built_in">diag</span>(<span class="built_in">ones</span>(<span class="number">1</span>, n));</span><br><span class="line">s = phi(x);</span><br><span class="line">y = <span class="built_in">zeros</span>(<span class="number">1</span>, n);</span><br><span class="line">a = [</span><br><span class="line">    <span class="number">1</span>/<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>/<span class="number">2</span>, <span class="number">0</span>, <span class="number">0</span>;</span><br><span class="line">     <span class="number">1</span>/<span class="number">3</span>, <span class="number">1</span>/<span class="number">3</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>/<span class="number">3</span>;</span><br><span class="line">    <span class="number">0</span>, <span class="number">1</span>/<span class="number">2</span>, <span class="number">1</span>/<span class="number">2</span>, <span class="number">0</span>, <span class="number">0</span>;</span><br><span class="line">    <span class="number">0</span>, <span class="number">1</span>/<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>/<span class="number">2</span>, <span class="number">0</span>;</span><br><span class="line">    <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>/<span class="number">2</span>, <span class="number">1</span>/<span class="number">2</span></span><br><span class="line">    ];</span><br><span class="line">ans_x = <span class="built_in">zeros</span>(<span class="number">1</span>, <span class="number">1</span>); <span class="comment">% ans_x[i][j]表示第i个点第j次迭代的x坐标</span></span><br><span class="line">ans_y = <span class="built_in">zeros</span>(<span class="number">1</span>, <span class="number">1</span>); <span class="comment">% ans_y[i][j]表示第i个点第j次迭代的y坐标</span></span><br><span class="line">s_x = <span class="built_in">zeros</span>(<span class="number">1</span>, <span class="number">1</span>);   <span class="comment">% s_x[i][j]表示第i个点第j次迭代x坐标的sigma</span></span><br><span class="line">s_y = <span class="built_in">zeros</span>(<span class="number">1</span>, <span class="number">1</span>);   <span class="comment">% s_y[i][j]表示第i个点第j次迭代y坐标的sigma</span></span><br><span class="line">xx = x;</span><br><span class="line">vv = v;</span><br><span class="line">ss = s;</span><br><span class="line">yy = y;</span><br><span class="line"></span><br><span class="line"><span class="comment">% 开始迭代</span></span><br><span class="line">solve();</span><br><span class="line">fprintf(<span class="string">&#x27;经过%d轮迭代, 最终F(x)收敛到: %f\n&#x27;</span>, Iter_Num, cal(Iter_Num));</span><br><span class="line">draw_1(); <span class="comment">% F_k的比率图</span></span><br><span class="line">draw_2(); <span class="comment">% 画演示图</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%下面是画图子函数</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">draw_2</span><span class="params">()</span></span></span><br><span class="line">    <span class="keyword">global</span> Iter_Num;</span><br><span class="line">    <span class="built_in">figure</span> (<span class="number">2</span>);</span><br><span class="line">    subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>);</span><br><span class="line">    draw_2_sub(<span class="number">1</span>);</span><br><span class="line">    subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>);</span><br><span class="line">    draw_2_sub(Iter_Num / <span class="number">2</span>);</span><br><span class="line">    subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>);</span><br><span class="line">    draw_2_sub(Iter_Num);</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">draw_2_sub</span><span class="params">(iter_num)</span></span></span><br><span class="line">    <span class="built_in">hold</span> on;</span><br><span class="line">    <span class="keyword">global</span> n r1 r2 ans_x ans_y;</span><br><span class="line">    axis([<span class="number">0</span>, <span class="number">10</span>, <span class="number">0</span>, <span class="number">10</span>]);</span><br><span class="line">    <span class="built_in">plot</span>(<span class="number">5</span>, <span class="number">5</span>, <span class="string">&#x27;*&#x27;</span>);</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : n</span><br><span class="line">        <span class="built_in">plot</span>(ans_x(<span class="built_in">i</span>, iter_num), ans_y(<span class="built_in">i</span>, iter_num), <span class="string">&#x27;+&#x27;</span>, <span class="string">&#x27;Color&#x27;</span>, <span class="string">&#x27;blue&#x27;</span>);</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    <span class="built_in">plot</span>([r1(<span class="number">1</span>), r1(<span class="number">2</span>), r1(<span class="number">3</span>), r1(<span class="number">5</span>), r1(<span class="number">4</span>), r1(<span class="number">1</span>)], [r2(<span class="number">1</span>), r2(<span class="number">2</span>), r2(<span class="number">3</span>), r2(<span class="number">5</span>), r2(<span class="number">4</span>), r2(<span class="number">1</span>)], <span class="string">&#x27;Marker&#x27;</span>,<span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;Color&#x27;</span>, <span class="string">&#x27;red&#x27;</span>);</span><br><span class="line">    sum_x = <span class="number">0</span>;</span><br><span class="line">    sum_y = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : n</span><br><span class="line">        sum_x = sum_x + ans_x(<span class="built_in">i</span>, iter_num);</span><br><span class="line">        sum_y = sum_y + ans_y(<span class="built_in">i</span>, iter_num);</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    <span class="built_in">plot</span>(sum_x / n, sum_y / n, <span class="string">&#x27;o&#x27;</span>);</span><br><span class="line">    xlabel(<span class="string">&#x27;$x_1$&#x27;</span>, <span class="string">&#x27;Interpreter&#x27;</span>, <span class="string">&#x27;latex&#x27;</span>);</span><br><span class="line">    ylabel(<span class="string">&#x27;$x_2$&#x27;</span>, <span class="string">&#x27;Interpreter&#x27;</span>, <span class="string">&#x27;latex&#x27;</span>);</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">draw_1</span><span class="params">()</span></span></span><br><span class="line">    <span class="built_in">figure</span> (<span class="number">1</span>);</span><br><span class="line">    <span class="keyword">global</span> Iter_Num;</span><br><span class="line">    F_best = <span class="number">18.874999999999645</span>;</span><br><span class="line">    error = [];</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : (Iter_Num - <span class="number">1</span>)</span><br><span class="line">        error = [error, <span class="built_in">abs</span>(cal(<span class="built_in">i</span> + <span class="number">1</span>) - F_best) / <span class="built_in">abs</span>(cal(<span class="built_in">i</span>) - F_best)];</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    <span class="built_in">plot</span>(error);</span><br><span class="line">    xlabel(<span class="string">&#x27;Iteration k&#x27;</span>);</span><br><span class="line">    ylabel(<span class="string">&#x27;$\frac&#123;f_&#123;k+1&#125; - f^*&#125;&#123;f_k - f^*&#125;$&#x27;</span>, <span class="string">&#x27;Interpreter&#x27;</span>, <span class="string">&#x27;latex&#x27;</span>, <span class="string">&#x27;FontSize&#x27;</span>, <span class="number">20</span>);</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% 下面是计算子函数</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">res</span> = <span class="title">cal</span><span class="params">(iter_num)</span></span></span><br><span class="line">    <span class="keyword">global</span> n ans_x r1 ans_y r2 s_x s_y;</span><br><span class="line">    sum = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : n</span><br><span class="line">        sum = sum + <span class="number">0.5</span> * ((ans_x(<span class="built_in">i</span>, iter_num) - r1(<span class="built_in">i</span>)) ^ <span class="number">2</span> + (ans_y(<span class="built_in">i</span>, iter_num) - r2(<span class="built_in">i</span>)) ^ <span class="number">2</span>)...</span><br><span class="line">            + <span class="number">0.5</span> * ((ans_x(<span class="built_in">i</span>, iter_num) - <span class="number">5</span>) ^ <span class="number">2</span> + (ans_y(<span class="built_in">i</span>, iter_num) - <span class="number">5</span>) ^ <span class="number">2</span>)...</span><br><span class="line">            + ((s_x(<span class="built_in">i</span>, iter_num) - <span class="number">5</span>) ^ <span class="number">2</span> + (s_y(<span class="built_in">i</span>, iter_num) - <span class="number">5</span>) ^ <span class="number">2</span>);</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    res = sum;</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">solve</span><span class="params">()</span></span></span><br><span class="line">    <span class="keyword">global</span> Iter_Num n xx ss ans_x ans_y s_x s_y;</span><br><span class="line">    init(<span class="number">1</span>);</span><br><span class="line">    <span class="keyword">for</span> T = <span class="number">1</span> : Iter_Num</span><br><span class="line">        <span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : n</span><br><span class="line">            upd_x(<span class="built_in">i</span>);</span><br><span class="line">            ans_x(<span class="built_in">i</span>, T) = xx(<span class="built_in">i</span>);</span><br><span class="line">            upd_v(<span class="built_in">i</span>);</span><br><span class="line">            upd_s(<span class="built_in">i</span>);</span><br><span class="line">            s_x(<span class="built_in">i</span>, T) = ss(<span class="built_in">i</span>);</span><br><span class="line">            upd_y(<span class="built_in">i</span>);</span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">        backup();</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    init(<span class="number">2</span>);</span><br><span class="line">    <span class="keyword">for</span> T = <span class="number">1</span> : Iter_Num</span><br><span class="line">        <span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> : n</span><br><span class="line">            upd_x(<span class="built_in">i</span>);</span><br><span class="line">            ans_y(<span class="built_in">i</span>, T) = xx(<span class="built_in">i</span>);</span><br><span class="line">            upd_v(<span class="built_in">i</span>);</span><br><span class="line">            upd_s(<span class="built_in">i</span>);</span><br><span class="line">            s_y(<span class="built_in">i</span>, T) = ss(<span class="built_in">i</span>);</span><br><span class="line">            upd_y(<span class="built_in">i</span>);</span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">        backup();</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">upd_y</span><span class="params">(i)</span></span></span><br><span class="line">    <span class="keyword">global</span> a ss yy y <span class="built_in">beta</span> n;</span><br><span class="line">    sum = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">1</span> : n</span><br><span class="line">        sum = sum + a(<span class="built_in">i</span>, <span class="built_in">j</span>) * ss(<span class="built_in">j</span>);</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    yy(<span class="built_in">i</span>) = y(<span class="built_in">i</span>) + <span class="built_in">beta</span> * (ss(<span class="built_in">i</span>) - sum);</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">upd_s</span><span class="params">(i)</span></span></span><br><span class="line">    <span class="keyword">global</span> a s ss x xx vv n v;</span><br><span class="line">    sum = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">1</span> : n</span><br><span class="line">        sum = sum + a(<span class="built_in">i</span>, <span class="built_in">j</span>) * s(<span class="built_in">j</span>);</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    ss(<span class="built_in">i</span>) = sum + phi(xx(<span class="built_in">i</span>)) / vv(<span class="built_in">i</span>, <span class="built_in">i</span>) - phi(x(<span class="built_in">i</span>)) / v(<span class="built_in">i</span>, <span class="built_in">i</span>);</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">upd_v</span><span class="params">(i)</span></span></span><br><span class="line">    <span class="keyword">global</span> n a v vv;</span><br><span class="line">    sum = <span class="built_in">zeros</span>(<span class="number">1</span>, n);</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">1</span> : n</span><br><span class="line">        sum = sum + a(<span class="built_in">i</span>, <span class="built_in">j</span>) * v(<span class="built_in">j</span>, :);</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    vv(<span class="built_in">i</span>, :) = sum;</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">upd_x</span><span class="params">(i)</span></span></span><br><span class="line">    <span class="keyword">global</span> xx x alpha r n y s v;</span><br><span class="line">    xx(<span class="built_in">i</span>) = x(<span class="built_in">i</span>) - alpha * ((x(<span class="built_in">i</span>) - r(<span class="built_in">i</span>)) + (x(<span class="built_in">i</span>) - <span class="number">5</span>) + (<span class="number">1</span> / n) * (y(<span class="built_in">i</span>) + <span class="number">2</span> * (s(<span class="built_in">i</span>) - <span class="number">5</span>) / n / v(<span class="built_in">i</span>, <span class="built_in">i</span>)));</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">backup</span><span class="params">()</span></span></span><br><span class="line">    <span class="keyword">global</span> x v s y xx vv ss yy</span><br><span class="line">    x = xx;</span><br><span class="line">    v = vv;</span><br><span class="line">    s = ss;</span><br><span class="line">    y = yy;</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">init</span><span class="params">(op)</span></span></span><br><span class="line">    <span class="keyword">global</span> n x r v s y xx vv ss yy r1 r2</span><br><span class="line">    x = randi([<span class="number">1</span>, <span class="number">5</span>], <span class="number">1</span>, n);</span><br><span class="line">    <span class="keyword">if</span> (op == <span class="number">1</span>)</span><br><span class="line">        r = r1;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        r = r2;</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    v = <span class="built_in">diag</span>(<span class="built_in">ones</span>(<span class="number">1</span>, n));</span><br><span class="line">    s = phi(x);</span><br><span class="line">    y = <span class="built_in">zeros</span>(<span class="number">1</span>, n);</span><br><span class="line">    xx = x;</span><br><span class="line">    vv = v;</span><br><span class="line">    ss = s;</span><br><span class="line">    yy = y;</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">res</span> = <span class="title">phi</span><span class="params">(x)</span></span></span><br><span class="line">    <span class="keyword">global</span> n;</span><br><span class="line">    res = x / n;</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><p>经过FigureBest美化后的图片：</p><p><img src="1.png" /></p><p><img src="2.png" /></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;学习Matlab，一方面是数模需要，另一方面是Matlab +
FigureBest绘制出来的图片非常精美。科研绘图时会用到，所以就学习一下。&lt;/p&gt;</summary>
    
    
    
    <category term="2. 技能栈" scheme="http://error666.top/categories/2-%E6%8A%80%E8%83%BD%E6%A0%88/"/>
    
    <category term="Matlab" scheme="http://error666.top/categories/2-%E6%8A%80%E8%83%BD%E6%A0%88/Matlab/"/>
    
    
  </entry>
  
  <entry>
    <title>各种工具使用手册</title>
    <link href="http://error666.top/2024/04/25/%E5%90%84%E7%A7%8D%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/"/>
    <id>http://error666.top/2024/04/25/%E5%90%84%E7%A7%8D%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/</id>
    <published>2024-04-24T16:28:29.000Z</published>
    <updated>2024-06-30T18:22:44.700Z</updated>
    
    <content type="html"><![CDATA[<p>工具能大大提升效率。所以掌握有必要掌握一些常用工具。</p><span id="more"></span><h3 id="一.-chatgpt">一. ChatGPT</h3><h4 id="如何订阅gpt4-plus">如何订阅GPT4 PLUS？</h4><ol type="1"><li>去<a href="https://chat.openai.com/">官网链接(openai.com)</a>注册一个OpenAI账号并登录（建议通过Google邮箱注册）</li><li>注册过程中，需要用到手机号验证，使用<ahref="https://sms-activate.org/cn">SMS-Activate</a>解决</li><li>1、2步完成后，就可以使用ChatGPT服务了，但是只能使用3.5而且有次数限制。点击升级，会看到一个支付界面，界面里要填银行卡相关信息。银行卡只能使用美国的。所以在<ahref="https://www.fomepay.com/">FOMEPay</a>上购买一张虚拟美国银行卡，往里充钱。然后将卡号信息填到刚才的界面中即可。</li><li>充值成功，可正常使用ChatGPT4</li></ol><p>（注意，充值过程尽量要全程使用美国IP的梯子！）</p><h4 id="如何使用">如何使用？</h4><p>想怎么用怎么用，推荐几个插件：</p><ol type="1"><li>WebPilot：帮助ChatGPT联网搜索信息</li><li>Wolfram：科学计算，图标绘制</li><li>Tutory：可以帮你制定任意领域的学习路线</li><li>Ai Tool Hunt：找插件的插件</li><li>MixerBox Scholar：可以访问一些学术资源</li></ol><hr /><h3 id="二.-vs-code">二. VS Code</h3><p>VS Code作为最常用的IDE，掌握其使用方法以及相关插件十分必要。</p><p>而且这玩意还可以通过Github同步配置，所以配置一次，永久享用。</p><p>而且VS Code里有终端，于是写项目配合git使用很方便。</p><h4 id="快捷键">快捷键</h4><ul><li>ctrl + ,：设置</li><li>ctrl + shift + p：命令面板</li><li>ctrl + p：最近文件列表</li><li>ctrl + j：切出下面板（用来在代码和终端输入间跳转）</li><li>ctrl + b：隐藏/显示目录</li><li>ctrl + shift + n：新建vscode窗口</li><li>ctrl + 1/2/3：分屏/不同分屏中跳转</li><li>ctrl + alt + right/left：将文件移动到不同分屏中</li><li>alt + ijkl/[]：上下左右/HOME,END（自己改的键）</li><li>alt + up/down：行移动</li></ul><h4 id="插件">插件</h4><h5 id="code-runner">code runner</h5><ul><li>ctrl + alt + n：编译运行程序</li><li>ctrl + c：退出当前命令（死循环时退出运行）</li></ul><h5 id="wsl">WSL</h5><ul><li>直接在本地vscode登入进wsl子系统中的vscode</li><li>或者在wsl中输入code .进入vscode界面</li></ul><h5 id="git-graph">Git Graph</h5><ul><li>装了之后，直接在vscode中就可以查看提交/分支状态</li><li>可以查看每次commit的id/author/date/parents/与上一次commit的不同之处<ul><li>查看与任意一次commit的不同之处：先点一个，再按住ctrl点另一个</li></ul></li></ul><h5 id="todo-tree">Todo Tree</h5><ul><li>TODO表示待办，FIXME表示写了一半还没写完，BUG表示这段程序有问题，HACK表示这段程序可以优化，NOTE表示想法</li><li>这个插件除了方便写注释，还可以充当打标签的作用</li></ul><h5 id="bookmarks">Bookmarks</h5><ul><li>打标签，当程序很长的时候，用鼠标跳转很不方便，用书签跳转就很快。</li></ul><h5 id="jupyter">Jupyter</h5><ul><li>无需安装jupyter notebook即可在vscode实现相同功能</li><li>Esc + M：markdown模式</li><li>Esc + Y：代码模式</li><li>ctrl + enter：运行</li><li>Esc + D, D：删除该单元</li><li>Esc + B：在下方添加一个单元</li><li>Esc + A：在上方添加一个单元</li><li>Esc + L：显示行号</li></ul><h5 id="draw.io-integration">Draw.io Integration</h5><ul><li>画流程图</li></ul><hr /><h3 id="三.-sai2">三. SAI2</h3><p>SAI2的最最最基本使用（我不玩板绘）。主要用来方便授课。</p><p>因为最近接了一个线上一对一的算法家教，所以买了一块数位板（高漫1060pro）方便授课，然后下了个SAI2。</p><p>首先改板子的映射区，这个型号的板子对我来说太大了，手移动距离太多很累。所以把板子工作区域改小。记得去官方下驱动。</p><p>然后是改板子的快捷键，我板子的快捷键从上到下分别是：（对应着SAI2里的功能）</p><ul><li>选中、剪切</li><li>复制、粘贴</li><li>画笔、橡皮</li><li>ctrl、shift</li><li>文字、ESC</li><li>合并图层、画面居中</li></ul><p>所以设置好快捷键后，基本上只需要打开SAI2，然后在板子上操作就好了。不怎么需要去SAI2里操作了。</p><p>说一下SAI2的操作逻辑，首先是文字，每次输入文字SAI2都会新建一个图层，所以在输入完文字后要按ESC+ 合并图层，才能将文字和原本内容合并到同一图层中。</p><p>然后是粘贴，在你选中、复制、粘贴后，粘贴的内容会新开一个图层。所以需要按住ctrl移动到恰当位置后，按下合并图层，才能实现粘贴的内容和原内容在同一图层中。</p><p>最后是操作时遇到的一些问题：</p><ol type="1"><li>为什么切换到画笔后，写不出东西？<ul><li>可能是因为你选中了某个区域，没有取消就切换到画笔模式了。所以可以先按剪切后，再切换到画笔模式即可正常工作。</li></ul></li></ol><hr /><h3 id="四.-adobe-illustrator">四. Adobe illustrator</h3><p>通俗的理解，Adobeillustrator就是针对于矢量图的画图工具。发英文期刊/会议，配图格式经常要求是矢量图且质量比较高。所以matplotlib/Matlab+ Adobe illustrator + MathType就足以制作论文的配图。</p><h4 id="界面">界面</h4><ul><li>视图 -&gt; 标尺：打开标尺</li><li>右键标尺：选择标尺单位</li><li>视图 -&gt; 智能参考线：打开自动吸附功能</li><li>文件 -&gt; 存储：即保存，格式有eps等</li><li>文件 -&gt; 导出：即导出，格式有jpg/png等</li><li>文件 -&gt; 置入：插入图片到该画板</li><li>窗口 -&gt; 描边：里面有更多关于描边的参数（例如画箭头/虚线）</li></ul><h4 id="操作">操作</h4><ul><li>鼠标中键：移动</li><li>alt + 滚轮：放大/缩小</li><li>双击对象：对象进入隔离模式，防止操作时误操作到其它对象</li></ul><h4 id="工具栏">工具栏</h4><ul><li>空心箭头（普通选择）<ul><li>单击对象：用于选择然后移动/放大/缩小/旋转<ul><li>右键：编组/取消编组</li></ul></li><li>按住ctrl：显示所有锚点，点击锚点可编辑锚点</li><li>按shift + 单击其它对象：可多选其它对象</li><li>按alt + 拖动：可复制一份对象出来</li></ul></li><li>形状工具<ul><li>右键可以选择画不同的形状，按住shift可画标准化图形</li><li>矩形、椭圆、多边形、星形、直线</li></ul></li></ul><h4 id="属性图层库">属性/图层/库</h4><ul><li>属性：调节对象的属性，比如改变位置/设置颜色/填充</li></ul><h3 id="五.-origin">五. Origin</h3><p>画图神器。我觉得比matlab那一套方便多了，图形化的操作更加容易上手，不会把时间浪费在很多无意义的细节上。</p><p>Origin的组织结构：项目(.opju) -&gt; 文件夹 -&gt;book(数据)/graph(图)</p><p>对于book，使用内置python编程导入数据（菜单栏 -&gt; 连接 -&gt;.py），下面是一个导入的例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">wks = op.new_sheet()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">    wks.from_list(i * <span class="number">2</span>, ans_x[i])</span><br><span class="line">    wks.from_list(i * <span class="number">2</span> + <span class="number">1</span>, ans_y[i])</span><br></pre></td></tr></table></figure><p>对于graph，其组织结构为：画布 -&gt; 图层(坐标轴) -&gt; 点/线。</p><p>上面是简单概念介绍，下面将对操作细节进行更多的阐述：</p><ol type="1"><li>A + 鼠标拖拽滚轮：实现页面的移动 + 放大/缩小</li><li>右上角有抗锯齿功能</li><li>最后画完图后，菜单栏 -&gt; 格式 -&gt;调整页面至图层大小，可以把白边裁掉。<ul><li>建议对于宽度选择边界为5，对于高度选择紧凑</li></ul></li><li>画风格类似的图时，可以使用复制风格功能</li><li>画子图时，建议先把每一张图画好，再使用“菜单栏 -&gt; 图 -&gt;合并图表”完成子图绘制</li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;工具能大大提升效率。所以掌握有必要掌握一些常用工具。&lt;/p&gt;</summary>
    
    
    
    <category term="杂项" scheme="http://error666.top/categories/%E6%9D%82%E9%A1%B9/"/>
    
    
  </entry>
  
</feed>
