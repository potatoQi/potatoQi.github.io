<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta name="google-site-verification" content="Nju-Ov4omLH_XBGFv1RDU5xdZaGOW05OvwIjIu_498Q">
  <meta name="msvalidate.01" content="2CE08595DA3460134EDC20E4D8663AD4">
  <meta name="baidu-site-verification" content="codeva-j0Zy96Rpmy">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"error666.top","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"manual","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.json"};
  </script>

  <meta name="description" content="参考内容：《机器学习》周志华">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习课程自学笔记">
<meta property="og:url" content="http://error666.top/2024/09/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/index.html">
<meta property="og:site_name" content="Error_666">
<meta property="og:description" content="参考内容：《机器学习》周志华">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://error666.top/2024/09/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/1.png">
<meta property="og:image" content="http://error666.top/2024/09/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/3.png">
<meta property="og:image" content="http://error666.top/2024/09/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/2.png">
<meta property="og:image" content="http://error666.top/2024/09/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/4.png">
<meta property="og:image" content="http://error666.top/2024/09/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/5.png">
<meta property="og:image" content="http://error666.top/2024/09/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/6.png">
<meta property="og:image" content="http://error666.top/2024/09/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/7.png">
<meta property="og:image" content="http://error666.top/2024/09/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/8.png">
<meta property="og:image" content="http://error666.top/2024/09/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/9.png">
<meta property="og:image" content="http://error666.top/2024/09/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/10.png">
<meta property="article:published_time" content="2024-09-19T04:25:47.000Z">
<meta property="article:modified_time" content="2024-10-03T07:45:41.271Z">
<meta property="article:author" content="Error_666">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://error666.top/2024/09/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/1.png">

<link rel="canonical" href="http://error666.top/2024/09/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>机器学习课程自学笔记 | Error_666</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

  <script src="/live2d-widget/autoload.js"></script>
<link rel="alternate" href="/atom.xml" title="Error_666" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Error_666</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-tools">

    <a href="/tools/" rel="section"><i class="fa fa-toolbox fa-fw"></i>工具</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://error666.top/2024/09/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Error_666">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Error_666">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          机器学习课程自学笔记
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-09-19 12:25:47" itemprop="dateCreated datePublished" datetime="2024-09-19T12:25:47+08:00">2024-09-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-10-03 15:45:41" itemprop="dateModified" datetime="2024-10-03T15:45:41+08:00">2024-10-03</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/4-%E5%A4%A7%E5%AD%A6/" itemprop="url" rel="index"><span itemprop="name">4. 大学</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/4-%E5%A4%A7%E5%AD%A6/%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%93%E4%B8%9A%E8%AF%BE/" itemprop="url" rel="index"><span itemprop="name">计算机专业课</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>12k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>11 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>参考内容：《机器学习》周志华</p>
<span id="more"></span>
<hr />
<p>成绩构成：</p>
<ol type="1">
<li>考勤、作业、研讨：10%</li>
<li>项目：40%</li>
<li>期末：50%</li>
</ol>
<h3 id="一.-绪论">一. 绪论</h3>
<ol type="1">
<li><p>根据训练数据是否拥有标记数据，学习任务大致可分为两类：监督学习、无监督学习。</p>
<ul>
<li>分类和回归是前者的代表，聚类是后者的代表</li>
<li>聚类意思是在训练过程中，机器会自动的对事物的潜在概念进行划分，并把物体分成若干组</li>
</ul></li>
<li><p>模型适用于新样本的能力，称为泛化能力</p></li>
<li><p>通常假设样本空间中全体样本服从一个位置分布<span
class="math inline">\(\mathcal{D}\)</span>，我们获得的每个样本都是独立地从这个分布上采样得到的，即“独立同分布”。</p></li>
<li><p>假设空间</p>
<ul>
<li>简单理解，就是所有输入的状态</li>
<li>书中举了个例子，有A、B、C三种属性，分别有3、2、2种取值方式。学习目标是某个状态<span
class="math inline">\((a, b, c)\)</span>是否是牛逼的？求所有状态数。
<ul>
<li>对于属性A，其实有4种状态，<span class="math inline">\(a_1, a_2, a_3,
*\)</span>，<span
class="math inline">\(*\)</span>表示这个属性取什么无所谓。对于B、C属性同理</li>
<li>那么状态数就有：<span class="math inline">\(4 * 3 * 3 =
36\)</span>种</li>
<li>其实还漏了一种，还有一种状态是世界上没有"牛逼"这个概念，也就是<span
class="math inline">\(\phi\)</span>状态。</li>
<li>所以这个例子的总状态数是37种。</li>
<li>我来列举其中的几种：
<ul>
<li>A是<span class="math inline">\(a_1\)</span>，B是<span
class="math inline">\(b_2\)</span>，C是<span
class="math inline">\(c_3\)</span>时，是牛逼的</li>
<li>A是<span class="math inline">\(*\)</span>，B是<span
class="math inline">\(b_1\)</span>，C是<span
class="math inline">\(*\)</span>时，是牛逼的</li>
<li><span class="math inline">\(\cdots\)</span></li>
<li>世界上没有"牛逼"这个东西</li>
</ul></li>
</ul></li>
</ul></li>
<li><p>版本空间</p>
<ul>
<li><p>简单理解，就是把假设空间中不符合样本的所有假设剔除掉的空间</p></li>
<li><p>以书中例子为例</p>
<ul>
<li>根据表1.1，我们知“好瓜”的概念是成立的，所以先删除 <span
class="math inline">\(\phi\)</span> 的假设</li>
<li>根据样本（（色泽=青绿）<sup>（根蒂=蜷缩）</sup>（敲声=浊响））——&gt;好瓜，删除所有状态对不上的假设</li>
<li>根据样本（（色泽=乌黑）<sup>（根蒂=蜷缩）</sup>（敲声=浊响））——&gt;好瓜，删除所有状态对不上的假设</li>
</ul>
<blockquote>
<p>这里把（（色泽=乌黑）<sup>（根蒂=蜷缩）</sup>（敲声=浊响））删除，这个和样本2符合，不要觉得心虚，因为利用样本2进行删除的时候也会删掉（（色泽=青绿）<sup>（根蒂=蜷缩）</sup>（敲声=浊响））这样刚好留下了（（色泽=*）<sup>（根蒂=蜷缩）</sup>（敲声=浊响）</p>
</blockquote>
<ul>
<li>根据样本（（色泽=青绿）<sup>（根蒂=硬挺）</sup>（敲声=清脆））——&gt;不是好瓜，删除所有状态对上的假设</li>
<li>根据样本（（色泽=乌黑）<sup>（根蒂=稍蜷）</sup>（敲声=沉闷））——&gt;不是好瓜，删除所有状态对上的假设</li>
<li>所以最后剩下了三个假设，这三个假设我们称之为版本空间：(色泽 = <em>,
根蒂 = 蜷缩, 敲声 = </em>)、(色泽 = <em>, 根蒂 = </em>, 敲声 =
清脆)、(色泽 = *, 根蒂 = 蜷缩, 敲声 = 清脆)</li>
</ul></li>
</ul></li>
<li><p>归纳偏好</p>
<ul>
<li>现实问题中，我们常面临很大的假设空间，但学习过程是根据有限的样本训练集进行的，那么对于不同版本的训练集，就会有不同的版本空间。版本空间内每一个假设都可以判断上面数据集中的每一条数据，是好瓜还是不是好瓜，但是用不同的假设判断一条新数据可能会得出不一样的结果，这就属于“归纳偏好”。</li>
</ul></li>
</ol>
<h3 id="二.-模型评估与选择">二. 模型评估与选择</h3>
<ol type="1">
<li>精度 = 1 - 错误率</li>
<li>学习器在训练集上的误差叫“训练误差”或“经验误差”</li>
<li>学习器在新样本上的误差叫“泛化误差”。显然，我们想要泛化误差小的学习器。</li>
<li>过拟合是指训练误差很小，但是泛化误差不理想。欠拟合是指俩误差都不理想。</li>
</ol>
<h4 id="模型评估方法">模型评估方法</h4>
<ul>
<li><p>评估方法存在的意义：实际中，我们不可能直接拿到泛化误差，因为泛化误差是指实际情况的误差。产品都没开发出来咋获得嘛。而训练误差又由于过拟合现象的存在而不适合作为标准。所以，这时候就需要设计一些精妙的评估方法</p></li>
<li><p>留出法</p>
<ul>
<li>将数据集D切为训练集S和验证集T</li>
<li>需要注意，S和T的划分要尽可能保持数据分布的一致性。例如在分类任务中至少要保证样本的类别比例相似</li>
<li>为了结果可靠，可以进行多次留出法，用平均值作为最终结果</li>
<li>留出法的缺点：我们希望的是评估用D训练出的模型的性能，但留出法本质上是评估的S训练出来的模型的性能。这就会陷入一个窘境：若S包含绝大多数样本，虽然S与D的差距拉近，但是T太小导致评估结果可能不稳定；若S太少，S与D的差距就更远了。所以这个bug没有完全的解决方案，常见做法是将约1/5
~ 1/3的样本用于测试，剩下的用于训练</li>
</ul></li>
<li><p>交叉验证法</p>
<ul>
<li>将D划分为k个大小相似的互斥子集：<span class="math inline">\(D = D_1
\cup D_2 \cup \cdots \cup D_k\)</span>。然后枚举<span
class="math inline">\(D_i\)</span>，每次(全集 - <span
class="math inline">\(D_i\)</span>)作为训练集，<span
class="math inline">\(D_i\)</span>作为验证集。进行k次训练测试，最后返回k次结果的均值。</li>
<li>可以发现，交叉验证法的稳定性和保真性很大程度取决于k，通常取10，称为10折交叉验证</li>
<li>需要注意，<span
class="math inline">\(D_i\)</span>尽可能保持数据分布的一致性</li>
<li>为了结果可靠，可以进行多次交叉验证法，用均值作为最终结果。常见的有：10次10折交叉验证</li>
<li>（当k = D样本数量时，是交叉验证法的一个特例，称为留一法）</li>
</ul></li>
<li><p>自助法</p>
<ul>
<li>无论是留出法还是交叉验证法，S !=
D，所以必然会存在一定偏差。留一法虽然可以使得S <span
class="math inline">\(\to\)</span>
D，但是训练集太大计算复杂度太高。有没有两全其美的方法呢——自助法。</li>
<li>做法：假设D样本数为m，则进行放回随机采样m次，得到D'。用D'作为训练集，D'作为验证集。</li>
<li>样本在m次采样始终不被采到的概率：</li>
</ul>
<p><span class="math display">\[
\lim_{m\mapsto\infty}\left(1-\frac1m\right)^m\mapsto\frac1e\approx0.368
\]</span></p>
<ul>
<li>即数据集D中约36.8%的样本不会出现在D'中。</li>
<li>优点：评估的模型和期望评估的模型都使用了m个样本，但仍有1/3的数据供我们验证。在数据集较小或难以划分训练/验证集时很有用。</li>
<li>缺点：自助法生产的数据集改变了初始数据集分布，会引入估计偏差。因此数据量充足时，留出法和交叉验证法更常用</li>
</ul></li>
<li><p>在进行完模型评估（也就是训练和评估）后，需要再将数据集D全部丢进模型训练一次。这么做是因为模型评估时S
!= D。</p></li>
</ul>
<h4 id="性能度量">性能度量</h4>
<ul>
<li>当得到一个模型后，如何评估它的泛化能力呢？显然需要去度量它的性能，衡量模型泛化能力的评价标准，就叫性能度量。不同的性能度量往往会导致不同的评判结果。</li>
<li>回归任务最常用的性能度量是“均方误差”：
<ul>
<li>离散：<span class="math inline">\(E(f; D) =
\frac1m\sum_{i=1}^m(f(x_i) - y_i)^2\)</span></li>
<li>一般：<span class="math inline">\(E(f; \mathcal{D}) = \int_{x \sim
\mathcal{D}}(f(x) - y)^2p(x)dx\)</span></li>
</ul></li>
<li>ok，接下来介绍分类任务的性能度量</li>
</ul>
<ol type="1">
<li><p>错误率和精度</p>
<ul>
<li>错误率定义
<ul>
<li>离散：<span class="math inline">\(E(f; D) =
\frac1m\sum_{i=1}^m\mathbb{I}(f(x_i) \ne y_i)\)</span></li>
<li>一般：<span class="math inline">\(E(f; \mathcal{D}) = \int_{x \sim
\mathcal{D}}\mathbb{I}(f(x)\ne y)p(x)dx\)</span></li>
</ul></li>
<li>精度定义
<ul>
<li>离散：<span class="math inline">\(acc(f; D) =
\frac1m\sum_{i=1}^m\mathbb{I}(f(x_i)=y_i)=1-E(f; D)\)</span></li>
<li>一般：<span class="math inline">\(acc(f; \mathcal{D}) = \int_{x \sim
\mathcal{D}}\mathbb{I}(f(x)=y)p(x)dx=1-E(f; \mathcal{D})\)</span></li>
</ul></li>
</ul></li>
<li><p>查准率、查全率、F1</p>
<ul>
<li>举个例子，若我们关心“挑出的西瓜中有多少比例是好瓜”以及“所有好瓜中有多少比例被挑了出来”。前者我们用“查准率”(precision)描述，后者用"查全率"(recall)来描述。</li>
<li>对二分类来说，我们将预测结果抽象为混淆矩阵：</li>
<li><img src="1.png" /></li>
<li>（TP, true positive表示它确实是正例，表示我们预测对了。TN, true
negative表示它确实是反例，表示我们预测对了。）</li>
<li>显然$TP + FN + FP + TN = $样例总数</li>
<li>查准率：<span class="math inline">\(P = \frac{TP}{TP +
FP}\)</span></li>
<li>查全率：<span class="math inline">\(R = \frac{TP}{TP +
FN}\)</span></li>
<li>可以发现，查准率和查全率都兼顾有些困难。因为如果想让查全率高，那么就要增加选瓜数量，但是选瓜数量增加后，选出的瓜中是好瓜的概率可能就下降，即查准率下降；若希望选出的好瓜比例高，那么只挑选最有把握的瓜，这样难免就会漏掉不少好瓜，即查全率较低。通常只有在一些简单任务中，才能使P和R都很高。</li>
<li>所以，有没有直观的比较方法呢——PR图。</li>
<li>我们根据学习器的预测结果对样例排序，＂最可能＂是正例的排在最前边或者说最左边，＂最不可能＂是正例的排在最后边或者说最右边．按此顺序逐个把样本作为正例进行预测，每次计算测试样本的查准率和查全率并把这两项作为PR曲线的纵轴和横轴。</li>
<li><img src="3.png" /></li>
<li>显然若一条曲线包住了另一条曲线，那么说明它在任意时刻表现都好。如果俩曲线有相交，那么就比较下俩曲线所形成的面积，谁大谁牛逼。</li>
<li>当然面积可能不好算，所以我们直接用“平衡点”（BEP, 即P =
R时的R坐标）来衡量，谁平衡点大谁牛逼。但是平衡点这方法还是太简陋了，所以我们使用F1度量（谁大谁牛逼）：</li>
</ul>
<p><span class="math display">\[
\frac{1}{F_1} = \frac12\cdot (\frac1P + \frac1R)
\]</span></p>
<ul>
<li>为了更定制化，还可以使用<span
class="math inline">\(F_\beta\)</span>度量（<span
class="math inline">\(F_1\)</span>是调和平均，<span
class="math inline">\(F_\beta\)</span>是加权调和平均）：</li>
</ul>
<p><span class="math display">\[
\frac{1}{F_\beta} = \frac{1}{1+\beta^2}\cdot (\frac1P +
\frac{\beta^2}{R})
\]</span></p>
<ul>
<li><span
class="math inline">\(\beta\)</span>度量了查全率对查准率的相对重要性，<span
class="math inline">\(\beta=1\)</span>为一样重要，<span
class="math inline">\(\beta &gt; 1\)</span>表示我们更重视查全率。</li>
<li>如果有多个混淆矩阵呢？</li>
<li>第一种方法，直接对所有P、R、F1取均值作为最终结果。这样得到的结果叫做：宏查准率、宏查全率、宏F1</li>
<li>第二种方法，先对所有混淆矩阵对应四个位置取均值，再算出对应的P、R、F1。这样得到的结果叫做：微查准率、微查全率、微F1</li>
</ul></li>
<li><p>ROC和AUC</p>
<ul>
<li>ROC曲线跟PR曲线绘制流程一样，只是横坐标换为了“假正例率”（FPR），纵坐标换为了“真正例率”（TPR）。</li>
<li><span class="math inline">\(TPR = \frac{TP}{TP + FN}, \quad FPR =
\frac{FP}{FP + FN}\)</span></li>
<li>TPR表示对于全部好瓜，你预测对了百分之TPR；FPR表示对于全部坏瓜，你预测错了百分之FPR。</li>
<li>显然TPR越高越好，FPR越低越好。</li>
<li>ROC曲线画出来的感觉如下：</li>
<li><img src="2.png" /></li>
<li>AUC是ROC曲线的面积。显然如果一个曲线包住另一个，但它就更牛逼。如果俩线有相交，那么就看看谁的AUC更大，越大越牛逼。</li>
</ul></li>
<li><p>代价敏感错误率和代价曲线</p>
<ul>
<li>在现实中同样是判断错误，但是后果可能不同。比如门禁系统错误的把陌生人放进来的危害肯定比把可通行人拦在外边危害更大。所以同样是判断错误，我们需要赋予其不同的权值。最后的目标是使平均代价最小。</li>
<li>所以可以抽象出代价矩阵的概念：</li>
<li><img src="4.png" /></li>
<li>代价敏感错误率：</li>
</ul>
<p><span class="math display">\[
E(f; D; cost) = \frac1m(\sum_{x_i \in D^+}\mathbb{I}(f(x_i) \ne y_i)
\times cost_{01} + \sum_{x_i \in D^-}\mathbb{I}(f(x_i) \ne y_i) \times
cost_{10})
\]</span></p>
<ul>
<li>当不同后果的权重不同时，上面说的ROC曲线就不能直接反映出学习器的期望总体代价了。所以这时候我们需要“代价曲线”。</li>
<li>略</li>
</ul></li>
</ol>
<h4 id="比较检验">比较检验</h4>
<ul>
<li><p>目前，我们已经可以使用某种模型评估方法，测出某个性能度量的结果。</p></li>
<li><p>但是泛化性能是对新样本进行预测的性能，新样本看成一个总体，那么这个总体我们永远无法完整获得，也就是真实泛化性能永远也不知道是多少。从这个总体中抽样得到一个样本集合，也就是我们通常说的“测试集”，很显然，每次抽样，获得的测试集都不相同，从而从测试集计算得到的性能值也就不同。从测试集得到的性能值可以看成是总体泛化性能的一个估计值，基于这个估计值可以对总体泛化性能进行假设检验和区间估计。</p></li>
<li><p>如果<span
class="math inline">\(\mathcal{D}\)</span>是服从二项分布的。那么测试集样本容量n你是知道的，错误次数k你也是知道的。那么你可以开始玩假设检验。比如假设<span
class="math inline">\(H_0: p_0 \le
0.5\)</span>。然后你就假设你这个假设是对的呗，然后算一算在此假设下，错误次数为k的概率。如果算出来的概率小于<span
class="math inline">\(\alpha\)</span>（显著性水平），说明在此假设下发生这件事的概率极低，那么说明你假设是错的。如果概率大于了<span
class="math inline">\(\alpha\)</span>，说明至少我不能信心满满的否决你的假设了，我只能说我有<span
class="math inline">\(1 -
\alpha\)</span>置信度认为你的假设是正确的。</p></li>
<li><p>很多时候我们并非做一次留出法估计，而是多次。所以我们会得到k个测试错误率。则我们可以计算平均错误率及其方差。那么检验统计量<span
class="math inline">\(\tau_t = \frac{\sqrt{k}(\mu -
\epsilon_0)}{\sigma}\)</span>服从t分布。（<span
class="math inline">\(\mu\)</span>是平均错误率，<span
class="math inline">\(\epsilon_0\)</span>是假设的错误率， <span
class="math inline">\(\sigma\)</span>是前面算的方差）</p></li>
<li><p>如果算出来的<span class="math inline">\(\tau_t\)</span>落在<span
class="math inline">\([t_{-\alpha/2},
t_{\alpha/2}]\)</span>内，则可下置信度为<span class="math inline">\(1 -
\alpha\)</span>的判断认为真实错误率为<span
class="math inline">\(\epsilon_0\)</span>；反之则可下真是错误率不为<span
class="math inline">\(\epsilon_0\)</span>的判断。</p></li>
<li><p>以上俩方法都是关于对单个学习器泛化性能的假设进行的检验，但在实际任务中，更多时候我们需要对不同学习器的性能进行比较，方法有：交叉验证t检验、McNemar检验、Friedman检验、Nemenyi后续检验</p></li>
</ul>
<ol type="1">
<li><p>交叉验证t检验</p>
<ul>
<li>略</li>
</ul></li>
<li><p>McNemar检验</p>
<ul>
<li><p>对二分类问题，使用留出法不仅可估计出学习器A、B的测试错误率，还可以获得俩学习器分类结果的差别，即：两者都分类正确、都错误、A对B错，A错B对的次数，即“列联表”：</p></li>
<li><p><img src="5.png" /></p></li>
<li><p>若我们的假设是俩学习器性能相同，则应有<span
class="math inline">\(e_{01} = e_{10}\)</span>。则<span
class="math inline">\(|e_{01} - e_{10}| \sim N(1, e_{01} +
e_{10})\)</span>。</p></li>
<li><p>则有：<span class="math inline">\(\tau_{\chi^2}=\frac{(|e_{01} -
e_{10}| - 1) ^ 2}{e_{01} + e_{10}}\)</span></p></li>
<li><p>即<span
class="math inline">\(\tau_{\chi^2}\)</span>服从自由度为1的卡方分布。</p></li>
<li><p>当<span
class="math inline">\(\tau_{\chi^2}\)</span>小于临界值<span
class="math inline">\(\chi_\alpha^2\)</span>时，即认为俩学习器性能没有显著差别；反之则认为有显著差别，平均错误率小的那个更牛逼。</p></li>
</ul></li>
<li><p>Friedman检验</p>
<ul>
<li>略</li>
</ul></li>
<li><p>Nemenyi后续检验</p>
<ul>
<li>略</li>
</ul></li>
</ol>
<h4 id="偏差与方差">偏差与方差</h4>
<ul>
<li>为取得良好的泛化能力，则需使偏差较小，此时能够充分拟合数据。并且要使方差较小，因为越小的方差表示受数据扰动的影响小。</li>
<li>但是往往两全不能齐美，称为偏差-方差窘境。</li>
<li>具体内容略。</li>
</ul>
<h3 id="三.-线性模型">三. 线性模型</h3>
<h4 id="线性回归">线性回归</h4>
<p>形如<span class="math inline">\(f(x) = w^\mathrm{T}x +
b\)</span>的，就是线性回归</p>
<p>对于有顺序关系的属性，可以将他们赋值为连续或者离散有顺序的数字。对于无顺序关系的属性，用one-hot。</p>
<p>线性回归的loss函数是MSE。</p>
<p>对于偏置项<span
class="math inline">\(b\)</span>，有一个小技巧就是把它纳入特征里，然后<span
class="math inline">\(w\)</span>里多加一项，这样就相当于<span
class="math inline">\(Xw = \hat{y}\)</span>了。然后如果<span
class="math inline">\(X^\mathrm{T}X\)</span>满秩的话，就直接用线代解出最优解就行了。</p>
<p>如果不满秩，可以引入正则化项，</p>
<h4 id="对数几率回归">对数几率回归</h4>
<p>也叫逻辑回归。</p>
<p>起始就是把前面线性回归<span class="math inline">\(w^\mathrm{T}x +
b\)</span>这个值通过sigmoid函数映射到0 ~
1之间。通常用来作二分类问题。</p>
<p>sigmoid函数：<span class="math inline">\(y = \frac{1}{1 +
e^{-z}}\)</span></p>
<p>逻辑回归：<span class="math inline">\(y = \frac{1}{1 +
e^{-w^\mathrm{T}x + b}}\)</span></p>
<p>另一种写法：<span class="math inline">\(\ln \frac{y}{1 - y} =
w^\mathrm{T}x + b\)</span></p>
<p>其loss函数是交叉熵。</p>
<h4 id="线性判别分析">线性判别分析</h4>
<p>略</p>
<h4 id="多分类学习">多分类学习</h4>
<p>OvO（One vs One）、OvR（One vs Other）、MvM（Many vs Many）</p>
<p>假设有n个类，OvO就是有<span
class="math inline">\(n(n-1)/2\)</span>个二分类器，俩俩类别有一个分类器。对于一个样例，经过这<span
class="math inline">\(n(n-1)/2\)</span>个分类器过一遍，而是看看哪些类别得分最高，就判定这个样例为哪个类别。这就是OvO的思路。</p>
<p>对于OvR，就是对于每个类别有一个分类器，就是分是当前这个类或者不是。所以一共n个分类器。结果就是看这n个分类器哪个的概率最大，就判定这个样例为哪个类别。</p>
<p>MvM就是每次将若干个类作为正类，若干个类作为负类。MvM中，最常见的一种分类技术叫“纠错输出码(ECOC)”</p>
<p>ECOC，具体来说，就是对n个类别做m次划分，每次划分是一个二分类器，将一部分类别划为正类，一部分划为负类。然后对于一个样例，经过这m个分类器跑一遍，得到预测向量，看看预测向量与类别自身向量的欧氏距离或者海明距离。去差距最小的那个类别作为预测值。</p>
<p>具体看下面这幅图，一目了然。</p>
<p><img src="6.png" /></p>
<p>上面这幅图中，一共有5个分类器，4个类别。然后对于一个样例，经过5个分类器，跑出了(-1,
-1, +1, -1, +1)这个预测向量。记此向量为x。那么x与<span
class="math inline">\(C_1\)</span>的向量(-1, +1, -1, +1,
+1)的欧氏距离是<span
class="math inline">\(2\sqrt{3}\)</span>，海明距离(即不同的个数)是3。</p>
<p>通过观察，可以发现预测向量与<span
class="math inline">\(C_3\)</span>的欧氏距离和海明距离均最小，那么就判定该样例属于<span
class="math inline">\(C_3\)</span>。</p>
<p>可以发现，EOOC编码越长(分类器越多)，那么纠错能力越强(鲁棒性越好)。</p>
<p>而且可以发现，两个类别<span class="math inline">\(C_i,
C_j\)</span>的编码距离越远越好，这样子区分度就越高。所以我们称任意俩类别之间编码距离最远的编码方式为理论最优编码。</p>
<h4 id="类别不平衡问题">类别不平衡问题</h4>
<p>如果有很多个类别，但是训练集中每个类别的样本数不同，这样会不好。所以说有三种方式解决这个问题(假设正样本多，负样本少)：第一就是删掉一些正样本，第二就是增加一些负样本，第三就是使用“再放缩”。</p>
<p>什么叫“再放缩”，具体来说，原本二分类逻辑回归实际是在执行：若<span
class="math inline">\(\frac{y}{1 - y} &gt;
1\)</span>，则预测为正例。那么改一改，改为：若<span
class="math inline">\(\frac{y}{1 - y} &gt;
\frac{m+}{m-}\)</span>，则预测为正例，其中<span
class="math inline">\(m+\)</span>是正样本数，<span
class="math inline">\(m-\)</span>是负样本数。</p>
<h3 id="四.-决策树">四. 决策树</h3>
<h4 id="基本流程">基本流程</h4>
<p>就是一种模拟人脑决策的模型。</p>
<p>举个例子，就明白了。</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>性别</th>
<th>年龄</th>
<th>城市</th>
<th>类别</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>A</td>
<td>男</td>
<td>老年</td>
<td>北京</td>
<td>1</td>
</tr>
<tr class="even">
<td>B</td>
<td>男</td>
<td>老年</td>
<td>上海</td>
<td>1</td>
</tr>
<tr class="odd">
<td>C</td>
<td>女</td>
<td>青年</td>
<td>北京</td>
<td>1</td>
</tr>
<tr class="even">
<td>D</td>
<td>女</td>
<td>中年</td>
<td>北京</td>
<td>1</td>
</tr>
<tr class="odd">
<td>E</td>
<td>女</td>
<td>青年</td>
<td>北京</td>
<td>2</td>
</tr>
<tr class="even">
<td>F</td>
<td>女</td>
<td>青年</td>
<td>北京</td>
<td>2</td>
</tr>
<tr class="odd">
<td>G</td>
<td>女</td>
<td>中年</td>
<td>北京</td>
<td>2</td>
</tr>
<tr class="even">
<td>H</td>
<td>女</td>
<td>中年</td>
<td>上海</td>
<td>2</td>
</tr>
</tbody>
</table>
<p>ok来吧，把决策树建出来把。</p>
<p>首先先按性别分呗，那么就会分出：</p>
<p><img src="7.png" style="zoom: 50%;" /></p>
<p>因为A、B都属于1类，属于没必要继续分下去了，所以直接以叶子节点类别1结束。</p>
<p>ok，那么就右边需要分了，性别这个属性用完了，那么就用年龄开分！</p>
<p><img src="8.png" style="zoom:67%;" /></p>
<p>老年，因为没有样本被分到这，所以这里的叶子节点的类别就要看父节点，也就是（C、D、E、F、G、H
+
年龄/城市）这个点哪个类别多，可以发现属于类别2的人多，所以老年这个节点就以类别2的叶子节点结束。</p>
<p>然后可以目光看向青年，发现C、E、F都在北京，所以没什么好分的了，那么直接接叶子节点，类别就看看哪个类别人多就哪个，E、F都是2，所以接类别2的叶子节点。</p>
<p>还剩（D、G、H + 城市）这个点了，继续分！</p>
<p><img src="9.png" /></p>
<p>分完H那个点只有它自己了，所以直接接自己类别的叶子节点即可。</p>
<p>然后（D、G）这个点，因为没有属性可用了，所以不用分了。哪个类别多接哪个类别的叶子节点。发现D是1，G是2，一样多，那随便了，我就接了一个类别为1的叶子节点。</p>
<p>至此，我们利用这个数据集，建出了一颗决策树！</p>
<p>相信聪明的你也发现了，我们用按顺序去用每一个属性的，那能不能调换顺序呢？顺序有什么讲究呢？这里头是有学问的，留到下一节讲。</p>
<h4 id="划分选择">划分选择</h4>
<ul>
<li>决策树学习的关键在于如何选择最优划分属性，一般而言，随着划分过程不断进行，我们希望决策树的分支节点所包含的样本尽可能属于同一类别。</li>
<li>经典的属性划分方法有：信息增益、增益率、基尼指数</li>
</ul>
<h5 id="信息增益">信息增益</h5>
<ul>
<li><p>首先要知道什么是信息量，信息量具有以下几个性质：</p>
<ol type="1">
<li>非负性：不存在负信息量的事件</li>
<li>单调性：事件发生的概率越大，其信息量越小</li>
<li>累加性：多个相互独立事件信息量等于各事件信息量之和</li>
</ol></li>
<li><p>香农提出了信息量这个概念，然后他用了一个数学函数，以满足上面的三条性质：<span
class="math inline">\(I(x) = \log \frac{1}{P(x)} = -\log
P(x)\)</span></p>
<ul>
<li><p><span class="math inline">\(I(x)\)</span>是事件<span
class="math inline">\(x\)</span>的信息量，<span
class="math inline">\(P(x)\)</span>是事件<span
class="math inline">\(x\)</span>发生的概率</p></li>
<li><p>图像长这样：</p>
<p><img src="10.png" style="zoom: 80%;" /></p></li>
</ul></li>
<li><p>信息量是针对某个具体事件的一种度量方式，香农觉得不够，于是定义了一个名叫“信息熵”的概念。信息熵度量的是随机变量的不确定性：<span
class="math inline">\(H(X) = E[-\log P(X)] = -\sum_{x \in X} P(x)\log
P(x)\)</span></p>
<ul>
<li>信息熵其实就是“随机变量的信息量的期望”，展开后就是所有事件概率与其信息量乘积之和</li>
<li><span class="math inline">\(H(X)\)</span>是随机变量<span
class="math inline">\(X\)</span>的信息熵</li>
<li><span class="math inline">\(H(X)\)</span>是非负的，如果<span
class="math inline">\(X\)</span>的可能性越多(越混乱)，那么<span
class="math inline">\(H(X)\)</span>越大</li>
</ul></li>
<li><p>条件熵也是一种信息熵，它考虑的是有两个随机变量的情况，且其中一个随机变量已知的情况时的期望信息熵：</p></li>
</ul>
<p><span class="math display">\[
\begin{align*}
H(Y | X) &amp;= \sum_{x \in X}P(x)H(Y|X=x) \\
&amp;= -\sum_{x \in X}P(x) \sum_{y \in Y} P(y|x)\log P(y|x) \\
&amp;= -\sum_{x \in X}\sum_{y \in Y}P(x,y)\log P(y|x)
\end{align*}
\]</span></p>
<hr />
<ul>
<li>OK，知道了这几个概念，我们来看看如何将信息熵的概念用到决策树的划分选择上！</li>
<li>对于样本集D，每个样本不是有一个label嘛，假设有k种label，那么样本集D的信息熵<span
class="math inline">\(H(D)\)</span>就为：</li>
</ul>
<p><span class="math display">\[
H(D) = -\sum p_k \cdot \log p_k
\]</span></p>
<ul>
<li><span
class="math inline">\(p_k\)</span>是第k个label占全部label的比例。这个定义很好理解，如果全部样本都是同一个label，那么信息熵就是0。如果有很多个label，那么信息熵就比较大，说明混乱程序比较高，不确定性程度高。</li>
<li>那如何知道按照什么属性划分会比较好呢？我们希望的是划分之后所产生的子数据集的纯度越高越好，所以有一个很自然的想法就冒出来了：我要找到一个属性A，使得<span
class="math inline">\(H(D) -
H(D|A)\)</span>最大，即划分后的信息熵尽可能小，这样就满足了划分后数据集纯度尽可能大的要求了。bingo，<span
class="math inline">\(H(D) -
H(D|A)\)</span>有个名字，“信息增益”。信息增益越大，意味着使用属性A来进行划分所获得的纯度提升越大。</li>
<li>用数学公式表示maybe会更直观些：</li>
</ul>
<p><span class="math display">\[
\begin{align*}
    &amp;G(D, A) = H(D) - H(D | A) \\
    &amp;H(D) = -\sum p_k \cdot \log p_k \\
    &amp;H(D | A) = \sum_{i=1}^{C} \frac{cnt_{D_i}}{cnt_D} \cdot H(D_i)
\end{align*}
\]</span></p>
<ul>
<li><p><span class="math inline">\(G(D, A)\)</span>就是考虑用属性<span
class="math inline">\(A\)</span>进行划分时的信息增益；<span
class="math inline">\(cnt_D\)</span>是数据集<span
class="math inline">\(D\)</span>的样本数量；<span
class="math inline">\(cnt_{D_i}\)</span>是数据集按照<span
class="math inline">\(A\)</span>划分后，第<span
class="math inline">\(i\)</span>类的样本数量；<span
class="math inline">\(C\)</span>是按照<span
class="math inline">\(A\)</span>划分后，划分出的类别数。</p></li>
<li><p>至此，如何构建一棵决策树就显然易见了，是一个递归的过程，每次先算出当前数据集D的信息熵<span
class="math inline">\(H(D)\)</span>，然后依次算当前所有属性划分划分之后的信息熵<span
class="math inline">\(H(D|(A/B/C/...))\)</span>，看看哪个的信息增益越大，选最大的划分，划分后得到了新的数据集<span
class="math inline">\(D&#39;\)</span>和属性集，然后对新的数据集和属性集递归做就好了。递归终止条件就是当前数据集的label全一样，或者属性集空了，或者数据集空了。</p></li>
<li><p>上面就是用信息增益(ID3)来建树的算法</p></li>
</ul>
<h5 id="增益率c4.5">增益率、C4.5</h5>
<ul>
<li><p>上面的信息增益算法其实我们没考虑到一个东西，就是不同属性天生包含的类别数不同，例如属性A可能有高、中、矮，但是属性B可能只有瘦、胖。类别多的属性它混乱的几率就会大。为了考虑这一点，所以我们在信息增益的基础上，引入增益率的概念。</p></li>
<li><p>直接给出数学定义，<span class="math inline">\(G(D,
A)\)</span>是信息增益(Gain)，<span class="math inline">\(Gr(D,
A)\)</span>是增益率(Gain ratio)</p></li>
</ul>
<p><span class="math display">\[
\begin{align*}
    &amp;G(D, A) = H(D) - H(D | A) \\
    &amp;Gr(D, A) = \frac{G(D, A)}{H(A)} \\
    &amp;H(A) = -\sum p_k \log p_k
\end{align*}
\]</span></p>
<ul>
<li><p>例如属性A有2个类别，第一个类别占1/2，第二个类别占1/2，那么<span
class="math inline">\(H(A)\)</span>就是<span
class="math inline">\(-(\frac12 \cdot \log \frac12 \cdot
2)=1\)</span></p></li>
<li><p>稍微改进一下上面的算法，把每次选择最大信息增益改为选择最大增益率的属性。</p></li>
<li><p>但简单这样改还不够好，因为增益率对类别数少的属性有偏好。</p></li>
<li><p>所以C4.5算法就说：先选出信息增益高于平均信息增益的属性，然后再在这些属性中选出增益率最高的属性作为划分属性！</p></li>
</ul>
<h5 id="基尼指数">基尼指数</h5>
<ul>
<li>OK，前面用熵去衡量数据集“纯度”的做法确实很优美。那现在，基尼指数就是另一种衡量数据集“纯度”的东西。</li>
<li>数据集<span
class="math inline">\(D\)</span>的纯度可用基尼值(Gini)来衡量：</li>
</ul>
<p><span class="math display">\[
Gn(D) = \sum_{i=1}^{C}\sum_{j=1}^{C}p_ip_j = 1 - \sum_{k=1}^{C}p_k^2,
\quad i \ne j
\]</span></p>
<ul>
<li>可以发现，基于指数反映了从<span
class="math inline">\(D\)</span>中随机抽取两个样本，其类别lable不一致的概率。假设有2种label，且样本数对半开，那么<span
class="math inline">\(Gn(D)\)</span>就是<span
class="math inline">\(\frac12 \cdot \frac12 =
\frac14\)</span>；假设有2种lable，且样本数为9:1，那么<span
class="math inline">\(Gn(D)\)</span>就是<span
class="math inline">\(\frac{9}{10} \cdot \frac{1}{10} =
\frac{9}{100}\)</span>。可以发现，基尼系数越小，数据集纯度越高。</li>
<li>那么用属性A划分后的基尼指数是多少？</li>
</ul>
<p><span class="math display">\[
Gn(D, A) = \sum_{k=1}^{C} \frac{cnt_{D_i}}{cnt_D}Gn(D_i)
\]</span></p>
<ul>
<li>CART算法就是利用基尼指数来建决策树的算法，它每次看看用哪个属性划分后的基尼指数最小，就用它划分。</li>
</ul>
<h4 id="剪枝处理">剪枝处理</h4>
<ul>
<li><p>剪枝的目的，就是为了避免把训练集自身的一些特点当作所有数据都具有的一般性质而导致的过拟合。</p></li>
<li><p>剪枝的基本策略分为预剪枝和后剪枝。</p></li>
<li><p>判断决策树泛化性能是否提升的方法：留出法(留出一部分作为验证集)</p></li>
</ul>
<h5 id="预剪枝">预剪枝</h5>
<ul>
<li>很简单，就是在划分节点的时候，拿验证集跑一跑，如果划分后效果反而不如不划分，那么就不继续划分该节点了，直接连个叶子节点上去，类别就是该节点内人数最多的类别。（从上到下）</li>
<li>优点：降低过拟合风险，显著减少训练时间</li>
<li>缺点：欠拟合风险</li>
</ul>
<h5 id="后剪枝">后剪枝</h5>
<ul>
<li>很简单，就是先用算法生成一棵决策树，然后从下到上依次考察是否将节点替换为叶子节点会更优，如果更优，就替换</li>
<li>优点：比预剪枝保留了更多分支，欠拟合风险更小</li>
<li>缺点：训练时间大</li>
</ul>
<h4 id="连续与缺失值">连续与缺失值</h4>
<p>连续值和缺失值都是针对属性的值说的。因为在前面讨论的决策树中，我们属性的值都是离散的，例如高、矮、胖、瘦，都是离散的。而现实中可能会存在很多连续值，例如身高是多少cm；也有可能会存在缺失值，比如有个属性是自我性别认知，可能有些人是未知。</p>
<p>首先来看如何处理连续值吧，思路很简单，就是设立划分点，就拿身高这个属性举例，假设样本中，有n个不同的身高，那么划分点就有n个，每确定一个划分点t，其实就可以算出按照t去划分身高这个属性之后得到的信息增益<span
class="math inline">\(G(D, A,
t)\)</span>。以用信息增益算法建树举例，那么划分身高这个属性的时候，就是找一个划分点t，使得<span
class="math inline">\(H(D) - G(D, A, t)\)</span>最大即可。</p>
<p>需要注意的是，连续值与离散值有一个地方不同就是，离散值的属性如果用过，那么后面就不会再用来划分了。但是连续值的属性可以再次使用，比如第一次划分是身高是否低于180，进入子节点后可以继续用身高这个属性划分，身高是否低于160。</p>
<p>OK，现在来讨论缺失值。</p>
<p>还是用信息增益算法来举例，当你计算<span class="math inline">\(H(D),
G(D, A)\)</span>的时候，<span
class="math inline">\(D\)</span>就是剔除<span
class="math inline">\(A\)</span>属性有缺失值得到样本子集，然后信息增益就是<span
class="math inline">\(\rho(H(D) - G(D, A))\)</span>，<span
class="math inline">\(\rho\)</span>是无缺失值样本数量占总样本数量的比例。然后选择信息增益最大的属性作为当前的划分属性。</p>
<p>ok，划分属性确定好了，如何划分呢？答案就是给每个样本一个全局变量<span
class="math inline">\(w_i\)</span>，为自己的权重，初始为1。对于那些属性<span
class="math inline">\(A\)</span>无缺失的样本呢，直接划分到对应子集中，对应进入的权重为<span
class="math inline">\(w_i\)</span>；对于那些属性<span
class="math inline">\(A\)</span>缺失的样本，就等无缺失的样本都划分完后，然后划分到每一个子集中，对应进入的权重变为<span
class="math inline">\(w_i \cdot
\frac{\sum\text{子集中样本的}w_i}{\sum\text{当前属性无缺失的样本的}w_i}\)</span>。同时将<span
class="math inline">\(\frac{\sum\text{子集中样本的}w_i}{\sum\text{当前属性无缺失的样本的}w_i}\)</span>记录下来作为该子节点的权重（后面验证时会用到）。</p>
<p>那么按照这样的算法，决策树就可以建立起来了。</p>
<p>那么验证时，对于属性缺失的样本，它该进入哪呢？</p>
<p>答案就是当建树的时候发现有缺失时，在验证的时候，就给每一个要验证的样本带一个权重<span
class="math inline">\(w\)</span>，初始值为1。假设走到某个属性A，若该样本在属性A上无缺失，则进入到对应子节点to，<span
class="math inline">\(dfs(to,
w)\)</span>；若有缺失，则dfs每个子集都进入，但是进入的权重要乘对应子节点的权重，<span
class="math inline">\(dfs(to_i, w \cdot
w_i)\)</span>，对应子节点的权重在建树时已经算好。那么最终会进入到多个叶子节点，选择权重最大的叶子节点的类别判定为该样本的label。</p>
<p>但是这么做，可能会存在对应子节点没有权重，也就是训练时该属性无缺失的情况，那么此时也还是要dfs每个子集都进入，只是把进入的权重改为乘<span
class="math inline">\(\frac{1}{\text{该节点儿子个数}}\)</span>，即等比例进入每个儿子节点。</p>
<h4 id="多变量决策树">多变量决策树</h4>
<p>略</p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2024/09/18/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B1/" rel="prev" title="微分方程1">
      <i class="fa fa-chevron-left"></i> 微分方程1
    </a></div>
      <div class="post-nav-item">
    <a href="/2024/09/20/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/" rel="next" title="操作系统自学笔记">
      操作系统自学笔记 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    
  <div class="comments">
    <div id="lv-container" data-id="city" data-uid="MTAyMC81OTA0Ny8zNTUwOQ=="></div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%80.-%E7%BB%AA%E8%AE%BA"><span class="nav-text">一. 绪论</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BA%8C.-%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9"><span class="nav-text">二. 模型评估与选择</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E6%96%B9%E6%B3%95"><span class="nav-text">模型评估方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F"><span class="nav-text">性能度量</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%AF%94%E8%BE%83%E6%A3%80%E9%AA%8C"><span class="nav-text">比较检验</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%81%8F%E5%B7%AE%E4%B8%8E%E6%96%B9%E5%B7%AE"><span class="nav-text">偏差与方差</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%89.-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B"><span class="nav-text">三. 线性模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="nav-text">线性回归</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AF%B9%E6%95%B0%E5%87%A0%E7%8E%87%E5%9B%9E%E5%BD%92"><span class="nav-text">对数几率回归</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90"><span class="nav-text">线性判别分析</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A4%9A%E5%88%86%E7%B1%BB%E5%AD%A6%E4%B9%A0"><span class="nav-text">多分类学习</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%B1%BB%E5%88%AB%E4%B8%8D%E5%B9%B3%E8%A1%A1%E9%97%AE%E9%A2%98"><span class="nav-text">类别不平衡问题</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%9B.-%E5%86%B3%E7%AD%96%E6%A0%91"><span class="nav-text">四. 决策树</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E6%B5%81%E7%A8%8B"><span class="nav-text">基本流程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%92%E5%88%86%E9%80%89%E6%8B%A9"><span class="nav-text">划分选择</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A"><span class="nav-text">信息增益</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%A2%9E%E7%9B%8A%E7%8E%87c4.5"><span class="nav-text">增益率、C4.5</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%9F%BA%E5%B0%BC%E6%8C%87%E6%95%B0"><span class="nav-text">基尼指数</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%89%AA%E6%9E%9D%E5%A4%84%E7%90%86"><span class="nav-text">剪枝处理</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%A2%84%E5%89%AA%E6%9E%9D"><span class="nav-text">预剪枝</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%90%8E%E5%89%AA%E6%9E%9D"><span class="nav-text">后剪枝</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BF%9E%E7%BB%AD%E4%B8%8E%E7%BC%BA%E5%A4%B1%E5%80%BC"><span class="nav-text">连续与缺失值</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A4%9A%E5%8F%98%E9%87%8F%E5%86%B3%E7%AD%96%E6%A0%91"><span class="nav-text">多变量决策树</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Error_666"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">Error_666</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">39</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/potatoQi" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;potatoQi" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://gitee.com/Error_666" title="Gitee → https:&#x2F;&#x2F;gitee.com&#x2F;Error_666" rel="noopener" target="_blank"><i class="fa fa-paper-plane fa-fw"></i>Gitee</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.acwing.com/user/myspace/index/8252/" title="AcWing → https:&#x2F;&#x2F;www.acwing.com&#x2F;user&#x2F;myspace&#x2F;index&#x2F;8252&#x2F;" rel="noopener" target="_blank"><i class="fas fa-rocket fa-fw"></i>AcWing</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://cn.overleaf.com/project" title="Overleaf → https:&#x2F;&#x2F;cn.overleaf.com&#x2F;project" rel="noopener" target="_blank"><i class="fa fa-edit fa-fw"></i>Overleaf</a>
      </span>
  </div>



  <div class="links-of-recent-posts motion-element">
    <div class="links-of-recent-posts-title">
      <i class="fa fa-history fa-fw"></i>
      最近文章
    </div>
    <ul class="links-of-recent-posts-list">
        <li class="links-of-recent-posts-item">
          <a href="/2024/10/06/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/" title="2024&#x2F;10&#x2F;06&#x2F;强化学习2&#x2F;">强化学习2</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2024/10/05/%E5%8A%A8%E6%89%8B%E5%AD%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" title="2024&#x2F;10&#x2F;05&#x2F;动手学强化学习&#x2F;">动手学强化学习</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2024/10/03/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A01/" title="2024&#x2F;10&#x2F;03&#x2F;强化学习1&#x2F;">强化学习1</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2024/10/02/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B2/" title="2024&#x2F;10&#x2F;02&#x2F;微分方程2&#x2F;">微分方程2</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2024/09/20/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/" title="2024&#x2F;09&#x2F;20&#x2F;操作系统自学笔记&#x2F;">操作系统自学笔记</a>
        </li>
    </ul>
  </div>
      </div>
	<br>
    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2023-10-1 – 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Error_666</span>
</div>

<!--
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>
 -->


    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <span id="busuanzi_container_site_pv">总访问量<span id="busuanzi_value_site_pv"></span>次</span>
    <span class="post-meta-divider">|</span>
    <span id="busuanzi_container_site_uv">总访客数<span id="busuanzi_value_site_uv"></span>人</span>
    <span class="post-meta-divider">|</span>
<!-- 不蒜子计数初始值纠正 -->
<script>
$(document).ready(function() {

    var int = setInterval(fixCount, 50);  // 50ms周期检测函数
    var countOffset = 20000;  // 初始化首次数据

    function fixCount() {            
       if (document.getElementById("busuanzi_container_site_pv").style.display != "none")
        {
            $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + countOffset); 
            clearInterval(int);
        }                  
        if ($("#busuanzi_container_site_pv").css("display") != "none")
        {
            $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + countOffset); // 加上初始数据 
            clearInterval(int); // 停止检测
        }  
    }
       	
});
</script> 

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  
  <script>
    (function(){
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x=document.getElementsByTagName("link");
		//Find the last canonical URL
		if(x.length > 0){
			for (i=0;i<x.length;i++){
				if(x[i].rel.toLowerCase() == 'canonical' && x[i].href){
					canonicalURL=x[i].href;
				}
			}
		}
    //Get protocol
	    if (!canonicalURL){
	    	curProtocol = window.location.protocol.split(':')[0];
	    }
	    else{
	    	curProtocol = canonicalURL.split(':')[0];
	    }
      //Get current URL if the canonical URL does not exist
	    if (!canonicalURL) canonicalURL = window.location.href;
	    //Assign script content. Replace current URL with the canonical URL
      !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=canonicalURL,t=document.referrer;if(!e.test(r)){var n=(String(curProtocol).toLowerCase() === 'https')?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";t?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var i=new Image;i.src=n}}(window);})();
  </script>




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

<script>
NexT.utils.loadComments(document.querySelector('#lv-container'), () => {
  window.livereOptions = {
    refer: location.pathname.replace(CONFIG.root, '').replace('index.html', '')
  };
  (function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
  })(document, 'script');
});
setInterval(function () {
    var box = document.querySelector(".trc_rbox_container");
    if(box) box.outerHTML = "";
}, 2000);
</script>

</body>
</html>
