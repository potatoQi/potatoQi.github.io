<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta name="google-site-verification" content="Nju-Ov4omLH_XBGFv1RDU5xdZaGOW05OvwIjIu_498Q">
  <meta name="msvalidate.01" content="2CE08595DA3460134EDC20E4D8663AD4">
  <meta name="baidu-site-verification" content="codeva-j0Zy96Rpmy">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"error666.top","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"manual","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.json"};
  </script>

  <meta name="description" content="看学校ppt + 西瓜书的一些总结">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习自学笔记">
<meta property="og:url" content="http://error666.top/2024/12/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/index.html">
<meta property="og:site_name" content="Error_666">
<meta property="og:description" content="看学校ppt + 西瓜书的一些总结">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://error666.top/2024/12/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/1.png">
<meta property="og:image" content="http://error666.top/2024/12/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/image-20241205233706344.png">
<meta property="og:image" content="http://error666.top/2024/12/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/image-20241206192619955.png">
<meta property="og:image" content="http://error666.top/2024/12/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/image-20241206195849226.png">
<meta property="og:image" content="http://error666.top/2024/12/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/image-20241206200121089.png">
<meta property="og:image" content="http://error666.top/2024/12/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/91157626bd33416d4cff2cf0b5fa643.jpg">
<meta property="og:image" content="http://error666.top/2024/12/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/ac38e05ba9b7237f1617037b6e8b920.jpg">
<meta property="og:image" content="http://error666.top/2024/12/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/image-20241207002341592.png">
<meta property="og:image" content="http://error666.top/2024/12/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/image-20241207182842913.png">
<meta property="og:image" content="http://error666.top/2024/12/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/image-20241207192319312.png">
<meta property="og:image" content="http://error666.top/2024/12/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/image-20241207192436420.png">
<meta property="og:image" content="http://error666.top/2024/12/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/image-20241207010044315.png">
<meta property="og:image" content="http://error666.top/2024/12/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/image-20241207010250215.png">
<meta property="og:image" content="http://error666.top/2024/12/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/image-20241207011345869.png">
<meta property="og:image" content="http://error666.top/2024/12/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/5a3e5bc7c09017ce20fa869b07f577c.jpg">
<meta property="og:image" content="http://error666.top/2024/12/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/image-20241208000557083.png">
<meta property="og:image" content="http://error666.top/2024/12/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/image-20241208001845615.png">
<meta property="og:image" content="http://error666.top/2024/12/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/image-20241208165450045.png">
<meta property="og:image" content="http://error666.top/2024/12/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/image-20241217001634379.png">
<meta property="og:image" content="http://error666.top/2024/12/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/image-20241217001700091.png">
<meta property="og:image" content="http://error666.top/2024/12/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/image-20241208173142846.png">
<meta property="og:image" content="http://error666.top/2024/12/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/image-20241208224231596.png">
<meta property="og:image" content="http://error666.top/2024/12/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/image-20241208225104052.png">
<meta property="og:image" content="http://error666.top/2024/12/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/image-20241208230513550.png">
<meta property="og:image" content="http://error666.top/2024/12/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/image-20241209004435722.png">
<meta property="og:image" content="http://error666.top/2024/12/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/image-20241209004608662.png">
<meta property="og:image" content="http://error666.top/2024/12/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/image-20241209004738727.png">
<meta property="og:image" content="http://error666.top/2024/12/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/image-20241209004828626.png">
<meta property="og:image" content="http://error666.top/2024/12/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/image-20241209005125739.png">
<meta property="og:image" content="http://error666.top/2024/12/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/image-20241209005254974.png">
<meta property="og:image" content="http://error666.top/2024/12/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/image-20241209010527965.png">
<meta property="og:image" content="http://error666.top/2024/12/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/image-20241209022227832.png">
<meta property="og:image" content="http://error666.top/2024/12/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/image-20241216215310297.png">
<meta property="article:published_time" content="2024-12-18T17:07:27.000Z">
<meta property="article:modified_time" content="2024-12-31T17:16:40.283Z">
<meta property="article:author" content="Error_666">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://error666.top/2024/12/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/1.png">

<link rel="canonical" href="http://error666.top/2024/12/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>机器学习自学笔记 | Error_666</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

  <script src="/live2d-widget/autoload.js"></script>
<link rel="alternate" href="/atom.xml" title="Error_666" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Error_666</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-tools">

    <a href="/tools/" rel="section"><i class="fa fa-toolbox fa-fw"></i>工具</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://error666.top/2024/12/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Error_666">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Error_666">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          机器学习自学笔记
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-12-19 01:07:27" itemprop="dateCreated datePublished" datetime="2024-12-19T01:07:27+08:00">2024-12-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-01-01 01:16:40" itemprop="dateModified" datetime="2025-01-01T01:16:40+08:00">2025-01-01</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/4-%E5%A4%A7%E5%AD%A6/" itemprop="url" rel="index"><span itemprop="name">4. 大学</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/4-%E5%A4%A7%E5%AD%A6/%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%93%E4%B8%9A%E8%AF%BE/" itemprop="url" rel="index"><span itemprop="name">计算机专业课</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>12k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>11 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>看学校ppt + 西瓜书的一些总结</p>
<span id="more"></span>
<h3 id="绪论">1 绪论</h3>
<ol type="1">
<li><p>什么是机器学习</p>
<ul>
<li>通过算法使得机器能从大量数据中学习规律从而对新的样本做决策。（相当于构建一个映射函数）</li>
</ul></li>
<li><p>机器学习研究的主要内容</p>
<ul>
<li>在计算机上从数据中产生“模型”的算法，即“学习算法”。我们把经验数据提供给它，它就能基于这些数据产生模型。在面对新的情况时，模型会给我们提供相应的判断</li>
</ul></li>
<li><p>常见的机器学习基本任务</p>
<ul>
<li>分类：是定性输出，输出类型是离散数据，是离散变量预测</li>
<li>回归：是定量输出，输出类型是连续数据，是连续变量预测</li>
</ul></li>
<li><p>常见的机器学习方法/类型(按学习形式分类)</p>
<ul>
<li>监督学习：数据都有明确的标签，根据机器学习产生的模型可以将新数据分到一个明确的类或得到一个预测值。</li>
<li>无监督学习：数据没有标签，机器学习出的模型是从数据中提取出来的模式（提取决定性特征或者聚类等）</li>
<li>半监督学习：利用少量的标记样本和大量的未标记样本来改善模型的学习能力。位于无监督学习和监督学习之间。</li>
</ul></li>
<li><p>几个常见的术语：数据集、样本(示例)、特征(属性)、样本空间(属性空间/输入空间)、特征向量、维数</p>
<ul>
<li>一行记录就是一个样本，然后字段就是特征(属性)，样本空间就是轴名字为所有字段的一个空间，特征向量就是(字段1,
字段2, ...)，维数就是字段的个数。</li>
</ul></li>
<li><p>独立同分布：是指随机过程中，任何时刻的取值都为随机变量，如果这些随机变量服从同一分布，并且互相独立，那么这些随机变量是独立同分布。</p></li>
<li><p>假设空间、版本空间、归纳偏好</p>
<ul>
<li><p>假设空间：属性所有可能取值的组合。</p>
<p><img src="1.png" style="zoom:67%;" /></p></li>
<li><p>版本空间：与训练集一致的“假设集合”。</p>
<ul>
<li><p>求法：把跟正例不一致的假设删除，跟反例一至的假设删除。</p>
<p><img src="image-20241205233706344.png" alt="" style="zoom:67%;" /></p></li>
</ul></li>
<li><p>归纳偏好：在学习过程中对某种类型假设的偏好，称为归纳偏好</p>
<ul>
<li>例如，上面西瓜的例子求出的版本空间中有三个假设，它们都能满足训练集，但是面对新样本时，会产生不同的输出。所以选择特殊点的假设呢？还是选一般点的假设呢？</li>
<li>奥卡姆剃刀原则：选最简单的假设。</li>
</ul></li>
</ul></li>
<li><p>没有免费的午餐定理：在所有“问题”出现机会相同的前提下，所有算法的期望性能相同。</p></li>
</ol>
<h3 id="模型评估与选择">2 模型评估与选择</h3>
<ol type="1">
<li><p>几个误差</p>
<ul>
<li>训练误差：训练集上的训练所得模型与目标的差异</li>
<li>泛化误差：在未见过的数据集上训练所得模型与目标的差异</li>
<li>测试误差：测试集上的训练所得模型与目标的差异</li>
</ul></li>
<li><p>过拟合/欠拟合</p>
<ul>
<li>过拟合：为了得到一致假设而使假设变得过度严格（解决方法：模型剪枝/减少参数量/增加训练数据量/使用正则化约束）</li>
<li>欠拟合：模型没有很好地捕捉到数据特征，不能够很好地拟合数据（解决方法：模型复杂化/增加特征数/降低正则化约束）</li>
</ul></li>
<li><p>评估方法</p>
<ul>
<li>K折交叉验证法：
<ul>
<li>你拿到一个数据集，先均分为k个互斥子集(k折)，依次将每一个子集作为测试集，剩余k-1个子集作为训练集，完成k次训练与测试，记录下每次的测试误差</li>
<li>进行完k次训练与测试后，对k个测试误差求均值。然后把均值测试误差最小的那个model选出来。它就是最优秀的model。</li>
<li>上述过程就是1次k折交叉验证。一般可以做p次，即p次k折交叉验证。
<ul>
<li>细节：采样记得分层采样，即如果男女比例3:7，那么train,
test子集里的男女比率都要为3:7。</li>
<li>留一法：若数据集有m个样本，那么就拆为m份，每次只有一个样本作为测试集，进行m次。</li>
</ul></li>
</ul></li>
<li>留出法：
<ul>
<li>无脑。就是选一部分作为训练集，另一部分作为测试集。结束。</li>
</ul></li>
<li>自助法：
<ul>
<li>若数据集<span
class="math inline">\(D\)</span>有m个样本，那么进行m次有放回采样，每次采样到的样本copy到<span
class="math inline">\(D_1\)</span>中。</li>
<li>采样m次后，将<span
class="math inline">\(D_1\)</span>作为训练集，<span
class="math inline">\(D/D_1\)</span>作为测试集，结束。</li>
<li>因为从概率上来讲，有<span
class="math inline">\(\frac{1}{e}=36.8\%\)</span>样本一次都不会被采样过，所以<span
class="math inline">\(D/D_1\)</span>作为测试集从概率上来讲就是<span
class="math inline">\(36.8\%\)</span>的数据集。比较不错。<span
class="math inline">\(\lim_{m\mapsto\infty}\left(1-\frac{1}{m}\right)^m\mapsto\frac{1}{e}\approx0.368\)</span></li>
</ul></li>
<li>优缺点：
<ul>
<li>留出法简单易懂计算量小，但是结果可信度不算很高；K折交叉验证法可信度高，但计算量大；留出法在小数据集时表现较好，但是引入了估计偏差，但大数据集时非常明显。</li>
</ul></li>
</ul></li>
<li><p>性能度量1️⃣</p>
<ul>
<li><p>回归问题一般用MSE(均方误差)：<span
class="math inline">\(E(f;D)={\frac{1}{m}}\sum_{i=1}^{m}\left(f\left(x_{i}\right)-y_{i}\right)^{2}\)</span></p></li>
<li><p>分类问题一般用错误率和精度。除此之外，还有P/R/F1/<span
class="math inline">\(F_\beta\)</span></p>
<p><img src="image-20241206192619955.png" alt="" style="zoom: 50%;" /></p></li>
<li><p>正例(Positive sample)，反例(Negative sample)</p></li>
<li><p>查准率(P)：你查的正例里对了多少？<span
class="math inline">\(\frac{TP}{TP + FP}\)</span></p></li>
<li><p>查全率(R)：正例都被查出来了吗？<span
class="math inline">\(\frac{TP}{TP + FN}\)</span></p></li>
<li><p>F1：是P和R的调和平均。<span class="math inline">\(\frac{1}{F1} =
\frac{1}{2}(\frac{1}{P} + \frac{1}{R})\)</span>。范围0 ~
1，越靠近1越好</p></li>
<li><p><span class="math inline">\(F_\beta\)</span>：<span
class="math inline">\(\frac{1}{F_\beta} = \frac{1}{1 +
\beta^2}(\frac{1}{P} + \frac{\beta^2}{R})\)</span>。<span
class="math inline">\(\beta&gt;1\)</span>时，认为较注重R指标</p></li>
<li><p>宏P、宏R、宏F1：就是做p次实验，然后宏P=<span
class="math inline">\(\frac{1}{p}\sum P_i\)</span>，宏R=<span
class="math inline">\(\frac{1}{p}\sum
R_i\)</span>，宏F1是宏P和宏R的调和平均。</p></li>
<li><p>微P、微R、微F1：就是做p次实验，先得到<span
class="math inline">\(\overline{TP}, \overline{FN}, \overline{FP},
\overline{TN}\)</span>，然后算P、R、F1，就是微P微R微F1。</p></li>
<li><p>混淆矩阵：就是上面那个表</p></li>
</ul></li>
<li><p>性能度量2️⃣</p>
<ul>
<li><p>PR曲线：</p>
<p><img src="image-20241206195849226.png" alt="" style="zoom:67%;" /></p>
<ul>
<li>绘制过程：阈值从1降到0，不同阈值会得到不同的混淆矩阵，对于每个混淆矩阵计算P、R。然后标点上去。</li>
<li>评估好坏：如果把别人包住就比别人好，如果面积比别人大就比别人好，如果BEP点(P=R=BEP)比别人大就比别人好</li>
</ul></li>
<li><p>ROC曲线：</p>
<p><img src="image-20241206200121089.png" alt="" style="zoom:67%;" /></p>
<ul>
<li>绘制过程：阈值从1降到0，不同阈值会得到不同的混淆矩阵，对于每个混淆矩阵计算TPR、FRR。然后标点上去。</li>
<li>意义：TPR是正例中预测正确的比率，FPR是负类中预测错误的比率</li>
<li>评估好坏：如果把别人抱住就比别人好，如果面积比别人大就比别人好。</li>
</ul></li>
<li><p>AUC：area under curve，曲线下的面积</p></li>
<li><p>代价敏感错误率：就是原本计算错误率的时候，每个样本的权重是1。但是代价敏感错误率就是每个样本的权重自己去定。</p></li>
</ul></li>
<li><p>偏差和方差</p>
<ul>
<li><p>对于特征<span class="math inline">\(x_1, x_2, \cdots,
x_m\)</span>，上帝视角存在标准映射：<span
class="math inline">\(f(X)\)</span>。</p></li>
<li><p>那么在得到数据集的过程中，由于数据集是采样得到的，假设采样到的特征为<span
class="math inline">\(x_1, x_2, \cdots,
x_m\)</span>，采样得到的标签为<span class="math inline">\(y_1, y_2,
\cdots, y_m\)</span>。因为采样存在噪声，所以有：<span
class="math inline">\(y_i = f(x_i) + \varepsilon_i\)</span>。</p></li>
<li><p>噪声<span
class="math inline">\(\varepsilon\)</span>是个随机变量，假设其<span
class="math inline">\(E(\varepsilon) = 0, D(\varepsilon) =
\sigma^2\)</span></p></li>
<li><p>假设我们训练得到的模型为<span
class="math inline">\(\tilde{f}(X)\)</span>，模型的期望为<span
class="math inline">\(E(\tilde{f}(X))\)</span>。</p></li>
<li><p>那么模型的泛化误差： <span class="math display">\[
\begin{align*}
E(Y - \tilde{f}(X)) &amp;= E(f(X) + \varepsilon - \tilde{f}(X)) \\
&amp;= E(f(X) - E(\tilde{f}(X)) + E(\tilde{f}(X)) - \tilde{f}(X) +
\varepsilon) \\
&amp;= [f(X) - E(\tilde{f}(X))]^2 + E[\tilde{f}(X) - E(\tilde{f}(X))]^2
+ E(\varepsilon^2) \\
&amp;= \text{偏差} + \text{方差} + \sigma^2\text{(误差)}
\end{align*}
\]</span></p></li>
<li><p>可以看出，偏差就是标准标签与模型期望之间的差距；方差就是模型预测与其期望的偏离程度；误差就是一个定值。</p></li>
<li><p>与欠拟合/过拟合之间的关系，当欠拟合时，偏差很大（因为根本拟合不了标准标签），方差小（因为都没有拟合什么数据）；当过拟合时，偏差很小，但是方差变大（因为拟合了过多的噪声）。</p></li>
</ul></li>
<li><p>补充</p>
<ul>
<li>二项分布的共轭分布是 Beta 分布</li>
<li>多项式分布的共轭分布是 Dirichlet 分布</li>
</ul></li>
</ol>
<h3 id="线性模型">3 线性模型</h3>
<ol type="1">
<li><p>线性回归</p>
<ul>
<li><p>线性回归的形式：<span
class="math inline">\(f(\textbf{x})=\textbf{w}^\mathrm{T}\textbf{x} +
b\)</span></p></li>
<li><p>基于MSE均方误差求解出线性回归模型的方法叫做最小二乘法。</p></li>
<li><p>求解目标，最小化<span class="math inline">\(F\)</span>： <span
class="math display">\[
F = \sum_{i=1}^{m}\left(f(x_i) - y_i\right)^2 = \sum_{i=1}^{m}\left(wx_i
+ b - y_i\right)^2
\]</span></p></li>
<li><p>求解方法（线性代数的方法）：</p>
<p><img src="91157626bd33416d4cff2cf0b5fa643.jpg" alt="" style="zoom: 15%;" /></p></li>
<li><p>最小二乘法的等效回归方法是线性均值和正态误差的最大似然回归。</p></li>
</ul></li>
<li><p>逻辑回归（对数几率回归）</p>
<ul>
<li><p>本质 loss 是用的最大似然估计，也可以说交叉熵。</p></li>
<li><p>就是把线性回归的结果，套一层sigmoid函数，使得输出的是一个0 ~
1的值，可用来做二分类问题。</p></li>
<li><p>逻辑回归的形式：<span
class="math inline">\(y={\frac{1}{1+e^{-(w^{\mathrm{T}}x+b)}}}\)</span></p></li>
<li><p>求解目标，最大化F：</p>
<p><img src="ac38e05ba9b7237f1617037b6e8b920.jpg" alt="" style="zoom: 33%;" /></p></li>
<li><p>求解方法：最大似然法</p></li>
</ul></li>
<li><p>线性判别分析（LDA）</p>
<ul>
<li>思想就是弄出一条线<span
class="math inline">\(w^\mathrm{T}x\)</span>，使得同类投影到这条线的距离尽可能近，不同类投影到这条线距离尽可能远。</li>
<li>从数学上，就是最大化这个东西：<span
class="math inline">\(J={\frac{w^{\textsf{T}}\mathrm{S}_{b}w}{w^{\mathrm{T}}\mathrm{S}_{w}w}}\)</span></li>
<li><span class="math inline">\(S_b\)</span>：类间散度矩阵</li>
<li><span class="math inline">\(S_w\)</span>：类内散度矩阵</li>
</ul></li>
<li><p>多分类方法</p>
<ul>
<li><p>OvO：两两配对产生<span
class="math inline">\(\frac{N(N-1)}{2}\)</span>个结果，投票。（需要<span
class="math inline">\(\frac{N(N-1)}{2}\)</span>个分类器）</p></li>
<li><p>OvR：依次将每个类作为正类，其余作为负类。若只有一个分类器输出正类，其余都为负类，则预测结果就为输出的那个正类。若有多个正类输出，选择置信度最大的类别标记。（需要<span
class="math inline">\(N\)</span>个分类器）</p></li>
<li><p>MvM：</p>
<ul>
<li><p>MvM就是在每个分类器中，将若干个类作为正类，若干个类作为负类。MvM中，最常见的一种分类技术叫“纠错输出码"(ECOC)</p></li>
<li><p>具体看下面这幅图，一目了然。</p>
<p><img src="image-20241207002341592.png" /></p></li>
<li><p>上面这幅图中，一共有5个分类器，4个类别。第一个分类器<span
class="math inline">\(f_1\)</span>规定<span
class="math inline">\(C_2\)</span>为正类，<span
class="math inline">\(C_1, C_3,
C_4\)</span>为负类。然后对于测试示例，经过5个分类器，跑出了(-1, -1, +1,
-1, +1)这个预测向量。记此向量为x。那么x与<span
class="math inline">\(C_1\)</span>的向量(-1, +1, -1, +1,
+1)的欧氏距离是<span
class="math inline">\(2\sqrt{3}\)</span>，海明距离(即不同的个数)是3。</p>
<p>通过观察，可以发现预测向量与<span
class="math inline">\(C_3\)</span>的欧氏距离和海明距离均最小，那么就判定该样例属于<span
class="math inline">\(C_3\)</span>。</p>
<p>可以发现，EOOC编码越长(分类器越多)，那么纠错能力越强(鲁棒性越好)。</p>
<p>而且可以发现，两个类别<span class="math inline">\(C_i,
C_j\)</span>的编码距离越远越好，这样子区分度就越高。所以我们称任意俩类别之间编码距离最远的编码方式为理论最优编码。</p></li>
</ul></li>
</ul></li>
<li><p>类别不平衡</p>
<ul>
<li>欠采样：直接丢弃过多类别的样本，使得不同类别的样本数均衡
<ul>
<li>代表算法：EasyEnsemble</li>
</ul></li>
<li>过采样：增加过少类别的样本，使得不同类别的样本数均衡
<ul>
<li>代表算法：SMOTE（通过对训练集过少类别的样本进行插值来生成新样本）</li>
<li>直接对初始过少类别样本进行重复采样会造成严重的过拟合现象</li>
</ul></li>
<li>再缩放：根据 <span class="math inline">\(\frac{m+}{m-}\)</span>
对预测值进行调整</li>
</ul></li>
</ol>
<h3 id="决策树">4 决策树</h3>
<ul>
<li><p>不会受到数据归一化的影响。</p></li>
<li><p>决策树学习的关键在于如何选择最优划分属性，一般而言，随着划分过程不断进行，我们希望决策树的分支节点所包含的样本尽可能属于同一类别。</p></li>
<li><p>经典的属性划分方法有：信息增益ID3、增益率C4.5、基尼指数CART。</p></li>
<li><p>信息量：<span class="math inline">\(I(x) = -\log
P(x)\)</span>。一个事件<span
class="math inline">\(x\)</span>发送的概率越大，那么其蕴含的信息越少，即信息量越小</p></li>
<li><p>信息熵：随机变量的信息量的期望，<span class="math inline">\(H(X)
= E(I(X)) = E(-\log P(X)) = -\sum p(x)\log
p(x)\)</span>。如果随机变量蕴含的值越多，那么就越混乱，其信息熵就越大。</p></li>
<li><p>三种建树方法</p>
<ol type="1">
<li><p>信息增益ID3</p>
<ul>
<li><p>先计算数据集<span class="math inline">\(D\)</span>得到信息熵<span
class="math inline">\(H(D) = -\sum p_k \cdot \log p_k\)</span></p>
<ul>
<li><span
class="math inline">\(p_k\)</span>是第k个label占全部label的比例。这个定义很好理解，如果全部样本都是同一个label，那么信息熵就是0。如果有很多个label，那么信息熵就比较大，说明混乱程序比较高，不确定性程度高。</li>
</ul></li>
<li><p>我们需要找到一个属性<span
class="math inline">\(A\)</span>，使得<span class="math inline">\(G(D,
A) = H(D) - H(D|A)\)</span>最大，即划分后的信息熵尽可能小。</p>
<ul>
<li><span class="math inline">\(G(D, A)\)</span>叫做信息增益</li>
<li><span class="math inline">\(H(D|A)\)</span>是用<span
class="math inline">\(A\)</span>去划分数据集<span
class="math inline">\(D\)</span>之后得到的信息熵：<span
class="math inline">\(H(D | A) = \sum_{i=1}^{C} \frac{cnt_{D_i}}{cnt_D}
\cdot H(D_i)\)</span></li>
</ul></li>
<li><p>划分后，对划分后新得到的数据集对相同的事情。递归结束条件为当前数据集label全一样，或者属性集空了，或者数据集在当前属性的label'都一样。</p></li>
<li><p>伪代码</p>
<p><img src="image-20241207182842913.png" alt="" style="zoom:67%;" /></p></li>
</ul></li>
<li><p>增益率C4.5</p>
<ul>
<li><p>上面的信息增益算法其实我们没考虑到一个东西，就是不同属性天生包含的类别数不同。即属性自身固有的“混乱”程度不一样。</p></li>
<li><p>直接给出数学定义，<span class="math inline">\(G(D,
A)\)</span>是信息增益(Gain)，<span class="math inline">\(Gr(D,
A)\)</span>是增益率(Gain ratio) <span class="math display">\[
\begin{align*}
&amp;G(D, A) = H(D) - H(D | A) \\
&amp;Gr(D, A) = \frac{G(D, A)}{H(A)} \\
&amp;H(A) = -\sum p_k \log p_k
\end{align*}
\]</span></p></li>
<li><p>所以C4.5算法就说：先选出信息增益高于平均信息增益的属性，然后再在这些属性中选出增益率最高的属性作为划分属性！基尼指数CART</p></li>
</ul></li>
<li><p>基尼指数CART</p>
<ul>
<li><p>除了用熵去衡量数据集“的纯度”，还可以用基尼指数去衡量数据集的纯度。</p></li>
<li><p>数据集<span
class="math inline">\(D\)</span>的纯度可用基尼值(Gini)来衡量：</p></li>
</ul>
<p><span class="math display">\[
Gn(D) = \sum_{i=1}^{C}\sum_{j=1}^{C}p_ip_j = 1 - \sum_{k=1}^{C}p_k^2,
\quad i \ne j
\]</span></p>
<ul>
<li><p>可以发现，基于指数反映了从<span
class="math inline">\(D\)</span>中随机抽取两个样本，其类别lable不一致的概率。基尼系数越小，数据集纯度越高，即类别数越少。那么用属性A划分后的基尼指数是多少？
<span class="math display">\[
Gn(D, A) = \sum_{k=1}^{C} \frac{cnt_{D_i}}{cnt_D}Gn(D_i)
\]</span></p></li>
<li><p>CART算法就是利用基尼指数来建决策树的算法，它每次看看用哪个属性划分后的基尼指数最小，就用它划分。</p></li>
<li><p>CART算法直接算的 <span class="math inline">\(Gn(D | A)\)</span>
然后取的最小的那个。总之跟笔记写的不太一样，建议去看真题复习一下。</p></li>
</ul></li>
</ol></li>
<li><p>预剪枝、后剪枝</p>
<ul>
<li>因为决策树是个强方差的算法，所以很容易出现过拟合。为了解决过拟合，就有预剪枝和后剪枝两种方法。</li>
<li>预剪枝：
<ul>
<li>就是在划分节点的时候，拿验证集跑一跑，如果划分后效果反而不如不划分，那么就不继续划分该节点了，直接连个叶子节点上去，类别就是该节点内人数最多的类别。（从上到下）</li>
<li>降低过拟合风险，训练速度快；但是有欠拟合风险</li>
<li>（此时引入了一个东西叫做“验证集”，他与测试集的区别主要是，测试集在训练的全程对于学习器来说是不可见的。而验证集的作用是辅助训练。）</li>
</ul></li>
<li>后剪枝：
<ul>
<li>就是先用算法生成一棵决策树，然后从下到上依次考察是否将节点替换为叶子节点会更优，如果更优，就替换</li>
<li>比预剪枝保留了更多分支，欠拟合风险较小；但是训练时间大</li>
</ul></li>
</ul></li>
<li><p>连续与缺失值处理</p>
<ul>
<li>连续
<ul>
<li>就拿身高这个属性举例，假设样本中，有n个不同的身高，那么划分点就有n个（就是先排序，然后俩俩身高的中点就是划分点），每确定一个划分点t，其实就可以算出按照t去划分身高这个属性之后得到的信息增益<span
class="math inline">\(G(D, A,
t)\)</span>。以用信息增益算法建树举例，那么划分身高这个属性的时候，就是找一个划分点t，使得<span
class="math inline">\(H(D) - G(D, A, t)\)</span>最大即可。</li>
<li>需要注意的是，连续值与离散值有一个地方不同就是，离散值的属性如果用过，那么后面就不会再用来划分了。但是连续值的属性可以再次使用，比如第一次划分是身高是否低于180，进入子节点后可以继续用身高这个属性划分，身高是否低于160。</li>
</ul></li>
<li>缺失值
<ul>
<li>最简单的方法当然就是丢弃有缺失值的样本，或者用众数/平均数去填充缺失值。但是这么做有些暴力。</li>
<li>比较复杂的方法就是划分建树过程中，给每个样本一个全局变量<span
class="math inline">\(w_i\)</span>，为自己的权重，初始为1。对于那些属性<span
class="math inline">\(A\)</span>无缺失的样本呢，直接划分到对应子集中，对应进入的权重为<span
class="math inline">\(w_i\)</span>；对于那些属性<span
class="math inline">\(A\)</span>缺失的样本，就等无缺失的样本都划分完后，然后划分到每一个子集中，对应进入的权重变为<span
class="math inline">\(w_i \cdot
\frac{\sum\text{子集中样本的}w_i}{\sum\text{当前属性无缺失的样本的}w_i}\)</span>。同时将<span
class="math inline">\(\frac{\sum\text{子集中样本的}w_i}{\sum\text{当前属性无缺失的样本的}w_i}\)</span>记录下来作为该子节点的权重。</li>
<li>在验证的时候，就给每一个要验证的样本带一个权重<span
class="math inline">\(w\)</span>，初始值为1。假设走到某个属性A，若该样本在属性A上无缺失，则进入到对应子节点，权重<span
class="math inline">\(w\)</span>不变；若有缺失，则每个子集都进入，但是进入的权重要乘对应子节点的权重。那么最终若进入到多个叶子节点，选择最终权重最大的那个叶子节点的类别判定为该样本的label。</li>
</ul></li>
</ul></li>
<li><p>多变量决策树</p>
<ul>
<li><p>传统的单变量决策树，其决策边界都是与轴平行的：</p>
<p><img src="image-20241207192319312.png" alt="" style="zoom:50%;" /></p></li>
<li><p>但如果我决策树的节点换为多变量的，那么我的决策边界就可以变为线性，更为灵活：</p>
<p><img src="image-20241207192436420.png" alt="" style="zoom:50%;" /></p></li>
</ul></li>
</ul>
<h3 id="神经网络">5 神经网络</h3>
<ol type="1">
<li><p>几个网络拓扑的概念</p>
<ul>
<li><p>单层感知机</p>
<p><img src="image-20241207010044315.png" alt="" style="zoom: 80%;" /></p>
<ul>
<li>只有输入层和输出层，输出层是M-P神经元，也称为阈值逻辑单元，负责加一个偏置项和经过激活函数，得到输出。</li>
<li>激活函数有：sgn阶跃函数，sigmoid函数</li>
<li>（单层感知机是线性模式，但神经网络以及多层感知机不是）</li>
</ul></li>
<li><p>多层感知机(MLP)</p>
<p><img src="image-20241207010250215.png" alt="" style="zoom:80%;" /></p>
<ul>
<li>多层感知机（MLP）是一种前向结构的人工神经网络，包含输入层、输出层及多个隐藏层。除了输入层，隐藏层和输出层的每个神经元都有加一个偏置项和经过激活函数的功能。</li>
</ul></li>
<li><p>多层前馈神经网络</p>
<p><img src="image-20241207011345869.png" alt="" style="zoom: 80%;" /></p>
<ul>
<li>定义：每层神经元与下一层神经元全互联，神经元之间不存在同层连接也不存在跨层连接。</li>
<li>前馈：网络拓扑结构不存在环或回路。</li>
<li>前向传播：从输入层开始，将上一层的输出作为下一层的输入，并计算下一层的输出，一直到运算到输出层为止</li>
</ul></li>
</ul></li>
<li><p>反向传播BP</p>
<ul>
<li><p>西瓜书上的例子和手推过程：</p>
<p><img
src="3a5ef9c69c1f5d4ecc439ec709f3273-1733563383350-7.jpg" /></p></li>
</ul></li>
<li><p>几个其它概念</p>
<ul>
<li><p>解决BP神经网络过拟合手段</p>
<ul>
<li><p>早停：将数据集分为训练集和验证集，当训练集误差降低但验证集误差升高时，停止训练。</p></li>
<li><p>正则化：就是在目标误差函数中加一项用于描述网络复杂度的部分，例如：
<span class="math display">\[
E=\lambda{\frac{1}{m}}\sum_{k=1}^{m}E_{k}+(1-\lambda)\sum_{i}w_{i}^{2}
\]</span></p></li>
<li><p><span
class="math inline">\(E_k\)</span>为第k个训练样例上的误差。这么搞的话网络将会偏好较小的权重<span
class="math inline">\(w_i\)</span>，从而使网络输出更为“光滑”，缓解过拟合现象。</p></li>
<li><p>L2正则化：对绝对值较大的给予较重乘法，且处处可导，方便计算</p></li>
<li><p>L1正则化：对所有权重基于相同力度乘法，因此较小权重乘法后就变为0了，从而达到稀疏化的目的。</p></li>
</ul></li>
<li><p>全局最小与局部极小</p>
<ul>
<li>全局最小：在函数的整个定义域中，如果一个点的函数值是所有可行点中最小的，那么这个点就是一个全局最小点、</li>
<li>局部极小：在函数的定义域内的某个区域中，如果一个点的函数值不大于其邻近点的函数值，那么这个点就是一个局部极小点。</li>
</ul></li>
<li><p>梯度爆炸：梯度由于误差累计变得非常大，导致网络权重大幅更新甚至权重值溢出</p></li>
<li><p>梯度消失（sigmoid /
tanh，因为它们的导数图像都形如正态分布）：梯度非常小甚至趋于0，导致网络训练不佳甚至无法训练。</p></li>
</ul></li>
</ol>
<h3 id="支持向量机">6 支持向量机</h3>
<ol type="1">
<li><p>几个定义</p>
<ul>
<li><p>超平面：n维线性空间中维度为n-1的子空间，它可以把n维线性空间分割为不相交的两部分</p></li>
<li><p>支持向量：距离超平面最近的且满足一定条件的几个训练样本点</p></li>
<li><p>间隔：两个异类支持向量到超平面的距离之和</p></li>
<li><p>SVM原理：m个样本分为两类，每个样本的数据维度为n维，然后我们需要找出一个n-1维的超平面，来区别这两类样本，使得间隔最大。</p></li>
<li><p>SVM求解目标推导：</p>
<p><img src="5a3e5bc7c09017ce20fa869b07f577c.jpg" alt="" style="zoom: 50%;" /></p></li>
</ul></li>
<li><p>核函数</p>
<ul>
<li>我们知道，升维可以使得原本不可分的数据变得可分。但是通过维度转换函数去升维很困难，因为维度转换函数很难找。</li>
<li>而通过前面的分析，原优化问题的对偶问题的最优解仅由支持向量的点积结果决定。</li>
<li>而核函数的功能就是得到转换后空间中向量点积。所以，我们只需找到一个恰当的核函数即可。</li>
<li>核函数定义：将原始空间中的向量作为输入向量，并返回转换后的数据空间中向量的点积的函数称为核函数。</li>
<li>常见的核函数：线性核、多项式核、高斯核、拉普拉斯核、sigmoid核</li>
</ul></li>
<li><p>软间隔</p>
<p><img src="image-20241208000557083.png" alt="" style="zoom: 67%;" /></p>
<ul>
<li>软间隔 SVM
的阈值趋于无穷，则只要最佳分类超平面存在，它就能将所有数据正确分类。</li>
</ul></li>
<li><p>支持向量回归（SVR）</p>
<p><img src="image-20241208001845615.png" alt="" style="zoom:50%;" /></p></li>
</ol>
<h3 id="贝叶斯分类器">7 贝叶斯分类器</h3>
<ol type="1">
<li><p>贝叶斯决策论</p>
<ul>
<li><p>是个理论框架，不是一个实际的模型。</p>
<p><img src="image-20241208165450045.png" alt="" style="zoom: 50%;" /></p></li>
<li><p>其实，就是拿到一个样本<span
class="math inline">\(x\)</span>，然后对于所有类别<span
class="math inline">\(c \in \mathcal{Y}\)</span>，计算出最小的那个<span
class="math inline">\(R(c | x)\)</span>，对应的<span
class="math inline">\(c\)</span>就是<span
class="math inline">\(x\)</span>分到的类别。</p></li>
<li><p>如何计算条件风险<span class="math inline">\(R(c |
x)\)</span>呢？如图公式即可，但是容易发现，<span
class="math inline">\(P(c_j |
x)\)</span>这个概率我们是不知道的。</p></li>
<li><p>我们的机器学习，其实本质上就是在求<span
class="math inline">\(P(c_j | x)\)</span>。</p></li>
<li><p>所以这是个理论框架，它反应了学习性能的理论上限。</p></li>
</ul></li>
<li><p>先验/后验</p>
<ul>
<li>先验概率就是通过历史经验来确定事件。</li>
<li>后验概率就是通过结果来推测原因。</li>
<li>贝叶斯公式：<span class="math inline">\(P(c | x) = \frac{P(c) \cdot
P(x | c)}{P(x)}\)</span></li>
<li>P(c)是先验，P(x|c)是条件概率(或者叫似然)，P(x)叫证据。</li>
</ul></li>
<li><p>生成式/判别式模型</p>
<ul>
<li><p>判别式模型：直接对 P(c|x)建模（SVM、神经网络、决策树）</p></li>
<li><p>生成式模型：对 P(x, c) 建模（贝叶斯分类器）</p></li>
<li><p>从前面的知识可以知道，难点就是在于求条件概率 <span
class="math inline">\(P(x|c)\)</span>
。所以历史上就出现了两派：频率主义学派和贝叶斯学派。频率主义学派就认为
<span class="math inline">\(P(x | c)\)</span>
潜在的是服从某种分布的。所以我们只需要根据现有的数据不断去估计条件概率，从而求出后验。</p></li>
<li><p>频率主义学派/贝叶斯学派 和前面的 判别式生成式模型
没有必然联系。两个概念。机器学习这门课我看到的算法，按照学派分类，应该都是频率主义学派。但是按照什么式来分类，大部分是判别式，小部分是生成式。</p></li>
<li><p>举个例子：</p>
<p><img src="image-20241217001634379.png" alt="" style="zoom: 67%;" /></p>
<p><img src="image-20241217001700091.png" alt="" style="zoom:67%;" /></p></li>
</ul></li>
<li><p>极大似然估计</p>
<ul>
<li>也是一种理论框架，不是实际的模型。没有考虑先验分布。</li>
<li>对于我们不是要求<span
class="math inline">\(P(x|c)\)</span>嘛，即要求<span
class="math inline">\(P(x|c)\)</span>满足什么分布。不妨假设<span
class="math inline">\(P(x|c)\)</span>具有确定的概率分布形式，由<span
class="math inline">\(\theta_c\)</span>唯一确定。所以我们的任务就是利用数据集<span
class="math inline">\(D\)</span>求出<span
class="math inline">\(\theta_C\)</span>。</li>
<li>那么<span
class="math inline">\(D\)</span>中所有类别为c的样本出现的概率：<span
class="math inline">\(\prod_{x \in D_c} P(x | \theta_c)\)</span></li>
<li>我们就是找到一个<span
class="math inline">\(\theta_c\)</span>，使得上面这个概率最大。这就是最大似然。</li>
<li>为了方便，取个log：<span class="math inline">\(f = \sum_{x \in D_c}
\log P(x | \theta_c)\)</span>。</li>
<li>注意，上面的参数<span
class="math inline">\(\theta_c\)</span>是指标针对<span
class="math inline">\(D_c\)</span>的，每类数据集合的参数不一样。</li>
</ul></li>
<li><p>朴素贝叶斯分类器</p>
<ul>
<li><p>基于先验推后验，可解决有监督学习问题。</p></li>
<li><p>由贝叶斯公式，得：<span class="math inline">\(P(c|x) = \frac{P(c)
\cdot P(x | c)}{P(x)}\)</span>。</p></li>
<li><p>不妨假设<span
class="math inline">\(x\)</span>的各个属性相互独立，且若为属性连续则假设<span
class="math inline">\(P(x_i | c)\)</span>满足正态分布。</p></li>
<li><p>那么可得：<span class="math inline">\(P(c|x) =
\frac{P(c)}{P(x)}\prod_{i=1}^d P(x_i | c)\)</span></p></li>
<li><p>对于不同类别<span class="math inline">\(c\)</span>，<span
class="math inline">\(P(x)\)</span>一样，所以只需要计算<span
class="math inline">\(P(c) \cdot \prod_{i=1}^d
P(x_i|c)\)</span>谁大就行，最大的对应的c就是其类别。</p></li>
<li><p>这里一定要用连乘噢！回去看看真题上的例子复习一下，易错。</p></li>
<li><p>下面是一个计算的例子：</p>
<p><img src="image-20241208173142846.png" /></p></li>
</ul></li>
<li><p>朴素贝叶斯分类器的改进</p>
<ul>
<li>因为朴素贝叶斯分类器的假设过于强，所以半朴素贝叶斯就是说对这个假设进行一定程序的放松。经典算法有：SPODE、TAN、AODE。</li>
<li>贝叶斯网：借助DAG来描述属性之间的依赖关系。</li>
<li>EM算法：我们知道，知道了数据概率，可以去估计背后的数据分布；知道了数据分布，可以推测数据概率。所以EM算法中的E就是去估计样本所属类别的概率，M就是用估计的分类来更新分布的参数。循环往复，蛋生鸡鸡生蛋，直到分布收敛。
<ul>
<li>经典的无监督分类模型。</li>
<li>对初始化敏感。</li>
</ul></li>
</ul></li>
</ol>
<h3 id="集成学习">8 集成学习</h3>
<ol type="1">
<li><p>好而不同</p>
<ul>
<li>要获得好的集成，个体学习器应该“好而不同”。个体学习器性能不能太坏（至少50%正确率），且学习器之间要有差异。</li>
<li>但是“准确性”与“多样性”之间存在冲突。准确性增高后就要牺牲掉一些多样性。</li>
</ul></li>
<li><p>两类集成学习方法</p>
<ul>
<li><p>个体学习器存在强依赖关系（序列化方法）</p>
<ul>
<li><p>Boosting框架（AdaBoost）</p>
<p><img src="image-20241208224231596.png" alt="" style="zoom: 67%;" /></p></li>
<li><p>上图就是Boosting框架的思路：对于数据集1里的每个样本有一个权重，初始都一样。然后通过基学习算法得到一个模型1。模型1做错的样本加大其权重；做对的样本减小其权重。然后从数据集1中采样得到数据集2。再做相同的事情，以此类推。直到得到T个模型。最后将这T个弱分类器加权线性组合为强分类器。</p></li>
</ul></li>
<li><p>个体学习器不存在强依赖关系（并行化方法）‘’</p>
<ul>
<li><p>Bagging框架（Random forest）</p>
<p><img src="image-20241208225104052.png" alt="" style="zoom: 50%;" /></p></li>
<li><p>对数据集进行n次自助采样，得到n个新的数据集。对于每个数据集训练用基学习算法训练出一个模型。对于分类任务，通常采用多数投票或平均概率决定最终类别；对于回归任务，采用平均值作为集成模型的预测结果。</p></li>
<li><p>算法伪代码：</p>
<p><img src="image-20241208230513550.png" alt="" style="zoom:67%;" /></p>
<ul>
<li><span
class="math inline">\(\mathcal{D_{bs}}\)</span>是自助采样出来的数据集。</li>
</ul></li>
</ul></li>
<li><p>随机森林</p>
<ul>
<li>如果以决策树算法作为基学习算法的集成学习就叫做随机森林。</li>
<li>随机森林的基学习器之间的差异由两方面带来，第一来自样本扰动（自助采样带来的训练集不一样），第二是属性扰动（决策树中的每个节点划分时只考虑一个属性子集）。</li>
</ul></li>
</ul></li>
<li><p>结合策略</p>
<ul>
<li>平均法、投票法</li>
<li>学习法
<ul>
<li>先从初始数据集中训练出初级学习器，然后“生成”一个新数据集用于训练次级学习器。在新数据集中，经过初级学习器的输出被当做输入特征，初始样本的标记仍被当做样本标记。这样去训练好一个次级学习器。然后对于预测一个样本，先得到各个初级学习器的输出，然后把这个输出丢进次级学习器中，得到预测标签输出。</li>
</ul></li>
</ul></li>
<li><p>多样性增强</p>
<ul>
<li>我们知道集成学习的性能由准确率和多样性决定。所以提升多样性的方法有如下几种：
<ul>
<li>数据样本扰动（例如自助采样）</li>
<li>输入属性扰动（例如随机森林）</li>
<li>输出表示扰动（例如利用ECOC将多分类输出转换为一系列二分类输出）</li>
<li>算法参数扰动（就基学习算法的参数进行设置，产生差异较大的基学习器）</li>
</ul></li>
</ul></li>
</ol>
<h3 id="聚类">9 聚类</h3>
<ol type="1">
<li><p>概念：聚类是一种无监督算法。它将数据集中的样本划分为若干个不相交的子集，称为簇。</p></li>
<li><p>性能度量</p>
<ul>
<li><p>目的：评估聚类的好坏、确定优化的目标</p></li>
<li><p>结论：簇内相似度越高越好，簇间相似度越低越好</p></li>
<li><p>外部指标（与标准聚类模型效果对比）：</p>
<p><img src="image-20241209004435722.png" alt="" style="zoom:67%;" /></p>
<ul>
<li>这仨指标都是[0, 1]，越高越好</li>
</ul></li>
<li><p>内部指标：</p>
<p><img src="image-20241209004608662.png" alt="" style="zoom:67%;" /></p>
<ul>
<li><p>avg(C)是类内的平均距离。diam(C)是类内的最大距离。dmin(C1,
C2)是俩类间的最小距离。dcen(C1, C2)是俩类间的中心之间的距离。</p>
<p><img src="image-20241209004738727.png" alt="" style="zoom:67%;" /></p>
<p><img src="image-20241209004828626.png" alt="" style="zoom: 50%;" /></p></li>
</ul></li>
</ul></li>
<li><p>距离计算</p>
<ul>
<li><p>前面我们用了一些性能指标去度量聚类的好坏，其中用到了“距离“。那么距离也需要一个度量方法，才能够进行计算。</p></li>
<li><p>距离度量满足直递性<img src="image-20241209005125739.png" alt="" style="zoom: 50%;" />，非距离度量不满足（例如相似度）</p></li>
<li><p>当属性为有序的时候，可以用闵可夫斯基距离（下图），无序属性可采用VDM距离。</p>
<p><img src="image-20241209005254974.png" alt="" style="zoom:50%;" /></p></li>
</ul></li>
<li><p>K-means</p>
<ul>
<li>算法描述：
<ol type="1">
<li>随机选取样本作为初始均值向量（初始值：k 的值【即几个簇】）</li>
<li>分别计算每个样本点到均值向量的距离，距离哪个近就属于哪簇</li>
<li>通过2计算出来的划分，重新计算均值向量（直接对簇内点取平均）</li>
<li>重复直到达到停止指标</li>
</ol></li>
<li>仔细看图片，很清晰了</li>
</ul>
<p><img src="image-20241209010527965.png" alt="" style="zoom:80%;" /></p>
<ul>
<li>优点就是简单快速。缺点事先得确定k值且对初始值敏感，对孤立点敏感。</li>
</ul></li>
<li><p>其它一些方法：</p>
<ul>
<li>学习向量量化：是监督学习，知道了每个样本的标签。这个算法返回的是每个簇最终的原型向量。每个类别的原型向量不是简单的均值向量，而是考虑了附近同/异类点的影响。</li>
<li>高斯混合聚类：采用概率模型来表达聚类原型。</li>
<li>密度聚类（DBSCAN）：基于密度的聚类，假设聚类结构能通过样本分布的紧密程序来决定。</li>
<li>层次聚类（AGNES）：试图在不同层次对数据集进行划分，从而形成树形的聚类结构。（自底向上真题上有一道例题，要去复习，易错！）</li>
</ul></li>
</ol>
<h3 id="降维与度量学习">10 降维与度量学习</h3>
<ol type="1">
<li><p>KNN</p>
<ul>
<li>这里的k是指要参考k个与自己最近的点，k-means里的k是簇的个数。</li>
<li>（最小距离分类器算法：通过求出未知类别向量 X
到事先已知的各类别（如A，B，C 等）中心向量的距离 D，然后将待分类的向量 X
归结为这些距离中最小的那一类的分类方法）</li>
<li>（k近邻属于分类算法，样本多且典型性不好容易造成分类错误，样本分布对其影响不大。但是样本分布对聚类算法的影响较大）</li>
<li>它是一个监督学习。需要很多有标签的数据。这样新数据来的时候才能做预测。</li>
<li>算法流程描述：
<ol type="1">
<li>计算测试数据与各个训练数据之间的距离</li>
<li>对距离从小到大进行排序</li>
<li>选取距离最小的k个点</li>
<li>确定前k个点类别出现概率</li>
<li>出现概率最高的类别作为预测分类</li>
</ol></li>
</ul></li>
<li><p>维数灾难：数据属性维数过高，出现数据样本系数、距离因为维数过高从而计算困难的问题，称为维数灾难。解决方法——降维。</p></li>
<li><p>降维方法：</p>
<ul>
<li><p>PCA主成分分析（线性）</p>
<ul>
<li><p>思想：简单来说就是第一阶段找了一个新的坐标系来表示数据，这个新的坐标系能最大限度的看出每个轴上的数据变化大小，第二阶段在新坐标系下取前k个变化最大的轴上的数据，从而实现降维。</p></li>
<li><p>算法伪代码：</p>
<p><img src="image-20241209022227832.png" alt="" style="zoom:80%;" /></p>
<ul>
<li>将数据投影到W坐标系下即可：<span class="math inline">\(X&#39; =
WX\)</span></li>
</ul></li>
</ul></li>
<li><p>核化线性降维：对于线性不可分数据，我们需要先利用核技巧先升维。然后再利用PCA进行降维。</p></li>
<li><p>流形学习：借鉴了拓扑流形概念的降维方法。</p></li>
</ul></li>
<li><p>度量学习</p>
<p><img src="image-20241216215310297.png" /></p></li>
</ol>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2024/10/09/RL%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/" rel="prev" title="RL代码学习框架">
      <i class="fa fa-chevron-left"></i> RL代码学习框架
    </a></div>
      <div class="post-nav-item">
    <a href="/2024/12/19/%E6%9A%82%E5%81%9C%E6%9B%B4%E6%96%B0/" rel="next" title="暂停更新">
      暂停更新 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    
  <div class="comments">
    <div id="lv-container" data-id="city" data-uid="MTAyMC81OTA0Ny8zNTUwOQ=="></div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BB%AA%E8%AE%BA"><span class="nav-text">1 绪论</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9"><span class="nav-text">2 模型评估与选择</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B"><span class="nav-text">3 线性模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91"><span class="nav-text">4 决策树</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-text">5 神经网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA"><span class="nav-text">6 支持向量机</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8"><span class="nav-text">7 贝叶斯分类器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0"><span class="nav-text">8 集成学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%81%9A%E7%B1%BB"><span class="nav-text">9 聚类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0"><span class="nav-text">10 降维与度量学习</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Error_666"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">Error_666</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">39</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/potatoQi" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;potatoQi" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://gitee.com/Error_666" title="Gitee → https:&#x2F;&#x2F;gitee.com&#x2F;Error_666" rel="noopener" target="_blank"><i class="fa fa-paper-plane fa-fw"></i>Gitee</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.acwing.com/user/myspace/index/8252/" title="AcWing → https:&#x2F;&#x2F;www.acwing.com&#x2F;user&#x2F;myspace&#x2F;index&#x2F;8252&#x2F;" rel="noopener" target="_blank"><i class="fas fa-rocket fa-fw"></i>AcWing</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://cn.overleaf.com/project" title="Overleaf → https:&#x2F;&#x2F;cn.overleaf.com&#x2F;project" rel="noopener" target="_blank"><i class="fa fa-edit fa-fw"></i>Overleaf</a>
      </span>
  </div>



  <div class="links-of-recent-posts motion-element">
    <div class="links-of-recent-posts-title">
      <i class="fa fa-history fa-fw"></i>
      最近文章
    </div>
    <ul class="links-of-recent-posts-list">
        <li class="links-of-recent-posts-item">
          <a href="/2024/12/19/%E6%9A%82%E5%81%9C%E6%9B%B4%E6%96%B0/" title="2024&#x2F;12&#x2F;19&#x2F;暂停更新&#x2F;">暂停更新</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2024/12/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/" title="2024&#x2F;12&#x2F;19&#x2F;机器学习自学笔记&#x2F;">机器学习自学笔记</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2024/10/09/RL%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/" title="2024&#x2F;10&#x2F;09&#x2F;RL代码学习框架&#x2F;">RL代码学习框架</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2024/10/07/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A03/" title="2024&#x2F;10&#x2F;07&#x2F;强化学习3&#x2F;">强化学习3</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2024/10/06/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/" title="2024&#x2F;10&#x2F;06&#x2F;强化学习2&#x2F;">强化学习2</a>
        </li>
    </ul>
  </div>
      </div>
	<br>
    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2023-10-1 – 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Error_666</span>
</div>

<!--
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>
 -->


    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <span id="busuanzi_container_site_pv">总访问量<span id="busuanzi_value_site_pv"></span>次</span>
    <span class="post-meta-divider">|</span>
    <span id="busuanzi_container_site_uv">总访客数<span id="busuanzi_value_site_uv"></span>人</span>
    <span class="post-meta-divider">|</span>
<!-- 不蒜子计数初始值纠正 -->
<script>
$(document).ready(function() {

    var int = setInterval(fixCount, 50);  // 50ms周期检测函数
    var countOffset = 20000;  // 初始化首次数据

    function fixCount() {            
       if (document.getElementById("busuanzi_container_site_pv").style.display != "none")
        {
            $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + countOffset); 
            clearInterval(int);
        }                  
        if ($("#busuanzi_container_site_pv").css("display") != "none")
        {
            $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + countOffset); // 加上初始数据 
            clearInterval(int); // 停止检测
        }  
    }
       	
});
</script> 

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  
  <script>
    (function(){
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x=document.getElementsByTagName("link");
		//Find the last canonical URL
		if(x.length > 0){
			for (i=0;i<x.length;i++){
				if(x[i].rel.toLowerCase() == 'canonical' && x[i].href){
					canonicalURL=x[i].href;
				}
			}
		}
    //Get protocol
	    if (!canonicalURL){
	    	curProtocol = window.location.protocol.split(':')[0];
	    }
	    else{
	    	curProtocol = canonicalURL.split(':')[0];
	    }
      //Get current URL if the canonical URL does not exist
	    if (!canonicalURL) canonicalURL = window.location.href;
	    //Assign script content. Replace current URL with the canonical URL
      !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=canonicalURL,t=document.referrer;if(!e.test(r)){var n=(String(curProtocol).toLowerCase() === 'https')?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";t?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var i=new Image;i.src=n}}(window);})();
  </script>




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

<script>
NexT.utils.loadComments(document.querySelector('#lv-container'), () => {
  window.livereOptions = {
    refer: location.pathname.replace(CONFIG.root, '').replace('index.html', '')
  };
  (function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
  })(document, 'script');
});
setInterval(function () {
    var box = document.querySelector(".trc_rbox_container");
    if(box) box.outerHTML = "";
}, 2000);
</script>

</body>
</html>
