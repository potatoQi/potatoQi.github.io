<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta name="google-site-verification" content="Nju-Ov4omLH_XBGFv1RDU5xdZaGOW05OvwIjIu_498Q">
  <meta name="msvalidate.01" content="2CE08595DA3460134EDC20E4D8663AD4">
  <meta name="baidu-site-verification" content="codeva-j0Zy96Rpmy">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"error666.top","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"manual","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.json"};
  </script>

  <meta name="description" content="值函数近似(DQN)、策略梯度方法(REINFORCE)、Actor-Critic方法">
<meta property="og:type" content="article">
<meta property="og:title" content="强化学习3">
<meta property="og:url" content="http://error666.top/2024/10/07/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A03/index.html">
<meta property="og:site_name" content="Error_666">
<meta property="og:description" content="值函数近似(DQN)、策略梯度方法(REINFORCE)、Actor-Critic方法">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://error666.top/2024/10/07/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A03/1.png">
<meta property="og:image" content="http://error666.top/2024/10/07/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A03/2.png">
<meta property="article:published_time" content="2024-10-07T15:30:16.000Z">
<meta property="article:modified_time" content="2024-10-10T16:40:37.837Z">
<meta property="article:author" content="Error_666">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://error666.top/2024/10/07/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A03/1.png">

<link rel="canonical" href="http://error666.top/2024/10/07/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A03/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>强化学习3 | Error_666</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

  <script src="/live2d-widget/autoload.js"></script>
<link rel="alternate" href="/atom.xml" title="Error_666" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Error_666</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-tools">

    <a href="/tools/" rel="section"><i class="fa fa-toolbox fa-fw"></i>工具</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://error666.top/2024/10/07/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A03/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Error_666">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Error_666">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          强化学习3
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-10-07 23:30:16" itemprop="dateCreated datePublished" datetime="2024-10-07T23:30:16+08:00">2024-10-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-10-11 00:40:37" itemprop="dateModified" datetime="2024-10-11T00:40:37+08:00">2024-10-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/1-%E7%A7%91%E7%A0%94/" itemprop="url" rel="index"><span itemprop="name">1. 科研</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/1-%E7%A7%91%E7%A0%94/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">强化学习</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>8.7k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>8 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>值函数近似(DQN)、策略梯度方法(REINFORCE)、Actor-Critic方法</p>
<span id="more"></span>
<h3 id="值函数近似">值函数近似</h3>
<h4 id="引入">引入</h4>
<p>上一章介绍了时序差分方法，也就是model-free下求state/action
value/optimal action value的迭代法。</p>
<p>其思路是在MC Exploring
Starts上演变过来的，即完全的记忆化。既然使用了记忆化，那么就有记忆化数组，即tabular。</p>
<p>这一章的值函数近似不是基于tabular，而是基于函数。这是有实际意义的，比如action
value是连续的时候，离散化后用tabular存就很有可能存不下，所以我们需要一个连续的算法。</p>
<p>从另一个方面考虑，假如state-action
pair太多太多，那么我很难把全部的state-action
pair都估计到，那么假如我们有一个函数，那么无论你状态有多少个，因为我是表达式，所以随便给一个状态我都能代入表达式估计出来。</p>
<p>想到啥了吗？用函数拟合任意一个散点图，是的，神经网络最喜欢干这件事了。</p>
<p>Interesting, right？</p>
<h4 id="目标函数">目标函数</h4>
<p>我们的目标就是通过拟合的方法估计<span
class="math inline">\(v_\pi(s)\)</span>嘛，所以<span
class="math inline">\(v_\pi(s)\)</span>是真实的值，我们拟合的函数是<span
class="math inline">\(\hat{v}(s, w)\)</span>。</p>
<p>注意这是个函数哦，<span
class="math inline">\(s\)</span>是自变量，<span
class="math inline">\(w\)</span>是参数。</p>
<p>显然我们的优化函数为： <span class="math display">\[
J(w) = \mathbb{E}[(v_\pi(S) - \hat{v}(S, w))^2]
\]</span> 显然，我们希望<span
class="math inline">\(J(w)\)</span>尽可能小。</p>
<p>这个优化函数是标准形式，但实际计算的时候我们需要将里面的随机变量和期望替换为样本。</p>
<h4 id="优化算法和函数设计">优化算法和函数设计</h4>
<p>前面我们有了目标函数，那么现在我们就来minimize <span
class="math inline">\(J(w)\)</span>.</p>
<p>来计算下<span class="math inline">\(J(w)\)</span>的导数： <span
class="math display">\[
\begin{align*}
\nabla J(w) &amp;= \nabla \mathbb{E}[(v_\pi(S) - \hat v(S, w))^2] \\
            &amp;= \mathbb{E}[\nabla (v_\pi(S) - \hat v(S, w))^2] \\
            &amp;= 2\mathbb{E}[(v_\pi(S) - \hat v(S, w))(-\nabla_w \hat
v(S, w))] \\
            &amp;= -2\mathbb{E}[(v_\pi(S) - \hat v(S, w))\nabla_w \hat
v(S, w)]
\end{align*}
\]</span> 当然，用SGD即可，那么迭代式为： <span class="math display">\[
w_{t+1} = w_t + \alpha_t(v_\pi(s_t) - \hat v(s_t, w_t))\nabla_w \hat
v(s_t, w_t)
\]</span> 可以看出，上面这个迭代式是没法用的。因为<span
class="math inline">\(v_\pi(s_t)\)</span>我们不知道啊。</p>
<p>所以可以结合MC或者TD algorithm，这里我就结合TD
algorithm其中的迭代式： <span class="math display">\[
v_{t+1}(s_t) = v_t(s_t) - \alpha_t(s_t)[v_t(s_t) - [r_{t+1} + \gamma
v_t(s_{t+1})]]
\]</span> 那么上面的迭代式可以改写为： <span class="math display">\[
w_{t+1} = w_t + \alpha_t(r_{t+1} + \gamma \hat v(s_{t+1}, w_t) - \hat
v(s_t, w_t)) \cdot \nabla_w \hat v(s_t, w_t)
\]</span> 那么我们可以得到TD + 值函数近似算法： <span
class="math display">\[
\begin{align*}
&amp;\textbf{Initialization: } \text{A function $\hat v(s, w)$ that is a
differentiable in $w$. Initial parameter $w_0$.} \\
&amp;\text{For each episode generated following the polticy $\pi$, do}
\\
&amp;\quad\quad \text{For each step $(s_t, r_{t+1}, s_{t+1})$, do} \\
&amp;\quad\quad\quad\quad w_{t+1} = w_t + \alpha_t[r_{t+1} + \gamma \hat
v(s_{t+1}, w_t) - \hat v(s_t, w_t)] \cdot \nabla_w \hat v(s_t, w_t)
\end{align*}
\]</span> 简洁而优美。</p>
<p>好了，就剩一件事了。<span class="math inline">\(\hat v(s,
w)\)</span>这个函数如何设计？</p>
<p>一般现在有两种设计方案，第一种是设计为线性的，第二种是用神经网络去拟合。</p>
<p>先来看第一种：</p>
<p><span class="math inline">\(\hat v(s, w) =
\phi(s)^\mathrm{T}w\)</span>，其中<span
class="math inline">\(\phi\)</span>是特征<span
class="math inline">\(s\)</span>的特征向量。</p>
<p>如何理解呢？就是你对于一个状态的value，看你想用什么特征来描述它，假设你用特征<span
class="math inline">\(a_1, a_2, a_3, 1\)</span>来描述一个状态<span
class="math inline">\(s\)</span>，那么其特征向量就为<span
class="math inline">\([a_1, a_2, a_3,
1]^{\mathrm{T}}\)</span>，那么函数就为：<span class="math inline">\(\hat
v(s,w) = a_1w_1 + a_2w_2 + a_3w_3 + w_4\)</span>。</p>
<p>所以这种方法的关键就是选好特征很关键。举个例子，比如想描述某人某时刻的state，那么特征就可以选择：身高、体重、性别。</p>
<p>再来看第二种：略，神经网络没啥数学推导，这里没必要再展开。</p>
<p>相同的，我们还可以得到Sarsa + 值函数近似算法： <span
class="math display">\[
w_{t+1} = w_t + \alpha_t[r_{t+1} + \gamma \hat q(s_{t+1}, a_{t+1}, w_t)
- \hat q(s_t, a_t, w_t)] \cdot \nabla_w \hat q(s_t, a_t, w_t)
\]</span> 相同的，我们还可以得到Q-learning + 值函数近似算法： <span
class="math display">\[
w_{t+1} = w_t + \alpha_t[r_{t+1} + \gamma \max_{a \in
\mathcal{A(s_{t+1})}} \hat q(s_{t+1}, a, w_t) - \hat q(s_t, a_t, w_t)]
\cdot \nabla_w \hat q(s_t, a_t, w_t)
\]</span></p>
<h4 id="deep-q-learning">Deep Q-learning</h4>
<p>Deep Q-learning，也可以叫deep Q-network，DQN。</p>
<p>此方法将DL那一套搬了过来，而且效果还很好。<del>（这世界的本质难道真的是无限拟合？）</del></p>
<p>Deep Q-learning就是在Q-learning + 值函数近似的基础上，<span
class="math inline">\(\hat q(s_t, a_t,
w_t)\)</span>用神经网络去算的一个算法。</p>
<p>回顾一下一下optimal bellman equation：</p>
<p><span class="math display">\[
q(s,a) = \mathbb{E}\left[ R_{t+1} + \gamma \max_{a \in
\mathcal{A}(S_{t+1})} q(S_{t+1}, a) | S_t = s, A_t = a \right], \forall
s, a
\]</span> 其实Q-learning本质就是在用迭代法使得<span
class="math inline">\(q_t(s_t, a_t) \to r_{t+1} + \gamma \max_{a \in
\mathcal{A}} q_t(s_t,a)\)</span></p>
<p>当用神经网络来拟合action
values时，它与Q-learning一样，本质是在minimize这个函数： <span
class="math display">\[
J(w) = \mathbb{E}\left[ (R + \gamma \max_{a \in \mathcal{A}(S&#39;)}
\hat q(S&#39;, a, w) - \hat q(S, A, w))^2 \right]
\]</span> 但是<span class="math inline">\(J(w)\)</span>这个函数对<span
class="math inline">\(w\)</span>的梯度很难求，因为第二项和第三项都包含了<span
class="math inline">\(w\)</span>。所以这里原文作者用了一个trick。就是设置了两个<span
class="math inline">\(w\)</span>，一个叫<span
class="math inline">\(w\)</span>，一个叫<span
class="math inline">\(w_T\)</span>。<span
class="math inline">\(w\)</span>是持续更新的，<span
class="math inline">\(w_T\)</span>是各种一段时间更新一次的。那么，loss函数可以写为下面这种形式：
<span class="math display">\[
J(w) = \mathbb{E}\left[ (R + \gamma \max_{a \in \mathcal{A}(S&#39;)}
\hat q(S&#39;, a, w_\mathrm{T}) - \hat q(S, A, w))^2 \right]
\]</span> 这样的话，<span
class="math inline">\(w_\mathrm{T}\)</span>就是个常数，那么<span
class="math inline">\(\nabla_w J(w)\)</span>就可以写出来了： <span
class="math display">\[
\nabla_w J(w) = -2 \mathbb{E}\left[ (R + \gamma \max_{a \in
\mathcal{A}(S&#39;)} \hat q(S&#39;, a, w_\mathrm{T}) - \hat q(S, A, w))
\cdot \nabla_w \hat q(S, A, w) \right]
\]</span>
然后既然都用神经网络了，那么全部的思路都转换为深度学习。现在有了目标函数，梯度，就差数据了。</p>
<p>这里的数据就是很多<span class="math inline">\((s, a, r,
s&#39;)\)</span> pairs。</p>
<p>那么可以写出下列算法： <span class="math display">\[
\begin{align*}
&amp;\text{Store the experience samples generated by $\pi_b$ in a replay
buffer $\mathcal{B} = \{(s,a,r,s&#39;)\}$} \\
&amp;\quad\quad \text{For each iteration, do} \\
&amp;\quad\quad\quad\quad \text{Uniformly draw a mini-batch of samples
from $\mathcal{B}$} \\
&amp;\quad\quad\quad\quad \text{For each sample $(s,a,r,s&#39;)$,
calculate the target values as $y_{\mathrm{T}} = r + \gamma \max_{a \in
\mathcal{A}(s&#39;)}\hat q(s&#39;,a,w_{\mathrm{T}})$, where} \\
&amp;\quad\quad\quad\quad \text{$w_\mathrm{T}$ is the parameter of the
target network} \\
&amp;\quad\quad\quad\quad \text{Update the main network to minimize
$(y_\mathrm{T} - \hat q(s,a,w))^2$ using the mini-batch
$\{(s,a,y_\mathrm{T})\}$} \\
&amp;\quad\quad \text{Set $w_\mathrm{T} = w$ every $C$ iterations}
\end{align*}
\]</span> 思考累了？换种角度重新看看DQN，会发现很简单。</p>
<p>首先，它有很多样本，每个样本会得到一个输出<span
class="math inline">\(y_\mathrm{T}\)</span>（通过optimal bellman
equation得到的输出），我们的目的，就是让我们的网络<span
class="math inline">\(\hat q(s,a,w)\)</span>（其中<span
class="math inline">\(s,a\)</span>是输入，<span
class="math inline">\(w\)</span>是模型参数）尽可能拟合所以样本的<span
class="math inline">\(y_\mathrm{T}\)</span>。
所以就是个简单的深度学习问题。</p>
<h3 id="策略梯度方法">策略梯度方法</h3>
<h4 id="引入-1">引入</h4>
<p>其实就是用连续函数去直接拟合policy，而非像以前那样关注中间量state
values、action values。</p>
<p>具体来说，即通常用神经网络去拟合一个函数<span
class="math inline">\(\pi(a | s, \theta)\)</span>，其中<span
class="math inline">\(\theta\)</span>是网络参数。</p>
<p>那如何评价我们拟合的这个<span class="math inline">\(\pi(a | s,
\theta)\)</span>是否好坏呢？</p>
<p>所以我们需要一个指标<span
class="math inline">\(J(\theta)\)</span>，我们的任务，就是通过大量经验(样本)，去训练拟合这些样本，从而改变<span
class="math inline">\(\theta\)</span>，去maximize这个指标。</p>
<p>Interesting，越来越像深度学习的感觉了。</p>
<p>回顾一下，RL从MDP开始，MDP就是建立在标准的数学动态规划、矩阵论、概率论上的数学框架。解决RL问题就是在解决这个数学框架。求解方法有迭代法或者直接解方程。</p>
<p>后面因为概率很难提前获得，也就是我们通常不能开“上帝视角”，所以解数学框架的时候会缺失一些信息。因此我们通过大量采样来近似模拟这些信息，进行解题。这就诞生MC系列算法、时序差分系列算法。</p>
<p>再后来，我们的视角逐渐不再放在最底层的MDP框架公式中，而是在借助MDP框架的关键定义和公式上，试图直接利用数学，拟合出最佳的state
values / action values / optimal policy。这就是Deep
Q-learning、REINFORCE、Actor-Critic。</p>
<p>未来，至少在短期内可以预见的是，RL将于DL深度结合，且大量的DL技巧将会被运用到RL中来。</p>
<p>未来RL会走出一条什么样的路，我们不知道。但是，万一哪天RL突破了“RL”的定义和边界，我想，会是件令人激动的事。</p>
<h4 id="目标函数-1">目标函数</h4>
<p>也就是引入中说到的<span
class="math inline">\(J(\theta)\)</span>是啥？</p>
<p>有两大类metrics，第一类是average state value，就是state
values的加权平均。记作<span
class="math inline">\(\bar{v}_\pi\)</span></p>
<p>第二类是average reward，就是<span
class="math inline">\(r_\pi(s)\)</span>的加权平均。记作<span
class="math inline">\(\bar{r}_\pi\)</span></p>
<p>似乎这两个metrics，前一个更加远视，后一个更加近视（因为考虑的是immediate
expected reward），但其实，对这两个指标做优化是等价的，因为在<span
class="math inline">\(\gamma &lt; 1\)</span>的时候，满足：<span
class="math inline">\(\bar{r}_\pi = (1 - \gamma)\bar{v}_\pi\)</span></p>
<p>average state value, <span
class="math inline">\(\bar{v}_\pi\)</span>，可写为： <span
class="math display">\[
\bar{v}_\pi = \sum_{s}d_\pi(s)v_\pi(s) = \mathbb{E}\left[
\sum_{t=0}^{\infty}\gamma^t R_{t+1} \right]
\]</span> 第一个定义就是本身的定义，<span
class="math inline">\(d_\pi(s)\)</span>是不同state的权重。</p>
<p>第二个定义不太直观，我来推导一下： <span class="math display">\[
\mathbb{E}\left[\sum_{t=0}^{\infty}\gamma^{t}R_{t+1}\right]
=\sum_{s\in\mathcal{S}}d(s)\mathbb{E}\left[\sum_{t=0}^{\infty}\gamma^{t}R_{t+1}|S_{0}=s\right]=\sum_{s\in\mathcal{S}}d(s)v_{\pi}(s)
\]</span> average reward, <span
class="math inline">\(\bar{r}_\pi\)</span>，可写为： <span
class="math display">\[
\bar{r}_\pi = \sum_{s}d_\pi(s)r_\pi(s) = \lim_{n \to
\infty}\frac{1}{n}\mathbb{E}(\sum_{t=1}^{n}R_t)
\]</span> 第一个定义就是本身的定义，第二个定义很好理解，<span
class="math inline">\(\bar{r}_\pi\)</span>表达的就是所有immediate
expected reward的期望，那么你把所有所有的<span
class="math inline">\(R_t\)</span>求平均就是<span
class="math inline">\(\bar{r}_\pi\)</span>了。</p>
<h4 id="梯度计算">梯度计算</h4>
<p>直接给出结论，详细证明去书里看： <span class="math display">\[
\nabla_\theta\bar{r}_\pi\simeq\sum_sd_\pi(s)\sum_a\nabla_\theta\pi(a|s,\theta)q_\pi(s,a),\\\nabla_\theta\bar{v}_\pi=\frac1{1-\gamma}\nabla_\theta\bar{r}_\pi
\]</span> 当<span class="math inline">\(\gamma &lt; 1\)</span>时<span
class="math inline">\(\nabla_\theta
\bar{r}_\pi\)</span>是约等于右边那一坨，当<span
class="math inline">\(\gamma = 1\)</span>时是严格等于。<span
class="math inline">\(\gamma &lt; 1\)</span>时第二个式子成立。</p>
<p>但是上面那个式子还可以进行化简：</p>
<p>不妨对<span class="math inline">\(\ln \pi(a|s,
\theta)\)</span>求导，<span class="math inline">\(\nabla_\theta \ln
\pi(a|s, \theta) = \frac{1}{\pi(a|s, \theta)} \cdot \nabla_\theta
\pi(a|s, \theta)\)</span></p>
<p><span class="math inline">\(\therefore \nabla_\theta \pi(a|s, \theta)
= \pi(a|s, \theta)\nabla_\theta\ln\pi(a|s, \theta)\)</span></p>
<p>带回上面的式子，得：<span class="math inline">\(\nabla_\theta
\bar{r}_\pi = \sum_{s}d_\pi(s)\sum_{a}\pi(a|s,
\theta)\nabla_\theta\ln\pi(a|s, \theta)q_\pi(s, a)\)</span></p>
<p>那么就可以把<span
class="math inline">\(\sum\)</span>写为期望的方式：<span
class="math inline">\(\nabla_\pi \bar{r}_\pi = \mathbb{E}_{\mathcal{S}
\sim d, \mathcal{A} \sim \pi}\left[ \nabla_\theta\ln\pi(A|S, \theta)
\cdot q_\pi(S, A) \right]\)</span></p>
<p>这样有什么好处呢？相当于我们只需要有state action
pairs的样本，就可以去拟合<span
class="math inline">\(\nabla_\pi\bar{r}_\pi\)</span>了，相比于前面求和的形式，训练简直不要简单太多。很牛的idea。</p>
<p>但是既然你取了<span
class="math inline">\(\ln\)</span>，那么就要保证<span
class="math inline">\(\pi(a|s, \theta) &gt;
0\)</span>，所以对于神经网络的话，在最后一层就要做一个softmax：</p>
<p><img src="1.png" style="zoom:67%;" /></p>
<p>就行了，但是这样搞的话，policy就具有探索性了需要注意。</p>
<p>所以我们的为了更新matrics的梯度上升就可以这么写，用SGD： <span
class="math display">\[
\theta_{t+1} = \theta_t + \alpha\nabla_\theta\ln\pi(a_t|s_t,
\theta_t)q_\pi(s_t,a_t)
\]</span> 但上面这个式子目前还用不了，因为<span
class="math inline">\(q_\pi(s_t,
a_t)\)</span>我们不知道。所以用MC系列或者TD系列呗，如果你用MC去拟合<span
class="math inline">\(q_\pi(s_t,
a_t)\)</span>，那么你就得到了REINFORCE算法，表示如下： <span
class="math display">\[
\begin{align*}
&amp;\textbf{Initialization: }\text{A parameterized function $\pi(a|s,
\theta)$} \\
&amp;\text{At time $k$, do} \\
&amp;\quad\quad \text{Select $s_0$ and generate an episode following
$\pi(\theta_k)$. Suppose the episode is $\{s_0, a_0, r_1, \cdots,
s_{T-1}, a_{T_1}, r_T\}$.} \\
&amp;\quad\quad \text{For $t = 0,1,\cdots, T-1$, do} \\
&amp;\quad\quad\quad\quad q_t(s_t, a_t) = \sum_{k=t+1}^{T}\gamma^{k - t
- 1}r_k \\
&amp;\quad\quad\quad\quad \theta_{t+1} = \theta_t +
\alpha\nabla_\theta\ln\pi(a_t|s_t, \theta_t)q_t(s_t, a_t)
\end{align*}
\]</span></p>
<h3 id="actor-critic">Actor-Critic</h3>
<h4 id="qac">QAC</h4>
<p>我们仍然是直接估计最优策略<span class="math inline">\(\pi(a | s,
\theta)\)</span>，然后前面已经推导出了<span
class="math inline">\(\theta\)</span>的更新式： <span
class="math display">\[
\theta_{t+1} = \theta_t + \alpha\nabla_\theta\ln\pi(a_t|s_t,
\theta_t)q_\pi(s_t,a_t)
\]</span> 关键这个<span class="math inline">\(q_\pi(s_t,
a_t)\)</span>我们不知道，所以用MC方法去估计得到的方法就叫REINFORCE。</p>
<p>但是其实可以通过神经网络的方法去估计它：<span
class="math inline">\(q(s_t,a_t,w_t)\)</span>，更新方式如下（为什么更新方式是这样，去看“值函数近似-优化算法和函数设计”部分）：
<span class="math display">\[
w_{t+1} = w_t + \alpha_w \left[ r_{t+1} + \gamma q(s_{t+1}, a_{t+1},
w_t) - q(s_t, a_t, w_t) \right] \nabla_w q(s_t, a_t, w_t)
\]</span> 所以我直接给出算法流程：</p>
<p><img src="2.png" style="zoom:67%;" /></p>
<p>但是直到现在为止，我还是没解释Actor-Critic这名字啥意思。其实我觉得这名字没啥意思。只需要记住直接估计最优policy的系列方法都是AC系列算法（除了REINFORCE开除AC学籍）</p>
<p>上面的算法叫QAC，首先是它属于AC系列算法，然后它的<span
class="math inline">\(q(s,
a)\)</span>是通过神经网络算的，所以叫QAC。</p>
<h4 id="a2c">A2C</h4>
<p>Advantage actor-critic, A2C，因为全称有俩A，所以叫A2。</p>
<p>它是QAC的一个改进版本，在更新<span
class="math inline">\(\theta\)</span>的那一步进行了优化，具体来说，通过添加偏置项，减小了梯度的方差，但是期望不变。</p>
<p>这么做的好处，就是在通过SGD优化目标函数时，因为梯度期望不变，所以优化结果不会改变。但是梯度方差减小，所以采样带来的误差会减小。</p>
<h4 id="重要性采样">重要性采样</h4>
<p>略</p>
<h4 id="dpg">DPG</h4>
<p>前面的三个AC系列算法，在<span class="math inline">\(\pi(a|s,
\theta)\)</span>这个神经网络中，最后一层都是加了softmax的，所以无论如何都是具有探索性的。</p>
<p>所以如何让其变为一个greedy的算法呢，Deterministic Policy Gradient,
DPG，就是greedy的直接估计最优policy的算法。</p>
<p>略</p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2024/10/06/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/" rel="prev" title="强化学习2">
      <i class="fa fa-chevron-left"></i> 强化学习2
    </a></div>
      <div class="post-nav-item">
    <a href="/2024/10/09/RL%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/" rel="next" title="RL代码学习框架">
      RL代码学习框架 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    
  <div class="comments">
    <div id="lv-container" data-id="city" data-uid="MTAyMC81OTA0Ny8zNTUwOQ=="></div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%80%BC%E5%87%BD%E6%95%B0%E8%BF%91%E4%BC%BC"><span class="nav-text">值函数近似</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BC%95%E5%85%A5"><span class="nav-text">引入</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0"><span class="nav-text">目标函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E5%92%8C%E5%87%BD%E6%95%B0%E8%AE%BE%E8%AE%A1"><span class="nav-text">优化算法和函数设计</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#deep-q-learning"><span class="nav-text">Deep Q-learning</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E6%96%B9%E6%B3%95"><span class="nav-text">策略梯度方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BC%95%E5%85%A5-1"><span class="nav-text">引入</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0-1"><span class="nav-text">目标函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%97"><span class="nav-text">梯度计算</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#actor-critic"><span class="nav-text">Actor-Critic</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#qac"><span class="nav-text">QAC</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#a2c"><span class="nav-text">A2C</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%87%8D%E8%A6%81%E6%80%A7%E9%87%87%E6%A0%B7"><span class="nav-text">重要性采样</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#dpg"><span class="nav-text">DPG</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Error_666"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">Error_666</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">41</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/potatoQi" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;potatoQi" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://gitee.com/Error_666" title="Gitee → https:&#x2F;&#x2F;gitee.com&#x2F;Error_666" rel="noopener" target="_blank"><i class="fa fa-paper-plane fa-fw"></i>Gitee</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.acwing.com/user/myspace/index/8252/" title="AcWing → https:&#x2F;&#x2F;www.acwing.com&#x2F;user&#x2F;myspace&#x2F;index&#x2F;8252&#x2F;" rel="noopener" target="_blank"><i class="fas fa-rocket fa-fw"></i>AcWing</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://cn.overleaf.com/project" title="Overleaf → https:&#x2F;&#x2F;cn.overleaf.com&#x2F;project" rel="noopener" target="_blank"><i class="fa fa-edit fa-fw"></i>Overleaf</a>
      </span>
  </div>



  <div class="links-of-recent-posts motion-element">
    <div class="links-of-recent-posts-title">
      <i class="fa fa-history fa-fw"></i>
      最近文章
    </div>
    <ul class="links-of-recent-posts-list">
        <li class="links-of-recent-posts-item">
          <a href="/2024/12/19/%E6%9A%82%E5%81%9C%E6%9B%B4%E6%96%B0/" title="2024&#x2F;12&#x2F;19&#x2F;暂停更新&#x2F;">暂停更新</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2024/12/19/%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/" title="2024&#x2F;12&#x2F;19&#x2F;软件工程自学笔记&#x2F;">软件工程自学笔记</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2024/12/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/" title="2024&#x2F;12&#x2F;19&#x2F;机器学习自学笔记&#x2F;">机器学习自学笔记</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2024/10/09/RL%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/" title="2024&#x2F;10&#x2F;09&#x2F;RL代码学习框架&#x2F;">RL代码学习框架</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2024/10/07/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A03/" title="2024&#x2F;10&#x2F;07&#x2F;强化学习3&#x2F;">强化学习3</a>
        </li>
    </ul>
  </div>
      </div>
	<br>
    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2023-10-1 – 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Error_666</span>
</div>

<!--
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>
 -->


    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <span id="busuanzi_container_site_pv">总访问量<span id="busuanzi_value_site_pv"></span>次</span>
    <span class="post-meta-divider">|</span>
    <span id="busuanzi_container_site_uv">总访客数<span id="busuanzi_value_site_uv"></span>人</span>
    <span class="post-meta-divider">|</span>
<!-- 不蒜子计数初始值纠正 -->
<script>
$(document).ready(function() {

    var int = setInterval(fixCount, 50);  // 50ms周期检测函数
    var countOffset = 20000;  // 初始化首次数据

    function fixCount() {            
       if (document.getElementById("busuanzi_container_site_pv").style.display != "none")
        {
            $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + countOffset); 
            clearInterval(int);
        }                  
        if ($("#busuanzi_container_site_pv").css("display") != "none")
        {
            $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + countOffset); // 加上初始数据 
            clearInterval(int); // 停止检测
        }  
    }
       	
});
</script> 

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  
  <script>
    (function(){
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x=document.getElementsByTagName("link");
		//Find the last canonical URL
		if(x.length > 0){
			for (i=0;i<x.length;i++){
				if(x[i].rel.toLowerCase() == 'canonical' && x[i].href){
					canonicalURL=x[i].href;
				}
			}
		}
    //Get protocol
	    if (!canonicalURL){
	    	curProtocol = window.location.protocol.split(':')[0];
	    }
	    else{
	    	curProtocol = canonicalURL.split(':')[0];
	    }
      //Get current URL if the canonical URL does not exist
	    if (!canonicalURL) canonicalURL = window.location.href;
	    //Assign script content. Replace current URL with the canonical URL
      !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=canonicalURL,t=document.referrer;if(!e.test(r)){var n=(String(curProtocol).toLowerCase() === 'https')?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";t?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var i=new Image;i.src=n}}(window);})();
  </script>




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

<script>
NexT.utils.loadComments(document.querySelector('#lv-container'), () => {
  window.livereOptions = {
    refer: location.pathname.replace(CONFIG.root, '').replace('index.html', '')
  };
  (function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
  })(document, 'script');
});
setInterval(function () {
    var box = document.querySelector(".trc_rbox_container");
    if(box) box.outerHTML = "";
}, 2000);
</script>

</body>
</html>
