<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta name="google-site-verification" content="Nju-Ov4omLH_XBGFv1RDU5xdZaGOW05OvwIjIu_498Q">
  <meta name="msvalidate.01" content="2CE08595DA3460134EDC20E4D8663AD4">
  <meta name="baidu-site-verification" content="codeva-j0Zy96Rpmy">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"potatoqi.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"manual","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.json"};
  </script>

  <meta name="description" content="值&#x2F;策略迭代算法、蒙特卡洛、随机近似理论、时序差分方法">
<meta property="og:type" content="article">
<meta property="og:title" content="强化学习2">
<meta property="og:url" content="http://potatoqi.github.io/2024/10/06/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/index.html">
<meta property="og:site_name" content="Error_666">
<meta property="og:description" content="值&#x2F;策略迭代算法、蒙特卡洛、随机近似理论、时序差分方法">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://potatoqi.github.io/2024/10/06/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/1.png">
<meta property="og:image" content="http://potatoqi.github.io/2024/10/06/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/2.png">
<meta property="article:published_time" content="2024-10-05T18:11:53.000Z">
<meta property="article:modified_time" content="2024-10-10T16:10:36.626Z">
<meta property="article:author" content="Error_666">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://potatoqi.github.io/2024/10/06/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/1.png">

<link rel="canonical" href="http://potatoqi.github.io/2024/10/06/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>强化学习2 | Error_666</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

  <script src="/live2d-widget/autoload.js"></script>
<link rel="alternate" href="/atom.xml" title="Error_666" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Error_666</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-tools">

    <a href="/tools/" rel="section"><i class="fa fa-toolbox fa-fw"></i>工具</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://potatoqi.github.io/2024/10/06/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Error_666">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Error_666">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          强化学习2
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-10-06 02:11:53" itemprop="dateCreated datePublished" datetime="2024-10-06T02:11:53+08:00">2024-10-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-10-11 00:10:36" itemprop="dateModified" datetime="2024-10-11T00:10:36+08:00">2024-10-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/1-%E7%A7%91%E7%A0%94/" itemprop="url" rel="index"><span itemprop="name">1. 科研</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/1-%E7%A7%91%E7%A0%94/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">强化学习</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>18k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>16 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>值/策略迭代算法、蒙特卡洛、随机近似理论、时序差分方法</p>
<span id="more"></span>
<h3 id="一.-值策略迭代算法">一. 值/策略迭代算法</h3>
<h4 id="值迭代算法">值迭代算法</h4>
<p>其实在BOE那一章的结尾我已经给出了值迭代算法的流程了：</p>
<ol type="1">
<li>设定好<span class="math inline">\(\gamma\)</span>，<span
class="math inline">\(r\)</span>，<span
class="math inline">\(p(r|s,a)\)</span>，<span
class="math inline">\(p(s&#39;|s,a)\)</span></li>
<li>随意取一个<span class="math inline">\(v_0\)</span>，然后通过<span
class="math inline">\(q_\pi(s, a) = \sum_{r}p(r|s, a)r + \gamma
\sum_{s&#39;}p(s&#39; | s, a)v_\pi(s&#39;)\)</span>算出对应的<span
class="math inline">\(q_0\)</span></li>
<li>For each state <span class="math inline">\(s_i\)</span>, at time
<span class="math inline">\(k\)</span>：
<ul>
<li>算出<span class="math inline">\(q_{k}(s_i, a)\)</span></li>
<li>Find the <span class="math inline">\(a_k^*(s_i)\)</span>, s.t, <span
class="math inline">\(q_k(s_i, a_k^*(s_i))\)</span>最大</li>
<li><span class="math inline">\(\pi_{k+1}(a|s_i)=\begin{cases} 1 \quad
a=a_k^*(s_i) \\ 0 \quad a \ne a_k^*(s_i) \end{cases}\)</span></li>
<li><span class="math inline">\(v_{k+1}(s_i) =
\sum_{a}\pi_{k+1}(a|s)q_k(s,a)\)</span></li>
</ul></li>
</ol>
<p>现在，用正式的语言描述这个algorithm： <span class="math display">\[
\begin{align*}
&amp;\textbf{Initialization: }\text{The probability model $p(r|s,a)$ and
$p(s&#39;|s,a)$ for all $(s,a)$ are known. Initial guess $v_0$.} \\
&amp;\text{At time $k$, do} \\
&amp;\quad\quad \text{For every state $s \in \mathcal{S}$, do} \\
&amp;\quad\quad\quad\quad \text{q-value: $q_k(s,a)=\sum_{r}p(r|s,a)r +
\gamma \sum_{s&#39;}p(s&#39;|s,a)v_k(s&#39;)$} \\
&amp;\quad\quad \text{Maximum action value: $a_k^*(s) =
\text{argmax}_{a}q_k(a,s)$} \\
&amp;\quad\quad \text{Policy update: $\pi_{k+1}(a|s)=1$ if $a=a_k^*(s)$,
and $\pi_{k+1}(a|s)=0$ otherwise} \\
&amp;\quad\quad \text{Value update: $v_{k+1}=q_k(a_k^*(s), s)$}
\end{align*}
\]</span></p>
<h4 id="策略迭代算法">策略迭代算法</h4>
<p>思想就是首先先初始化一个策略，然后先得到该策略下的state
value（即Policy evaluation, PE），然后得到state
value后就可以算出对应的action value，然后选择action
value最大的action，即优化当前policy（Policy
improvement），得到新的policy。依次类推下去，最终即可得到<span
class="math inline">\(\pi^*, v^*\)</span>。</p>
<p>用正式的语言描述这个algorithm： <span class="math display">\[
\begin{align*}
&amp;\textbf{Initialization: }\text{The probability model $p(r|s,a)$ and
$p(s&#39;|s,a)$ for all $(s,a)$ are known. Initial guess $\pi_0$.} \\
&amp;\text{At time $k$, do} \\
&amp;\quad\quad \text{Policy evaluation:} \\
&amp;\quad\quad \text{Initialization: an arbitrary initial guess
$v_{\pi_k}^{(0)}$} \\
&amp;\quad\quad \text{While $v_{\pi_k}^{(j)}$ has not converged, for the
$j$th iteration, do} \\
&amp;\quad\quad\quad\quad \text{For every state $s \in \mathcal{S}$, do}
\\
&amp;\quad\quad\quad\quad\quad\quad v_{\pi_k}^{(j+1)}(s) =
\sum_{a}\pi_k(a|s)\left[ \sum_{r}p(r|s,a)r +
\gamma\sum_{s&#39;}p(s&#39;|s,a)v_{\pi_k}^{(j)}(s&#39;) \right] \\
&amp;\quad\quad \text{Policy improvement:} \\
&amp;\quad\quad \text{For every state $s \in \mathcal{S}$, do} \\
&amp;\quad\quad\quad\quad q_{\pi_k}(s,a) = \sum_{r}p(r|s,a)r +
\gamma\sum_{s&#39;}p(s&#39;|s,a)v_{\pi_k}(s&#39;) \\
&amp;\quad\quad a_k^*(s) = \text{argmax}_{a}q_{\pi_k}(s,a) \\
&amp;\quad\quad \text{$\pi_{k+1}(a|s) = 1$ if $a = a_k^*(s)$, and
$\pi_{k+1}(a|s)=0$ otherwise}
\end{align*}
\]</span></p>
<h4 id="两者比较">两者比较</h4>
<p>值迭代算法是从一个初始state value开始，有了state
value，就可以算出action value，进而得出当前最优策略，然后去更新state
value，依次类推。</p>
<p>策略迭代算法是从一个初始policy开始，然后通过迭代算法求出当前policy下的最优state
value，然后再通过state value得到action
value，进而更新当前最优策略。依次类推。</p>
<p>可以发现，不同点就在于，同样是得到一个policy，值迭代是立马用其代入bellman-equation算出迭代一次后的state
value。而策略迭代是代入bellman-equation迭代很多次算出的state
value。所以直观上来说，策略迭代的收敛次数会更少，但是单次计算量会更大。</p>
<p><img src="1.png" style="zoom:50%;" /></p>
<h3 id="二.-蒙特卡洛">二. 蒙特卡洛</h3>
<h4 id="引入">引入</h4>
<p>前面的值/策略迭代算法都是model-based
RL，蒙特卡洛是我们接触到的第一个model-free的方法。</p>
<p>model不知道的时候怎么办呢？蒙特卡洛其实就是大量采样，用样本的分布来估计model的分布。</p>
<p>蒙特卡洛，Monte Carlo，MC。</p>
<h4 id="mc-basic">MC Basic</h4>
<p>MC
Basic算法其实就跟policy迭代算法一样，只不过把policy迭代算法里的model-based部分，即计算<span
class="math inline">\(v_{\pi_k},
q_{\pi_k}\)</span>的部分，换成了依靠采样直接算出基于一个策略<span
class="math inline">\(\pi_{k}\)</span>的<span
class="math inline">\(q_{\pi_k}\)</span>。第二步policy
improvement就一样了。</p>
<p>原本policy迭代算法里求<span
class="math inline">\(q_{\pi_k}\)</span>是依赖于这个公式：<span
class="math inline">\(q_{\pi_k}(s,a)=\sum_{r}p(r|s,a)r +
\gamma\sum_{s&#39;}p(s&#39;|s,a)v_{\pi_k}(s&#39;)\)</span></p>
<p>但是MC Basic算<span
class="math inline">\(q_{\pi_k}\)</span>是依赖于它的原始定义：<span
class="math inline">\(q_{\pi_k}(s,a)=\mathbb{E}(G_t | S_t = s, A_t =
a)\)</span></p>
<p>即对于state-action pairs, 通过大量采样估计出所有的<span
class="math inline">\(q_{\pi_k}(s,a)\)</span>，然后再进行policy
improvement。</p>
<p>用数学语言来描述如下： <span class="math display">\[
\begin{align*}
&amp;\textbf{Initialization: }\text{Initial guess $\pi_0$.} \\
&amp;\text{At time $k$, do} \\
&amp;\quad\quad \text{For every state $s \in \mathcal{S}$, do} \\
&amp;\quad\quad\quad\quad \text{For every action $a \in \mathcal{A}(s)$,
do} \\
&amp;\quad\quad\quad\quad\quad\quad \text{Collect sufficiently many
episodes starting from $(s,a)$ following $\pi_k$} \\
&amp;\quad\quad\quad\quad\quad\quad \text{$q_{\pi_k}(s,a)=$ average
return of all the episodes starting from $(s,a)$} \\
&amp;\quad\quad\quad\quad \text{Policy improvement step:} \\
&amp;\quad\quad\quad\quad a_k^*(s) = \text{argmax}_{a}q_{\pi_k}(s,a) \\
&amp;\quad\quad\quad\quad \text{$\pi_{k+1}(a|s)=1$ if $a=a_k^*()s$, and
$\pi_{k+1}(a|s)=0$ otherwise}
\end{align*}
\]</span> 很简单，right？</p>
<p>为啥这里要用episode这个词而非trajectory这个词呢？因为trajectory可能是无限的，而采样是离散的，所以通常我们设置一个采样长度上限，那么每采样一条trajectory其实就是有限的，也叫一条episode。</p>
<h4 id="mc-exploring-starts">MC Exploring Starts</h4>
<p>MC Exploring Starts其实就是对MC Basic算法的一个时间复杂度优化。</p>
<p>MC Basic是对每一个<span class="math inline">\((s,a)\)</span>
pair都采样很多episode来估计其<span class="math inline">\(q_{\pi_k}(s,
a)\)</span>，采样的途中可能会路过很多其余的<span
class="math inline">\((s&#39;,a&#39;)\)</span>
pair，其实采样出来的return也可以用来估计它们的action
value。下面这个图就可以很好的解释了MC Basic的数据浪费：</p>
<p><img src="2.png" style="zoom: 67%;" /></p>
<p>看第一条episode，在MC
Basic算法里那么一长条episode，我们只用它来估计了<span
class="math inline">\((s_1, a_2)\)</span>的action
value。但其实，还可以用来估计<span class="math inline">\((s_2, a_4),
\cdots\)</span>的action value，它们的return之间只差了一个<span
class="math inline">\(\gamma\)</span>和reward。</p>
<p>所以MC Exploring
Starts就是抓住了这点进行优化，就是类似记忆化搜索的思想 +
dp填表法的思想。它的具体思想如下：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> T;                <span class="comment">// episode的长度</span></span><br><span class="line"><span class="type">int</span> q_sum[][];        <span class="comment">// (s,a)的action value的总和</span></span><br><span class="line"><span class="type">int</span> q_cnt[][];        <span class="comment">// (s,a)的action value的采样次数</span></span><br><span class="line"><span class="type">int</span> q[][];            <span class="comment">// (s,a)的action value</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> (<span class="number">1</span>) &#123;</span><br><span class="line">   <span class="keyword">if</span> (如果不想再迭代了) <span class="keyword">break</span>;</span><br><span class="line">   <span class="keyword">else</span> 确定起点(s0, a0)，按照当前policy生成一条长度为T的episode，将episode路上的(si, ai, ri)存到vector: path中</span><br><span class="line">       </span><br><span class="line">   <span class="type">int</span> G = <span class="number">0</span>;    <span class="comment">// episode的return</span></span><br><span class="line">   <span class="keyword">for</span> (<span class="type">int</span> i = path.<span class="built_in">size</span>() - <span class="number">1</span>; i &gt;= <span class="number">0</span>; i--) &#123;</span><br><span class="line">       G = gamma * G + path[i].r;</span><br><span class="line">       q_sum[s][a] += G;</span><br><span class="line">       q_cnt[s][a] += <span class="number">1</span>;</span><br><span class="line">       </span><br><span class="line">       q[s][a] = q_sum[s][a] / q_cnt[s][a];</span><br><span class="line">       更新<span class="built_in">pi</span>(a|s)</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>用数学语言描述如下： <span class="math display">\[
\begin{align*}
&amp;\textbf{Initialization: }\text{ Initial policy $\pi_0(a|s)$ and
initial value $q(s,a)$ for all $(s,a)$.} \\
&amp;\quad\quad\quad\quad\quad\quad\quad~\text{Returns(s,a) = 0 and
Num(s,a) = 0 for all $(s,a)$.} \\
&amp;\text{For each episode, do} \\
&amp;\quad\quad \text{Episode generation: Select a starting state-action
pair $(s_0, a_0)$}  \\
&amp;\quad\quad \text{and ensure that all pairs can be possibly selected
(this is the exploring-starts condition).} \\
&amp;\quad\quad \text{Following the current policy, generate an episode
of length $T$: $s_0, a_0, r_1, \cdots, s_{T-1}, a_{T-1}, r_T$.} \\
&amp;\quad\quad \text{Initialization for each episode: $g \gets 0$} \\
&amp;\quad\quad \text{For each step of the episode, $t = T - 1, T - 2,
\cdots, 0, $ do} \\
&amp;\quad\quad\quad\quad g \gets \gamma g + r_{t+1} \\
&amp;\quad\quad\quad\quad \text{Returns($s_t, a_t$) $\gets$
Returns($s_t, a_t$) + $g$} \\
&amp;\quad\quad\quad\quad \text{Policy evaluation:} \\
&amp;\quad\quad\quad\quad q(s_t, a_t) \gets \text{Returns($s_t, a_t$) /
Num($s_t,a_t$)} \\
&amp;\quad\quad\quad\quad \text{Policy improvement:} \\
&amp;\quad\quad\quad\quad \text{$\pi(a|s_t)=1$ if $a =
\text{argmax}_aq(s_t, a)$ and $\pi(a|s_t)=0$ otherwise}
\end{align*}
\]</span> 最后我们来看看这个“Exploring
Starts”是什么意思。Exploring我个人理解就是由于是dp填表法，所以episode就需要自己去生成，也就是exploring的过程。Starts是因为此时的算法不是像MC
Basic样每个state-action
pair都去强制估计了，所以为了尽量确保每个state-action都被估计到，每个episode的起点的选法就很有讲究，最好每个state-action都被作为起点选择一次（当然这就退化为MC
Basic了）</p>
<h4 id="mc-varepsilon-greedy">MC <span
class="math inline">\(\varepsilon\)</span>-Greedy</h4>
<p>MC Exploring Starts算法很好，但是它不能保证每个state-action
pair都被估计到，所以选多少个episode，每个episode的起点是啥就很有讲究。可能起点选少了直接导致效果不好，选多了又速度慢。</p>
<p>为了解决上述问题，MC <span
class="math inline">\(\varepsilon\)</span>-Greedy 算法应运而生。</p>
<p><span class="math inline">\(\varepsilon\)</span>-Greedy与MC Exploring
Starts的区别就在Policy improvement这一步，MC Exploring
Starts在这一步是直接将最大的action
value对应的action的概率设为1其余为0，但是<span
class="math inline">\(\varepsilon\)</span>-Greedy是最大的action
value对应的action概率设为<span class="math inline">\(1 -
\varepsilon\)</span>，其余的action概率设为<span
class="math inline">\(\varepsilon\)</span>。</p>
<p>这样的好处就是不需要对全部的state-action
pair都作为起点生成episode，只要生成一些episode（起点随便），并且只要保证这条episode的长度T很长，那你在就几乎可以路过所有state-action
pairs，从而估计它们。就不需要从很多不同的起点开始了。</p>
<p>坏处就是通过<span
class="math inline">\(\varepsilon\)</span>-Greedy得到的policy并不是最优的，因为它始终带着探索的概率。所以通常我们的做法是：</p>
<p>初始化policy为<span
class="math inline">\(\varepsilon\)</span>策略，进行多次episode，每次的episode的<span
class="math inline">\(\varepsilon\)</span>递减。这样就可以保证前面的episode随机性很强，从而可以覆盖到大多数state-action
pair，但是毕竟我们是要求最优解的，所以后期的episode的<span
class="math inline">\(\varepsilon\)</span>就得减小。最后，在通过<span
class="math inline">\(\varepsilon\)</span>-Greedy得到一个policy后，还是要将其转为确定性的策略（即不带概率的）。</p>
<p><span
class="math inline">\(\varepsilon\)</span>-Greedy这个算法的目的就是为了避免要进行多次全部state-action
pair起点选择，仅此而已。它是如何做到的？尝试不是最优的策略，通过尝试不是最优的策略，从而尽可能的，覆盖所有的state-action
paris。所以从原理上，这个算法就是会降低准确率。但是它也很具有实际意义，因为你想啊，你在仿真中设计的算法是要用到实际环境中去的。你的机器人通常就从安全的起点出发开始探索，如果你要让它从全部的state-action开始，那显然不现实。例如深海作业，你都是让robot从浅水区开始，然后让它自己去探索，尽可能覆盖所有state-action
pairs。你总不可能让它从深海区开始吧，因为你人类到不了深海区。</p>
<p>下面是用数学描述： <span class="math display">\[
\begin{align*}
&amp;\textbf{Initialization: } \text{Initial policy $\pi_0(a|s)$ and
initial value $q(s,a)$ for all $(s,a)$. Returns(s,a)=0 and Num(s,a)=0}
\\
&amp;\text{for all $(s,a)$. $\varepsilon \in (0, 1]$} \\
&amp;\text{For each episode, do} \\
&amp;\quad\quad \text{Episode generation: Select a starting state-action
pair $(s_0, a_0)$. Following the current policy,} \\
&amp;\quad\quad \text{generate an episode of length $T: s_0, a_0, r_1,
\cdots, s_{T-1}, a_{T-1}, r_T$.} \\
&amp;\quad\quad \text{Initialization for each episode: $g \gets 0$} \\
&amp;\quad\quad \text{For each step of the episode, $t = T-1, T-2,
\cdots, 0,$ do} \\
&amp;\quad\quad\quad\quad g \gets \gamma g + r_{t+1} \\
&amp;\quad\quad\quad\quad \text{Returns($s_t, a_t$) $\gets$
Returns($s_t, a_t$) + $g$} \\
&amp;\quad\quad\quad\quad \text{Num($s_t, a_t$) $\gets$ Num($s_t, a_t$)
+ 1} \\
&amp;\quad\quad\quad\quad \text{Policy evaluation:} \\
&amp;\quad\quad\quad\quad q(s_t, a_t) \gets \text{Returns($s_t, a_t$) /
Num($s_t, a_t$)} \\
&amp;\quad\quad\quad\quad \text{Let } a^* = \text{argmax}_a q(s_t, a)
\text{ and} \\
&amp;\quad\quad\quad\quad\quad\quad \pi(a|s_t) = \begin{cases}1 -
\frac{|\mathcal{A}(s_t)|-1}{|\mathcal{A}|}\varepsilon, \quad a = a^* \\
\frac{1}{|\mathcal{A}(s_t)|}\varepsilon, \quad a \ne a^* \end{cases}
\end{align*}
\]</span> 个人觉得，<span
class="math inline">\(\varepsilon\)</span>-Greedy的探索性与收敛性是严重矛盾的，因为明明已经通过采样得到state-action
value值了，只需要一直不断的更新deterministic的策略，就可以收敛了。但是<span
class="math inline">\(\varepsilon\)</span>-Greedy为了探索性，会将"探索"这个元素，加入进自己的"策略"里。所以，就像一个明知道最优解的人，在做事情的时候，仍然小概率选择不是最优的东西。所以，<span
class="math inline">\(\varepsilon\)</span>-Greedy
这个算法收敛性，有些靠天。</p>
<h4 id="总结">总结</h4>
<p>我们从model-based的算法（值/策略迭代算法）开始说起，model-based算法的收敛性和最优性都是有保证的，在"强化学习1"中有提到证明。</p>
<p>随后我们进入了model-free算法，此时我们只能依靠采样来估计<span
class="math inline">\(p(s,a)\)</span>，所以MC
Basic只要采样数量无限大，那么其准确性和收敛性也是可以得到保证的。</p>
<p>但是MC Basic效率太慢了，为此MC Exploring
Starts应运而生，运用了记忆化的思想加速了收敛。只要保证每个action-pair都被大量采样到，该算法也能保证准确性和收敛性。</p>
<p>但是问题就是为了保证“每个action-pair都被大量采样到”，MC Exploring
Starts就需要从不同的action-pair起点去生成episode进行采样。而现实环境中这是有难度的，例如你不能让机器人从深海区开始，一般都是从浅水区开始。</p>
<p>所以MC <span
class="math inline">\(\varepsilon\)</span>-Greedy算法应运而生，增加了"探索"机制，从而不必使每一个action-pair都要作为起点去生成episode进行采样。但是因为探索是直接加到policy里，所以该算法的准确率会下降，甚至收敛都不一定。不过这对未来的算法具有启发意义。</p>
<h3 id="三.-随机近似理论">三. 随机近似理论</h3>
<p>这一章的内容是为了下一章时序差分方法打基础。</p>
<h4 id="引入-1">引入</h4>
<p>以前我们用采样估计一个平均数的时候，都是收集m个样本，然后求它们的平均数作为估计值。</p>
<p>但这样做的话，你需要等到所有的样本都收集到了，才能进行估计。</p>
<p>所以我们可以用增量式的方法来解决这个问题：</p>
<p>令：<span class="math inline">\(w_{k+1} = \frac{1}{k}\sum_{i=1}^k
x_i, k=1,2,\cdots\)</span></p>
<p>所以有：<span class="math inline">\(w_k =
\frac{1}{k-1}\sum_{i=1}^{k-1}x_i, k=1,2,\cdots\)</span></p>
<p>那么：<span class="math inline">\(w_{k+1} = w_k - \frac{1}{k}(w_k -
x_k)\)</span></p>
<p>所以来一个样本，就做一次迭代，最终估计的效果跟全部收集到是一样的。</p>
<p>其实，上面的算法可以进一步推广为： <span class="math display">\[
w_{k+1} = w_k - \alpha_k(w_k - x_k)
\]</span> 当<span class="math inline">\(\alpha_k &gt;
0\)</span>并且满足一定条件时，上面的<span
class="math inline">\(w_k\)</span>也能收敛于<span
class="math inline">\(\mathbb{E}(X)\)</span></p>
<p>上面的算法其实就是一种特殊的Stochastic Approximation algorithm.</p>
<h4 id="rm">RM</h4>
<p>Stochastic Apporximation (SA)
algorithm其实是一大类算法的总称，描述的是“涉及到随机变量采样”、"迭代式"的算法。</p>
<p>Robbins-Monro (RM)算法是SA algorithm算法里的一项开创性工作。</p>
<p>下面来思考这么一个问题，解方程： <span class="math display">\[
g(w) = 0
\]</span> 其中，<span
class="math inline">\(g(\cdot)\)</span>未知，但是<span
class="math inline">\(\nabla g\)</span>是正数且有上下界。</p>
<p>那么，RM algorithm可以解决这个问题： <span class="math display">\[
w_{k+1} = w_k - a_k \tilde{g}(w_k, \eta_k), \quad k=1,2,3,\cdots
\]</span></p>
<ul>
<li><span class="math inline">\(w_k\)</span> 是第k次迭代对<span
class="math inline">\(g(w)=0\)</span>解的估计</li>
<li><span class="math inline">\(\tilde{g}(w_k, \eta_l)=g(w_k) +
\eta_k\)</span>是第k次迭代的黑盒输出(这个输出跟真实的输出可能存在误差)</li>
<li><span class="math inline">\(a_k\)</span>是第k次迭代的正系数</li>
</ul>
<p>只要一直迭代下去，那么最终迭代到的<span
class="math inline">\(w^*\)</span>就是<span
class="math inline">\(g(w)=0\)</span>的解。</p>
<p>为啥成立呢？</p>
<p>从直观上很容易理解，因为<span class="math inline">\(\nabla
g\)</span>是正数且有上下界，所以如果<span
class="math inline">\(w_k\)</span>越过了零点，那么其函数值就&gt;0了，那么<span
class="math inline">\(w_k\)</span>就要往回走一点，也就是减去一个正数，用系数
* 函数值刚好可以用作这个系数。如果<span
class="math inline">\(w_k\)</span>还没到零点，那么其函数值&lt;0，那么<span
class="math inline">\(w_k\)</span>就要前进一点，也就是减去一个负数，用系数
* 函数值刚好可以用作这个系数。一直迭代下去，<span
class="math inline">\(w_k\)</span>自然趋近零点。</p>
<p>以上，都是直观上的描述。现在，让我们给出Robbins-Monor (RM)
算法的严谨数学表述： <span class="math display">\[
\begin{align*}
&amp;\text{In the Robbins-Monro algorithm: } w_{k+1} = w_k - a_k
\tilde{g}(w_k, \eta_k), \quad k=1,2,3,\cdots\\
&amp;if \\
&amp;\quad\quad \text{1) $0 &lt; c_1 \le \nabla_w g(w) \le c_2$ for all
$w$;} \\
&amp;\quad\quad \text{2) $\sum_{k=1}^{\infty}a_k = \infty$ and
$\sum_{k=1}^{\infty}a_k^2 &lt; \infty$;} \\
&amp;\quad\quad \text{3) $\mathbb{E}[\eta_k | \mathcal{H}_k] = 0$ and
$\mathbb{E}[\eta_k^2 | \mathcal{H}_k] &lt; \infty$}; \\
&amp;\text{where $\mathcal{H}_k = \{w_k, w_{k-1}, \cdots, \}$, then
$w_k$ converges with probability 1 (w.p.1) to the root $w^*$ satisfying
$g(w^*)=0$.}
\end{align*}
\]</span></p>
<ul>
<li>这里用依概率收敛(w.p.1)是因为<span
class="math inline">\(w_k\)</span>是涉及到随机变量采样的一个数，所以为了严谨，这里用了w.p.1</li>
<li>(1)是对梯度的要求，即要求梯度是大于0的且有上下界</li>
<li>(2)是对系数的要求，<span
class="math inline">\(\sum_{k=1}^{\infty}a_k^2 &lt;
\infty\)</span>保证了<span
class="math inline">\(a_k\)</span>会收敛到0，<span
class="math inline">\(\sum_{k=1}^{\infty}a_k =
\infty\)</span>保证了<span
class="math inline">\(a_k\)</span>收敛的速度不会很快</li>
<li>(3)是对测量误差的要求，就是说假设你采样的误差的期望要是0，且误差的平方的期望不能发散</li>
</ul>
<p>这里，我想讨论一下为什么第二个条件很重要：</p>
<p>若<span class="math inline">\(\sum_{k=1}^{\infty}a_k^2 &lt;
\infty\)</span>，则会保证<span
class="math inline">\(a_k\)</span>收敛到0，则会使得<span
class="math inline">\(w_{\infty+1} = w_{\infty}\)</span>，即<span
class="math inline">\(w_k\)</span>收敛。</p>
<p>若<span class="math inline">\(\sum_{k=1}^{\infty}a_k =
\infty\)</span>呢？有啥用？不妨写出下列式子： <span
class="math display">\[
\begin{cases}
&amp;w_2 = w_1 - a_1\tilde{g}(w_1, \eta_1) \\
&amp;w_3 = w_2 - a_2\tilde{g}(w_2, \eta_2) \\
&amp;\cdots
\end{cases}
\]</span> 将以上式子全加起来，可得到：<span
class="math inline">\(w_{\infty} - w_1 = \sum_{k=1}^{\infty}a_k
\tilde{g}(w_k, \eta_k)\)</span></p>
<p>如果<span class="math inline">\(\sum_{k=1}^{\infty}a_k =
\infty\)</span>，就可以保证<span
class="math inline">\(\sum_{k=1}^{\infty}a_k \tilde{g}(w_k,
\eta_k)\)</span>发散，这样我们的<span
class="math inline">\(w_1\)</span>就随便取都行了。如果有界的话，那我的<span
class="math inline">\(w_1\)</span>的取值就被限定在一个范围了。</p>
<h4 id="sgd">SGD</h4>
<p>GD、BGD、SGD其实是一个系列的算法，它们的目的，都是解决下列这个优化问题：
<span class="math display">\[
\min_w J(w) = \mathbb{E}\left[ f(w, X) \right]
\]</span> （即找到<span class="math inline">\(w\)</span>，使得<span
class="math inline">\(J(w)\)</span>最小）</p>
<p>梯度下降大家都很熟悉了，这里直接给出定义和简单解释。</p>
<p>首先是gradient descent, GD, 梯度下降算法： <span
class="math display">\[
w_{k+1} = w_k - \alpha_k \mathbb{E}\left[ \nabla_wf(w_k, X) \right]
\]</span>
但是这个形式涉及到期望，是理想的式子，在现实中，我们往往用样本去估计这个期望，所以就有了batch
gradient descent, BGD： <span class="math display">\[
\mathbb{E}\left[ \nabla_wf(w_k, X) \right] \approx
\frac{1}{n}\sum_{i=1}^{n} \nabla_wf(w_k, x_i) \\
w_{k+1} = w_k - \alpha_k \frac{1}{n} \sum_{i=1}^{n} \nabla_w f(w_k, x_i)
\]</span>
但是毕竟还是要求出一个batch后才能迭代更新一次嘛，还是慢了，那就来一个样本就更新一次，于是就有了stochastic
gradient descent, SGD, 随机梯度下降： <span class="math display">\[
w_{k+1} = w_k - \alpha_k \nabla_w f(w_k, x_k)
\]</span> 证明SGD收敛的过程我这里大概证一下：</p>
<p>首先，SGD可以写为RM算法形式： <span class="math display">\[
w_{k+1} = w_k - \alpha_k \tilde{g}(w_k, \eta_k) \\
\tilde{g}(w_k, \eta_k) = \nabla_wf(w_k, x_k) =
\mathbb{E}[\nabla_wf(w,X)] + \left( \nabla_w f(w_k,x_k) -
\mathbb{E}[\nabla_w f(w,X)] \right)
\]</span> 令<span class="math inline">\(g(w,X) = \nabla_w f(w,
X)\)</span>，其实我们就是想求解<span class="math inline">\(g(w, X) =
0\)</span>这个方程。</p>
<p>那么上面的<span class="math inline">\(\tilde{g}(w_k,
\eta_k)\)</span>就可写为<span class="math inline">\(g(w, X) +
\eta\)</span>的形式，所以SGD被写为了RM算法的形式。</p>
<p>只要保证<span class="math inline">\(g(w,
X)\)</span>的梯度是正数且有上下界（即<span class="math inline">\(f(w,
X)\)</span>是凸的），且系数满足那俩条件，且误差<span
class="math inline">\(\eta\)</span>满足那俩条件，那么SGD算法的收敛性就可以得到保证。</p>
<p>用数学语言描述SGD的收敛条件如下： <span class="math display">\[
\begin{align*}
&amp;\text{In the SGD algorithm, if} \\
&amp;\quad\quad \text{1) } 0 &lt; c_1 \le \nabla_w^2 f(w, X) \le c_2 \\
&amp;\quad\quad \text{2) } \sum_{k=1}^{\infty}a_k = \infty \text{ and }
\sum_{k=1}^{\infty} a_k^2 &lt; \infty \\
&amp;\quad\quad \text{3) } \{x_k\}_{k=1}^{\infty} \text{ is iid} \\
&amp;\text{then $w_k$ converges to the root of $\nabla_w\mathbb{E}[f(w,
X)] = 0$ with probability 1.}
\end{align*}
\]</span> SGD这个算法究竟好不好呢？其实是挺好的，当<span
class="math inline">\(w_k\)</span>与<span
class="math inline">\(w^*\)</span>相距较远时，它的表现和GD的表现差不多。我们可以通过误差来看看：
<span class="math display">\[
\delta_k\doteq\frac{|\nabla_wf(w_k,x_k)-\mathbb{E}[\nabla_wf(w_k,X)]|}{|\mathbb{E}[\nabla_wf(w_k,X)]|}
\]</span> 那么通过理论分析，我们可以得到，SGD算法下，这个误差满足：
<span class="math display">\[
\delta_k\leq\frac{\mid\overbrace{\nabla_wf(w_k,x_k)}^\text{stochastic
gradient}-\overbrace{\mathbb{E}[\nabla_wf(w_k,X)]}^\text{true
gradient}\mid}{\underbrace{c|w_k-w^*|}_{\text{distance to the optimal
solution}}}
\]</span> 所以当<span class="math inline">\(w_k\)</span>与<span
class="math inline">\(w^*\)</span>相距较远时，它的表现和GD的表现差不多，这是个很不错的算法。</p>
<h4 id="总结-1">总结</h4>
<ul>
<li>这一章其实上是介绍了优化算法。</li>
<li>首先先介绍了解决<span class="math inline">\(g(w) =
0\)</span>的RM算法：<span class="math inline">\(w_{k+1} = w_k - a_k
\tilde{g}(w_k, \eta_k)\)</span>，它需要满足下列三个条件才能收敛：
<ol type="1">
<li><span class="math inline">\(0 &lt; c_1 \le \nabla_w g(w) \le
c_2\)</span></li>
<li><span class="math inline">\(\sum_{k=1}^{\infty}a_k = \infty\)</span>
and <span class="math inline">\(\sum_{k=1}^{\infty}a_k^2 &lt;
\infty\)</span></li>
<li><span class="math inline">\(\mathbb{E}[\eta_k | \mathcal{H}_k] =
0\)</span> and <span class="math inline">\(\mathbb{E}[\eta_k^2 |
\mathcal{H}_k] &lt; \infty\)</span></li>
</ol></li>
<li>GD系列算法也属于RM算法，它们负责解决<span
class="math inline">\(\min_w J(w) = \mathbb{E}\left[ f(w, X)
\right]\)</span>问题，换句话说，就是解决<span
class="math inline">\(\mathbb{E}\left[ \nabla_wf(w_k, X) \right] =
0\)</span>问题，所以也可以转换为RM去证明收敛性。最终证明出需要满足下列三个条件才能收敛：
<ol type="1">
<li><span class="math inline">\(0 &lt; c_1 \le \nabla_w^2 f(w, X) \le
c_2\)</span></li>
<li><span class="math inline">\(\sum_{k=1}^{\infty}a_k = \infty \text{
and } \sum_{k=1}^{\infty} a_k^2 &lt; \infty\)</span></li>
<li><span class="math inline">\(\{x_k\}_{k=1}^{\infty} \text{ is
iid}\)</span></li>
</ol></li>
</ul>
<h3 id="四.-时序差分方法">四. 时序差分方法</h3>
<h4 id="td-algorithm">TD algorithm</h4>
<p>TD算法通常是指一大类算法，但是这一小节的TD算法就是具体的一个小算法，它用来在已知一个策略<span
class="math inline">\(\pi\)</span>下，来估计<span
class="math inline">\(v_\pi\)</span>的值。</p>
<p>首先回想一下MC Exploring
Starts的思路，进行很多次起点选择，每次选择一个起点后生成一条episode，然后倒着更新一路上的<span
class="math inline">\(q(s,a)\)</span></p>
<p>是有点记忆化的味道了，不过还不够记忆化，因为</p>
<p>我直接给出TD algorithm： <span class="math display">\[
\begin{cases}
v_{t+1}(s_t) = v_t(s_t) - \alpha_t(s_t)\left[ v_t(s_t) - \left[ r_{t+1}
+ \gamma v_t(s_{t+1}) \right] \right], \\
v_{t+1}(s) = v_t(s), \forall s \ne s_t.
\end{cases}
\]</span>
这就是完全的记忆化了。在程序上可以这么实现：进行若干次迭代，每次迭代循环所有的state:
<span
class="math inline">\(s_t\)</span>，根据policy可以找到它的下一个状态:
<span
class="math inline">\(s_{t+1}\)</span>，然后按照上面的迭代式更新一遍<span
class="math inline">\(v(s_t)\)</span>。若干次迭代结束后，<span
class="math inline">\(v(s) \to v_{\pi}(s), \forall s\)</span></p>
<p>上面的式子从直观上也很好理解，<span
class="math inline">\(v_t(s_t)\)</span>就是对<span
class="math inline">\(v_\pi(s_t)\)</span>的估计，所以<span
class="math inline">\(v_t(s_t)\)</span>是不断在迭代更新的，咋更新的呢？就是不断的逼近<span
class="math inline">\(r_{t+1} + \gamma
v_t(s_{t+1})\)</span>。这就是bellman
equation啊。所以本质上我觉得就是迭代法求bellman
equation，只不过与最开始那个迭代法求bellman
equation是在model-based的情况下，现在这个迭代法是无需知道model的。</p>
<p>我们来从数学上证明一下上面那个算法为什么会收敛到<span
class="math inline">\(r_{t+1} + \gamma v_t(s_{t+1})\)</span>：</p>
<p>令：<span class="math inline">\(y = r_{t+1} + \gamma
v_t(s_{t+1})\)</span></p>
<p>则迭代式可写为：<span class="math inline">\(v_{t+1}(s_t) = v_t(s_t) -
\alpha_t(s_t)[v_t(s_t) - y]\)</span></p>
<p>两边同减y：<span class="math inline">\(v_{t+1}(s_t) - y = (v_t(s_t) -
y) - \alpha_t(s_t)[v_t(s_t) - y]\)</span></p>
<p>整理：<span class="math inline">\(v_{t+1}(s_t) - y = [1 -
\alpha_t(s_t)](v_t(s_t) - y)\)</span></p>
<p><span class="math inline">\(\therefore \|v_{t+1}(s_t) - y\| \le \| 1
- \alpha_t(s_t) \| \cdot \|v_t(s_t) - y\|\)</span></p>
<p>所以最终<span class="math inline">\(v_{t}(s_t) \to y\)</span></p>
<p>当<span class="math inline">\(v_t(s_t) \to r_{t+1} + \gamma
v_t(s_{t+1})\)</span>时其实就是bellman equation了。</p>
<p>（你可能会问，你这个bellman
equation没带概率啊。别急，概率在迭代过程中policy选择当前state下一个状态<span
class="math inline">\(s_{t+1}\)</span>进行更新的时候用到了，所以估计出来的这个<span
class="math inline">\(v_t(s)\)</span>，就是<span
class="math inline">\(v_\pi(s)\)</span>）</p>
<p>（数学证明出，当<span class="math inline">\(\sum_{t} \alpha_t(s) =
\infty\)</span>, <span class="math inline">\(\sum_{t} \alpha_t^2(s) &lt;
\infty\)</span>, <span class="math inline">\(\forall
s\)</span>时，上述TD算法就能使<span class="math inline">\(v_t(s) \to
v_\pi(s), \forall s\)</span>）</p>
<h4 id="sarsa">Sarsa</h4>
<p>前面的TD是在model-free的情况下估计出state
value。Sarsa就是在model-free的情况下估计出action value。</p>
<p>直接给出算法，非常好理解： <span class="math display">\[
\begin{cases}
q_{t+1}(s_t, a_t) = q_t(s_t, a_t) - \alpha_t(s_t, a_t)\left[ q_t(s_t,
a_t) - [r_{t+1} + \gamma q_t(s_{t+1}, a_{t+1})] \right], \\
q_{t+1}(s,a) = q_t(s,a), \quad \forall (s,a) \ne (s_t, a_t)
\end{cases}
\]</span>
有意思的小故事，为啥叫Sarsa？其实就是state-action-reward-state-action的缩写，难蚌。</p>
<p>程序也很好写，进行若干次迭代，每次迭代双重循环枚举state、action，然后按照上面迭代式更新就好了。最终<span
class="math inline">\(q_t(s,a) \to q_\pi(s,t)\)</span></p>
<p>其收敛条件为：<span class="math inline">\(\sum_{t} \alpha_t(s,a) =
\infty\)</span>, <span class="math inline">\(\sum_{t}\alpha_t^2(s,a)
&lt; \infty\)</span>, <span class="math inline">\(\forall
(s,a)\)</span></p>
<p>action value都求出来了，后续你是greedy还是<span
class="math inline">\(\varepsilon\)</span>-Greedy去update你的policy都可以。</p>
<h4 id="q-learning">Q-learning</h4>
<p>跟Sarsa不同，Sarsa是估计action value，结合policy
improvement才可以得到最优policy。而Q-learing是一步到位直接估计optimal
action value。</p>
<p>其实就修改了Sarsa迭代式里的一个地方，直观感觉就是把policy
improvement这一步直接换成在更新时就优化action
value了。这样直接可以得到最优action value，再greedy的求出最优策略即可。
<span class="math display">\[
\begin{cases}
q_{t+1}(s_t, a_t) = q_t(s_t, a_t) - \alpha_t(s_t, a_t)\left[ q_t(s_t,
a_t) - [r_{t+1} + \gamma \max_{a \in \mathcal{A}}q_t(s_{t+1}, a)]
\right], \\
q_{t+1}(s,a) = q_t(s,a), \forall (s,a) \ne (s_t, a_t)
\end{cases}
\]</span>
因为Q-learning用的很多，所以这里我给出其正式的流程描述，分为on-policy和off-policy两个版本：</p>
<p><strong>On-policy version：</strong> <span class="math display">\[
\begin{align*}
&amp;\text{For each episode, do} \\
&amp;\quad\quad \text{If the current $s_t$ is not the target state, do}
\\
&amp;\quad\quad\quad\quad \text{Collect the experience $(s_t, a_t,
r_{t+1}, s_{t+1})$: In particular, take action $a_t$ follwing} \\
&amp;\quad\quad\quad\quad \pi_t(s_t), \text{ generate } r_{t+1},
s_{t+1}. \\
&amp;\quad\quad\quad\quad \text{Update q-value:} \\
&amp;\quad\quad\quad\quad\quad\quad q_{t+1}(s_t, a_t) = q_t(s_t, a_t) -
\alpha_t(s_t, a_t) - \alpha_t(s_t, a_t)\left[ q_t(s_t, a_t) - \left[
r_{t+1} + \gamma \max_{a}q_t(s_{t+1}, a) \right] \right] \\
&amp;\quad\quad\quad\quad \text{Update policy:} \\
&amp;\quad\quad\quad\quad\quad\quad \pi_{t+1}(a|s_t) = 1 -
\frac{\varepsilon}{|\mathcal{A}|}(|\mathcal{A}| - 1) \text{ if } a =
\text{argmax}_a q_{t+1}(s_t, a) \\
&amp;\quad\quad\quad\quad\quad \quad \pi_{t+1}(a|s_t) =
\frac{\varepsilon}{|\mathcal{A}|} \text{ otherwise}
\end{align*}
\]</span> <strong>Off-policy version：</strong> <span
class="math display">\[
\begin{align*}
&amp;\text{For each episode $\{s_0, a_0, r_1, s_1, a_1, r_2, \cdots \}$
generated by $\pi_b$, do} \\
&amp;\quad\quad \text{For each step $t = 0,1,2,\cdots$ of the episode,
do} \\
&amp;\quad\quad\quad\quad \text{Update q-value:} \\
&amp;\quad\quad\quad\quad\quad\quad q_{t+1}(s_t, a_t) = q_t(s_t, a_t) -
\alpha_t(s_t, a_t) - \alpha_t(s_t, a_t)\left[ q_t(s_t, a_t) - \left[
r_{t+1} + \gamma \max_{a}q_t(s_{t+1}, a) \right] \right] \\
&amp;\quad\quad\quad\quad \text{Update target policy:} \\
&amp;\quad\quad\quad\quad\quad\quad \pi_{T,t+1}(a|s_t) = 1 \text{ of } a
= \text{argmax}_a q_{t+1}(s_t, a) \\
&amp;\quad\quad\quad\quad\quad\quad \pi_{T,t+1}(a|s_t) = 0 \text{
otherwise}
\end{align*}
\]</span>
可以发现，off-policy与on-policy的区别就是，off-policy生成experience数据的policy与优化出的policy不同。</p>
<p>所以off-policy的一个特点就是最终的优化结果跟生成数据的policy相关性很大，因为它是在生成experience数据的policy基础上优化的。</p>
<p>但是on-policy，即使初始策略很烂，但是因为是持续优化，最终仍可以收敛到全局最优policy。</p>
<p>那么off-policy就没有好处了吗？并不是的，它有着自身的优点，在后续与DL结合的时候你就知道了。</p>
<h4 id="总结-2">总结</h4>
<p>这一章叫时序差分方法，但是我更愿意把其叫做model-free的迭代法求state
value(TD)、action value(Sarsa)、optimal action
value(Q-learing)。与MC系列算法相比，我觉得是MC系列的完全记忆化版本，即MC
Exploring Starts算法的优化版本。</p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2024/10/03/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A01/" rel="prev" title="强化学习1">
      <i class="fa fa-chevron-left"></i> 强化学习1
    </a></div>
      <div class="post-nav-item">
    <a href="/2024/10/07/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A03/" rel="next" title="强化学习3">
      强化学习3 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    
  <div class="comments">
    <div id="lv-container" data-id="city" data-uid="MTAyMC81OTA0Ny8zNTUwOQ=="></div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%80.-%E5%80%BC%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3%E7%AE%97%E6%B3%95"><span class="nav-text">一. 值&#x2F;策略迭代算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%80%BC%E8%BF%AD%E4%BB%A3%E7%AE%97%E6%B3%95"><span class="nav-text">值迭代算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3%E7%AE%97%E6%B3%95"><span class="nav-text">策略迭代算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%A4%E8%80%85%E6%AF%94%E8%BE%83"><span class="nav-text">两者比较</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BA%8C.-%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B"><span class="nav-text">二. 蒙特卡洛</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BC%95%E5%85%A5"><span class="nav-text">引入</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#mc-basic"><span class="nav-text">MC Basic</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#mc-exploring-starts"><span class="nav-text">MC Exploring Starts</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#mc-varepsilon-greedy"><span class="nav-text">MC \(\varepsilon\)-Greedy</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%89.-%E9%9A%8F%E6%9C%BA%E8%BF%91%E4%BC%BC%E7%90%86%E8%AE%BA"><span class="nav-text">三. 随机近似理论</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BC%95%E5%85%A5-1"><span class="nav-text">引入</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#rm"><span class="nav-text">RM</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#sgd"><span class="nav-text">SGD</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%80%BB%E7%BB%93-1"><span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%9B.-%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%E6%96%B9%E6%B3%95"><span class="nav-text">四. 时序差分方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#td-algorithm"><span class="nav-text">TD algorithm</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#sarsa"><span class="nav-text">Sarsa</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#q-learning"><span class="nav-text">Q-learning</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%80%BB%E7%BB%93-2"><span class="nav-text">总结</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Error_666"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">Error_666</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">39</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/potatoQi" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;potatoQi" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://gitee.com/Error_666" title="Gitee → https:&#x2F;&#x2F;gitee.com&#x2F;Error_666" rel="noopener" target="_blank"><i class="fa fa-paper-plane fa-fw"></i>Gitee</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.acwing.com/user/myspace/index/8252/" title="AcWing → https:&#x2F;&#x2F;www.acwing.com&#x2F;user&#x2F;myspace&#x2F;index&#x2F;8252&#x2F;" rel="noopener" target="_blank"><i class="fas fa-rocket fa-fw"></i>AcWing</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://cn.overleaf.com/project" title="Overleaf → https:&#x2F;&#x2F;cn.overleaf.com&#x2F;project" rel="noopener" target="_blank"><i class="fa fa-edit fa-fw"></i>Overleaf</a>
      </span>
  </div>



  <div class="links-of-recent-posts motion-element">
    <div class="links-of-recent-posts-title">
      <i class="fa fa-history fa-fw"></i>
      最近文章
    </div>
    <ul class="links-of-recent-posts-list">
        <li class="links-of-recent-posts-item">
          <a href="/2024/12/19/%E6%9A%82%E5%81%9C%E6%9B%B4%E6%96%B0/" title="2024&#x2F;12&#x2F;19&#x2F;暂停更新&#x2F;">暂停更新</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2024/12/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%AD%A6%E7%AC%94%E8%AE%B0/" title="2024&#x2F;12&#x2F;19&#x2F;机器学习自学笔记&#x2F;">机器学习自学笔记</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2024/10/09/RL%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/" title="2024&#x2F;10&#x2F;09&#x2F;RL代码学习框架&#x2F;">RL代码学习框架</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2024/10/07/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A03/" title="2024&#x2F;10&#x2F;07&#x2F;强化学习3&#x2F;">强化学习3</a>
        </li>
        <li class="links-of-recent-posts-item">
          <a href="/2024/10/06/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/" title="2024&#x2F;10&#x2F;06&#x2F;强化学习2&#x2F;">强化学习2</a>
        </li>
    </ul>
  </div>
      </div>
	<br>
    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2023-10-1 – 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Error_666</span>
</div>

<!--
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>
 -->


    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <span id="busuanzi_container_site_pv">总访问量<span id="busuanzi_value_site_pv"></span>次</span>
    <span class="post-meta-divider">|</span>
    <span id="busuanzi_container_site_uv">总访客数<span id="busuanzi_value_site_uv"></span>人</span>
    <span class="post-meta-divider">|</span>
<!-- 不蒜子计数初始值纠正 -->
<script>
$(document).ready(function() {

    var int = setInterval(fixCount, 50);  // 50ms周期检测函数
    var countOffset = 20000;  // 初始化首次数据

    function fixCount() {            
       if (document.getElementById("busuanzi_container_site_pv").style.display != "none")
        {
            $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + countOffset); 
            clearInterval(int);
        }                  
        if ($("#busuanzi_container_site_pv").css("display") != "none")
        {
            $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + countOffset); // 加上初始数据 
            clearInterval(int); // 停止检测
        }  
    }
       	
});
</script> 

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  
  <script>
    (function(){
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x=document.getElementsByTagName("link");
		//Find the last canonical URL
		if(x.length > 0){
			for (i=0;i<x.length;i++){
				if(x[i].rel.toLowerCase() == 'canonical' && x[i].href){
					canonicalURL=x[i].href;
				}
			}
		}
    //Get protocol
	    if (!canonicalURL){
	    	curProtocol = window.location.protocol.split(':')[0];
	    }
	    else{
	    	curProtocol = canonicalURL.split(':')[0];
	    }
      //Get current URL if the canonical URL does not exist
	    if (!canonicalURL) canonicalURL = window.location.href;
	    //Assign script content. Replace current URL with the canonical URL
      !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=canonicalURL,t=document.referrer;if(!e.test(r)){var n=(String(curProtocol).toLowerCase() === 'https')?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";t?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var i=new Image;i.src=n}}(window);})();
  </script>




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

<script>
NexT.utils.loadComments(document.querySelector('#lv-container'), () => {
  window.livereOptions = {
    refer: location.pathname.replace(CONFIG.root, '').replace('index.html', '')
  };
  (function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
  })(document, 'script');
});
setInterval(function () {
    var box = document.querySelector(".trc_rbox_container");
    if(box) box.outerHTML = "";
}, 2000);
</script>

</body>
</html>
